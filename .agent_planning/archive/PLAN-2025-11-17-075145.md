# HUMANIFYJS DEVELOPMENT BACKLOG
**Generated**: 2025-11-17 07:51:45
**Source STATUS**: STATUS-2025-11-17-074619.md (2025-11-17 07:46:19)
**Specification**: PROJECT_SPEC.md + CLAUDE.md
**Project**: HumanifyJS Deobfuscation Tool

---

## EXECUTIVE SUMMARY

### Current State Analysis

**Progress Tracking Work**: COMPLETE (5/5 user requirements achieved)
- Work estimation before API calls: ‚úÖ COMPLETE
- Global progress bar with ETA: ‚úÖ COMPLETE
- Iteration display with color coding: ‚úÖ COMPLETE
- No overlapping progress bars: ‚úÖ COMPLETE
- Color for emphasis: ‚úÖ COMPLETE

**Critical Issues Identified**:
1. **E2E Test Failure**: "CLI should deobfuscate all functions in file" shows 10/10 single-letter variables
2. **Refinement Chaining Broken**: Hardcoded filename prevents multi-file bundle refinement
3. **Core Value Proposition Unverified**: Test failure suggests LLM renaming may not be working

**Test Status**: 128/137 passing (93.4%)
- 8 skipped (intentional - checkpoint runtime, TTY tests)
- 1 CRITICAL FAILURE: output quality validation

### Gap Analysis Summary

**What Works** (High Confidence):
- ‚úÖ Core architecture and plugin pipeline
- ‚úÖ AST traversal and transformation
- ‚úÖ Dependency graph and turbo mode
- ‚úÖ Checkpoint system (after recent fix)
- ‚úÖ Progress tracking infrastructure
- ‚úÖ Display management

**What's Broken or Unverified** (Evidence-Based):
- ‚ùå Output quality (E2E test proves single-letter variables persist)
- ‚ùå Refinement chaining (hardcoded `deobfuscated.js` filename)
- ‚ùì LLM integration (may not be working as expected)

### Recommended Focus Areas

1. **Investigate and fix E2E test failure** (validates core functionality)
2. **Fix refinement chaining bug** (user requirement #3)
3. **Verify with manual testing** (end-to-end validation)

---

## BACKLOG BY PRIORITY

---

## P0 (CRITICAL) - Core Functionality Blockers

### P0-1: Investigate E2E Test Failure - Output Quality

**Status**: Not Started
**Effort**: Medium (3-5 hours)
**Dependencies**: None
**Spec Reference**: PROJECT_SPEC.md lines 5-7, 11-13 ‚Ä¢ STATUS-2025-11-17-074619.md lines 519-566

#### Description

The E2E test `src/cli-output-quality.e2etest.ts:15` ("CLI should deobfuscate all functions in file") is failing with:
```
AssertionError: Expected zero single-letter variables, found 10/10: a, b, c, d, e, f, g, h, j, k
```

This directly validates the user's complaint from PROJECT_SPEC.md:
> "even bigger problem...it's looking like we are not even using the results of our API calls in the outputted code! The outputted code (in output) is full of single letter variables."

This is CRITICAL because it questions whether the core deobfuscation functionality works at all.

#### Acceptance Criteria

- [ ] Identify which provider the failing test uses (local/OpenAI/Gemini)
- [ ] Verify whether test makes actual API calls or uses dry-run/mock mode
- [ ] Determine root cause: test expectations wrong OR implementation broken OR LLM not providing semantic names
- [ ] If test is wrong: Update test expectations with documented reasoning
- [ ] If implementation is broken: Identify exact point in pipeline where names are lost
- [ ] If LLM isn't working: Verify API calls complete successfully and return semantic names
- [ ] Document findings in a clear investigation report
- [ ] Create follow-up work items for any bugs discovered

#### Technical Notes

**Investigation Steps**:
1. Read full test file `src/cli-output-quality.e2etest.ts`
2. Check test configuration (provider, API keys, mode flags)
3. Run test with `--verbose` flag to see LLM responses
4. Add debug logging to trace data flow: API response ‚Üí visitor function ‚Üí scope.rename() ‚Üí AST generation
5. Compare test execution path vs. production CLI execution path
6. Verify test input file complexity (may be too complex for LLM)

**Possible Causes**:
- Test using local provider without model downloaded
- Test running in dry-run mode (no actual API calls)
- LLM returning poor quality suggestions (non-semantic names)
- Visitor function not being called
- `scope.rename()` mutations not being applied
- AST generation losing renamed identifiers
- Test expectations too strict (some single-letter vars acceptable)

**Data Flow Verification** (from STATUS report):
1. OpenAI API call returns `newName` (openai-rename.ts:154-164)
2. Visitor function passes `newName` to `visitAllIdentifiers`
3. `scope.rename()` mutates AST in-place (visit-all-identifiers.ts:419)
4. AST transformed to code via `generate()` (visit-all-identifiers.ts:162-164)
5. Code written to file (unminify.ts:299)

Verify each step actually executes in test environment.

---

### P0-2: Fix Refinement Chaining - Hardcoded Filename Bug

**Status**: Not Started
**Effort**: Small (2-3 hours)
**Dependencies**: None
**Spec Reference**: PROJECT_SPEC.md lines 8-10 ‚Ä¢ STATUS-2025-11-17-074619.md lines 454-491

#### Description

The refinement stage (pass 2) uses a hardcoded filename `deobfuscated.js`, but webcrack actually produces multiple files like `bundle_1.js`, `bundle_2.js`, or module names like `index.js`, `utils.js`.

User complaint from PROJECT_SPEC.md:
> "Is it possible that the 'refinement' stage is starting over completely? [...] any refinement stages should use as INPUT the already-processed code from the previous decompilation!"

**Current Code** (src/commands/openai.ts line 278):
```typescript
if (opts.refine) {
  const pass1OutputFile = `${opts.outputDir}/deobfuscated.js`; // ‚ùå HARDCODED!
  await unminify(pass1OutputFile, opts.outputDir, [ /* ... */ ]);
}
```

This will fail with "file not found" or process the wrong file for bundled code.

#### Acceptance Criteria

- [ ] Update `src/commands/openai.ts` to discover actual output files from pass 1
- [ ] Update `src/commands/gemini.ts` with same fix
- [ ] Update `src/commands/local.ts` with same fix
- [ ] Add `skipWebcrack` parameter to `unminify()` function to prevent re-bundling
- [ ] Process EACH output file separately in refinement pass
- [ ] Add console output showing which files are being refined
- [ ] Add warning if no JavaScript files found in output directory
- [ ] Test with single-file output (verify works)
- [ ] Test with multi-file bundle output (verify all files processed)
- [ ] Verify webcrack doesn't run twice (pass 1 only)

#### Technical Notes

**Implementation**:
```typescript
// Pass 2: Refinement (if enabled)
if (opts.refine) {
  displayManager.showIterationHeader(2, iterations);
  progressManager.startIteration(2);

  // Discover ACTUAL files from pass 1
  const outputContents = await fs.readdir(opts.outputDir);
  const pass1Files = outputContents
    .filter(file => file.endsWith('.js'))
    .map(file => path.join(opts.outputDir, file));

  if (pass1Files.length === 0) {
    console.warn(colorize("‚ö†Ô∏è  No JavaScript files found for refinement", "red"));
    return;
  }

  console.log(colorize(`\nüìÇ Refining ${pass1Files.length} file(s)...`, "cyan"));

  // Process EACH file from pass 1
  for (const file of pass1Files) {
    displayManager.showCurrentFile(path.basename(file), /* ... */);

    await unminify(file, opts.outputDir, [
      babel,
      openaiRename({ /* ... */ }),
      prettier
    ], {
      skipWebcrack: true, // ‚Üê NEW: Don't re-bundle!
      progressManager,
      displayManager
    });
  }
}
```

**Files to Modify**:
1. `src/commands/openai.ts` (lines 274-291)
2. `src/commands/gemini.ts` (similar section)
3. `src/commands/local.ts` (similar section)
4. `src/unminify.ts` (add `skipWebcrack` parameter handling)

**Testing**:
- Create test bundle with 3 modules
- Run with `--refine` flag
- Verify all 3 modules processed in pass 2
- Verify webcrack output from pass 1 not overwritten

---

## P1 (HIGH) - Verification and Quality

### P1-1: Verify Refinement Chaining Works End-to-End

**Status**: Not Started
**Effort**: Small (2 hours)
**Dependencies**: P0-2 (Fix refinement filename bug)
**Spec Reference**: PROJECT_SPEC.md lines 8-10, 22-27 ‚Ä¢ STATUS-2025-11-17-074619.md lines 718-732

#### Description

After fixing the hardcoded filename bug (P0-2), verify that refinement actually works correctly:
1. Pass 2 uses pass 1 output (not original file)
2. All files from pass 1 are refined
3. Webcrack doesn't run twice
4. Variable names improve between passes

This validates user requirement #3: "any refinement stages should use as INPUT the already-processed code from the previous decompilation"

#### Acceptance Criteria

- [ ] Create test case: single file input
- [ ] Verify pass 2 reads pass 1 output (not original)
- [ ] Verify renamed variables from pass 1 visible in pass 2 input
- [ ] Create test case: multi-file bundle input
- [ ] Verify ALL files processed in pass 2
- [ ] Verify webcrack runs only once (during pass 1)
- [ ] Verify pass 2 output overwrites pass 1 output (same directory)
- [ ] Add E2E test: refinement improves variable names
- [ ] Document expected behavior in CLAUDE.md
- [ ] Add troubleshooting guide for refinement issues

#### Technical Notes

**Test Structure**:
```typescript
test("refinement uses pass 1 output", async () => {
  // Run pass 1
  await unminify(input, outputDir, plugins);
  const pass1Output = await fs.readFile(`${outputDir}/bundle.js`, 'utf-8');

  // Verify pass 1 renamed some variables
  assert.ok(pass1Output.includes('someSemanticName'));

  // Run pass 2 (refinement)
  await unminify(input, outputDir, plugins, { refine: true });
  const pass2Output = await fs.readFile(`${outputDir}/bundle.js`, 'utf-8');

  // Verify pass 2 used pass 1 as input (semantic names still present)
  assert.ok(pass2Output.includes('someSemanticName'));
});
```

**Validation Points**:
1. Pass 1 output has semantic names (not all single-letter)
2. Pass 2 input matches pass 1 output (content hash comparison)
3. Pass 2 output shows further improvement (more semantic names)
4. Webcrack directory not recreated during pass 2

---

### P1-2: Add Output Quality Validation to Test Suite

**Status**: Not Started
**Effort**: Medium (3-4 hours)
**Dependencies**: P0-1 (Investigate E2E test failure)
**Spec Reference**: PROJECT_SPEC.md lines 11-13 ‚Ä¢ STATUS-2025-11-17-115400.md lines 414-418

#### Description

The test suite currently lacks automated validation of output quality. The failing E2E test proves we need better checks to ensure LLM renaming actually produces semantic variable names.

From STATUS-2025-11-17-115400.md:
> "BUG #2: No Output Validation [HIGH] - Cannot verify if deobfuscation actually works"

#### Acceptance Criteria

- [ ] Create helper function: `countSingleLetterVars(code: string): number`
- [ ] Create helper function: `calculateIdentifierComplexity(code: string): number`
- [ ] Add test: output has fewer single-letter vars than input (not zero, but fewer)
- [ ] Add test: output has higher average identifier length than input
- [ ] Add test: output has more camelCase identifiers than input
- [ ] Add test: output is syntactically valid JavaScript (parses without error)
- [ ] Add test: output has same number of functions as input (no loss)
- [ ] Document acceptable quality thresholds in test comments
- [ ] Add `--quality-report` flag to CLI (optional output metrics)
- [ ] Generate comparison report: input vs output identifier stats

#### Technical Notes

**Quality Metrics**:
```typescript
interface QualityMetrics {
  totalIdentifiers: number;
  singleLetterVars: number;
  twoLetterVars: number;
  semanticNames: number; // identifiers >= 3 chars with camelCase
  avgIdentifierLength: number;
  syntacticallyValid: boolean;
}

function calculateQuality(code: string): QualityMetrics {
  const ast = parse(code);
  const identifiers = findAllBindingIdentifiers(ast);

  const singleLetter = identifiers.filter(id => id.length === 1);
  const twoLetter = identifiers.filter(id => id.length === 2);
  const semantic = identifiers.filter(id =>
    id.length >= 3 && /^[a-z]+[A-Z]/.test(id) // camelCase pattern
  );

  return {
    totalIdentifiers: identifiers.length,
    singleLetterVars: singleLetter.length,
    twoLetterVars: twoLetter.length,
    semanticNames: semantic.length,
    avgIdentifierLength: identifiers.reduce((sum, id) => sum + id.length, 0) / identifiers.length,
    syntacticallyValid: true // if parse succeeded
  };
}
```

**Test Assertions** (realistic, not perfect):
- Single-letter vars reduced by at least 50%
- Average identifier length increased by at least 20%
- Semantic names increased by at least 30%
- Syntactically valid (no parse errors)

**Example Quality Report**:
```
=== Deobfuscation Quality Report ===

Input:
  Total identifiers: 1,245
  Single-letter vars: 523 (42%)
  Two-letter vars: 312 (25%)
  Semantic names: 98 (8%)
  Avg identifier length: 2.3 chars

Output:
  Total identifiers: 1,245
  Single-letter vars: 89 (7%) ‚Üê 83% reduction
  Two-letter vars: 156 (13%)
  Semantic names: 892 (72%) ‚Üê 9x increase
  Avg identifier length: 8.7 chars ‚Üê 3.8x increase

Quality Score: 87/100 (EXCELLENT)
```

---

### P1-3: Manual Testing with Real Files - OpenAI Provider

**Status**: Not Started
**Effort**: Medium (3-4 hours)
**Dependencies**: P0-1 (Investigate E2E test failure)
**Spec Reference**: PROJECT_SPEC.md (all user complaints) ‚Ä¢ STATUS-2025-11-17-074619.md lines 987-1022

#### Description

After investigating the E2E test failure, perform comprehensive manual testing with actual OpenAI API to verify end-to-end functionality. This is critical because unit tests passing doesn't mean the tool works in production.

#### Acceptance Criteria

- [ ] Obtain valid OpenAI API key (check .env file in repo root)
- [ ] Test case 1: Simple minified function (10 lines)
- [ ] Verify: Simple test produces semantic variable names
- [ ] Test case 2: Medium complexity (100-500 lines, ~50 identifiers)
- [ ] Verify: Medium test shows significant improvement
- [ ] Test case 3: Large file with webpack bundle (1MB+, 1000+ identifiers)
- [ ] Verify: Large file completes without OOM errors
- [ ] Verify: Progress tracking displays correctly (no flickering, accurate ETA)
- [ ] Test with `--refine` flag (2 passes)
- [ ] Verify: Refinement uses pass 1 output and improves quality
- [ ] Test with `--turbo` flag (parallel processing)
- [ ] Verify: Turbo mode completes faster and produces quality output
- [ ] Test with `--verbose` flag
- [ ] Verify: Can see actual LLM API requests and responses
- [ ] Document actual output quality in test report
- [ ] Compare actual vs expected behavior, note any discrepancies

#### Technical Notes

**Test Files**:
1. **Simple**: `const a = (b, c) => b + c;` (expect: `const add = (num1, num2) => num1 + num2;`)
2. **Medium**: Use `just test-small` (sample file from repo)
3. **Large**: Use `just test-tensorflow` (1.4MB, 35K identifiers)

**Test Commands**:
```bash
# Simple test
echo "const a = (b, c) => b + c;" > /tmp/test-simple.js
humanify unminify --provider openai /tmp/test-simple.js --verbose

# Medium test (existing sample)
just test-small

# Large test with turbo
just test-tensorflow

# Refinement test
humanify unminify --provider openai /tmp/test-simple.js --refine --verbose

# Full feature test
humanify unminify --provider openai large.js \
  --turbo \
  --refine \
  --max-concurrent 20 \
  --perf \
  --verbose
```

**What to Check**:
1. Console output: Iteration headers (yellow/blue), progress bars (no overlap), ETA accuracy
2. API logs: Request/response content, actual suggested names
3. Output files: Count single-letter vs semantic names, syntactic validity
4. Performance: Time taken, memory usage, API call count
5. Error handling: Rate limits, network errors, OOM conditions

**Document Findings**:
- Actual output quality (percentage of semantic names)
- LLM performance (what model suggests for different contexts)
- Edge cases discovered (failures, poor quality)
- User experience (progress display clarity)

---

## P2 (MEDIUM) - Enhancement and Polish

### P2-1: Improve LLM Prompts for Better Variable Names

**Status**: Not Started
**Effort**: Medium (3-4 hours)
**Dependencies**: P1-3 (Manual testing - to identify prompt weaknesses)
**Spec Reference**: STATUS-2025-11-17-115400.md lines 449-453, 360-395

#### Description

If manual testing (P1-3) reveals that LLMs are providing poor quality variable names, improve the prompts to get better suggestions. Current prompt may be too simple:
```
"Rename Javascript variables/function \`${name}\` to have descriptive name based on their usage in the code."
```

#### Acceptance Criteria

- [ ] Analyze LLM responses from manual testing (P1-3)
- [ ] Identify patterns in poor suggestions (e.g., generic names like "var1", "func1")
- [ ] Add examples of good variable names to prompt
- [ ] Add specific naming patterns to follow (camelCase, descriptive, no abbreviations)
- [ ] Add context about variable purpose (function param, loop counter, config, etc.)
- [ ] Test improved prompt on sample code
- [ ] Verify: Average identifier length increases
- [ ] Verify: More semantic names (recognizable purpose)
- [ ] Test with different models (gpt-4o-mini vs gpt-4o)
- [ ] Document prompt engineering decisions in code comments
- [ ] Add prompt customization option (advanced users)

#### Technical Notes

**Improved Prompt Structure**:
```typescript
const prompt = `
You are renaming JavaScript identifiers to improve code readability.

CONTEXT:
- Original name: ${name}
- Variable type: ${inferType(binding)} // function, parameter, constant, variable
- Scope level: ${scopeLevel} // global, module, function, block

SURROUNDING CODE:
${surroundingCode}

INSTRUCTIONS:
1. Provide a descriptive, meaningful name based on how the identifier is used
2. Use camelCase for variables and functions
3. Use PascalCase for classes and constructors
4. Use SCREAMING_SNAKE_CASE for constants
5. Avoid abbreviations unless universally understood (e.g., "url", "id", "max")
6. Consider the semantic context and variable purpose
7. If uncertain, preserve the original name

EXAMPLES:
- Loop counter: "i" ‚Üí "index" or "itemIndex"
- Function parameter for numbers: "a, b" ‚Üí "num1, num2" or "value1, value2"
- Configuration object: "c" ‚Üí "config" or "options"
- Callback function: "f" ‚Üí "callback" or "handler"
- Result variable: "r" ‚Üí "result" or "output"

RESPOND WITH JSON:
{ "newName": "suggestedName", "confidence": 0.0-1.0, "reasoning": "brief explanation" }
`;
```

**Context Enhancement**:
- Infer variable type from AST (FunctionDeclaration, VariableDeclarator, etc.)
- Detect scope level (global, function, block)
- Identify usage patterns (assigned once = constant, mutated = variable, called = function)
- Extract semantic hints from surrounding identifiers (if renaming "b" in "a + b", check if "a" already has semantic name)

**Model Comparison**:
- Test gpt-4o-mini (cheap, fast, current default)
- Test gpt-4o (expensive, slower, higher quality)
- Document quality vs cost trade-off
- Add `--model` CLI flag to override default

---

### P2-2: Add Quality Metrics and Comparison Report

**Status**: Not Started
**Effort**: Medium (4-5 hours)
**Dependencies**: P1-2 (Output quality validation)
**Spec Reference**: STATUS-2025-11-17-115400.md lines 465-477

#### Description

Generate detailed before/after comparison reports showing the quality improvement from deobfuscation. This helps users understand if the tool is providing value and identify areas needing manual review.

#### Acceptance Criteria

- [ ] Implement `IdentifierAnalyzer` class to parse and analyze code
- [ ] Calculate input metrics (before deobfuscation)
- [ ] Calculate output metrics (after deobfuscation)
- [ ] Generate comparison report showing deltas
- [ ] Add `--quality-report` CLI flag to enable report generation
- [ ] Save report to `${outputDir}/quality-report.json`
- [ ] Print human-readable summary to console
- [ ] Include quality score (0-100) based on improvement
- [ ] Add recommendations for manual review (low-quality sections)
- [ ] Test with various input complexities (simple, medium, large)

#### Technical Notes

**Report Structure**:
```typescript
interface QualityReport {
  timestamp: string;
  inputFile: string;
  outputFile: string;

  input: {
    totalIdentifiers: number;
    singleLetterVars: number;
    twoLetterVars: number;
    semanticNames: number;
    avgIdentifierLength: number;
    complexityScore: number; // 0-100, higher = more obfuscated
  };

  output: {
    totalIdentifiers: number;
    singleLetterVars: number;
    twoLetterVars: number;
    semanticNames: number;
    avgIdentifierLength: number;
    complexityScore: number;
  };

  improvement: {
    singleLetterReduction: number; // percentage
    semanticNameIncrease: number; // percentage
    avgLengthIncrease: number; // multiplier
    complexityReduction: number; // points
    qualityScore: number; // 0-100, overall improvement
  };

  recommendations: string[]; // e.g., "Section around line 45 still has 15 single-letter variables"
}
```

**Quality Score Calculation**:
```typescript
function calculateQualityScore(input: Metrics, output: Metrics): number {
  // Weight different factors
  const singleLetterWeight = 0.4;
  const semanticNameWeight = 0.3;
  const avgLengthWeight = 0.2;
  const complexityWeight = 0.1;

  const singleLetterImprovement = 1 - (output.singleLetterVars / input.singleLetterVars);
  const semanticImprovement = (output.semanticNames - input.semanticNames) / input.totalIdentifiers;
  const lengthImprovement = (output.avgIdentifierLength - input.avgIdentifierLength) / input.avgIdentifierLength;
  const complexityImprovement = (input.complexityScore - output.complexityScore) / 100;

  return Math.min(100, Math.max(0,
    singleLetterImprovement * singleLetterWeight * 100 +
    semanticImprovement * semanticNameWeight * 100 +
    lengthImprovement * avgLengthWeight * 100 +
    complexityImprovement * complexityWeight * 100
  ));
}
```

**Console Output Example**:
```
=== Quality Report ===

Input: obfuscated.js (12,345 identifiers)
  Single-letter vars: 5,234 (42%)
  Semantic names: 234 (2%)
  Avg length: 2.1 chars
  Complexity: 87/100 (highly obfuscated)

Output: deobfuscated.js (12,345 identifiers)
  Single-letter vars: 234 (2%) ‚Üì 95% reduction
  Semantic names: 10,234 (83%) ‚Üë 81% increase
  Avg length: 12.3 chars ‚Üë 5.9x increase
  Complexity: 23/100 (readable) ‚Üì 64 points

Quality Score: 92/100 (EXCELLENT)

Recommendations:
  ‚úì Excellent improvement overall
  ‚ö† Section around line 1,234 still has 45 single-letter variables (may need manual review)
  ‚ö† Function "processData" has 12 parameters with generic names

Report saved to: output/quality-report.json
```

---

### P2-3: Document Known Limitations and Troubleshooting

**Status**: Not Started
**Effort**: Small (2 hours)
**Dependencies**: P1-3 (Manual testing - to identify real limitations)
**Spec Reference**: STATUS-2025-11-17-115400.md lines 459-462

#### Description

Based on findings from investigation (P0-1) and manual testing (P1-3), document known limitations, expected behavior, and troubleshooting steps in CLAUDE.md.

#### Acceptance Criteria

- [ ] Add "Known Limitations" section to CLAUDE.md
- [ ] Document that deeply obfuscated code may need manual review
- [ ] Document model quality differences (gpt-4o-mini vs gpt-4o vs Gemini vs local)
- [ ] Document when to use refinement mode (2+ passes)
- [ ] Document optimal turbo mode settings for different file sizes
- [ ] Add "Troubleshooting" section to CLAUDE.md
- [ ] Document: "Output still has single-letter variables" ‚Üí causes and solutions
- [ ] Document: "Out of memory errors" ‚Üí chunking settings
- [ ] Document: "Rate limit errors" ‚Üí concurrency settings
- [ ] Document: "Poor quality output" ‚Üí prompt/model recommendations
- [ ] Add examples of expected vs actual output for different input types
- [ ] Document debugging techniques (--verbose, --perf, checkpoints)

#### Technical Notes

**Known Limitations to Document**:
1. **Extremely obfuscated code**: Some patterns (control flow flattening, string encryption) beyond LLM capability
2. **Context window limits**: Very large functions may not fit in LLM context, limiting quality
3. **Model capabilities**: gpt-4o-mini may struggle with complex code that gpt-4o handles well
4. **Not a magic solution**: LLMs provide best-effort renaming, not perfect semantic understanding
5. **Cost**: Large files with turbo mode can be expensive (thousands of API calls)

**Troubleshooting Guide Structure**:
```markdown
## Troubleshooting

### Issue: Output still has many single-letter variables

**Possible Causes**:
1. Using gpt-4o-mini with very complex code
2. Context window too small (increase with --context-size)
3. LLM uncertain about variable purpose (preserves original name)

**Solutions**:
- Try upgrading to gpt-4o: `--model gpt-4o`
- Increase context: `--context-size 200000`
- Use refinement mode: `--refine`
- Use turbo mode for better context: `--turbo`

### Issue: Out of memory (OOM) errors

**Possible Causes**:
1. File too large for single processing
2. Turbo mode creating too many batches in memory
3. Dependency graph too large

**Solutions**:
- Enable chunking: `--chunk-size 50000`
- Reduce max batch size: `--max-batch-size 50`
- Reduce concurrency: `--max-concurrent 5`
- Enable memory monitoring: `--max-memory 2048`

...
```

---

## P3 (LOW) - Nice-to-Have Improvements

### P3-1: Improve Chunking Reassembly for Cross-Chunk References

**Status**: Not Started
**Effort**: Medium (4-5 hours)
**Dependencies**: None
**Spec Reference**: STATUS-2025-11-17-115400.md lines 474-479

#### Description

Current chunking reassembly simply concatenates processed chunks. This works but may lose some context for symbols that span chunk boundaries. Verify and improve if needed.

#### Acceptance Criteria

- [ ] Review current chunk-reassembler.ts implementation
- [ ] Create test case: variable declared in chunk 1, used in chunk 2
- [ ] Verify: Cross-chunk references work correctly
- [ ] Create test case: function defined in chunk 1, called in chunk 2
- [ ] Verify: Function calls resolve correctly
- [ ] If issues found: Implement symbol table sharing between chunks
- [ ] If issues found: Add cross-chunk reference resolution
- [ ] Add tests for edge cases (symbol at exact chunk boundary)
- [ ] Document chunking behavior in CLAUDE.md
- [ ] Add `--debug-chunks` option to insert boundary markers in output

#### Technical Notes

**Current Implementation** (chunk-reassembler.ts):
```typescript
export function reassembleChunks(chunks: ProcessedChunk[]): string {
  return chunks
    .map((chunk, i) => {
      const marker = `\n// === Chunk ${i + 1}/${chunks.length} ===\n`;
      return debugMarkers ? marker + chunk.code : chunk.code;
    })
    .join('\n\n');
}
```

**Potential Issues**:
- Chunk 1: `const helper = () => { /* ... */ };`
- Chunk 2: `function main() { helper(); }` ‚Üê If "helper" renamed differently in each chunk, breaks

**Possible Enhancement**:
```typescript
interface SymbolTable {
  [originalName: string]: string; // original ‚Üí renamed mapping
}

export function reassembleChunks(
  chunks: ProcessedChunk[],
  symbolTable: SymbolTable
): string {
  // Verify cross-chunk references use consistent names
  for (const chunk of chunks) {
    for (const [original, renamed] of Object.entries(symbolTable)) {
      // If chunk references a symbol renamed in another chunk,
      // ensure consistency
      chunk.code = chunk.code.replace(
        new RegExp(`\\b${original}\\b`, 'g'),
        renamed
      );
    }
  }

  return chunks.map(c => c.code).join('\n\n');
}
```

**Testing**:
1. Create large file (>100KB) with cross-file references
2. Enable chunking
3. Verify output is syntactically valid
4. Verify cross-chunk calls work correctly
5. Run output through `node` to check runtime behavior

---

### P3-2: Add Iterative Refinement (3+ Passes)

**Status**: Not Started
**Effort**: Small (2-3 hours)
**Dependencies**: P0-2 (Fix refinement chaining)
**Spec Reference**: PROJECT_SPEC.md lines 8-10 ‚Ä¢ STATUS-2025-11-17-115400.md line 477

#### Description

Current `--refine` flag enables 2 passes. Allow arbitrary number of refinement iterations for extremely complex code that benefits from multiple passes.

#### Acceptance Criteria

- [ ] Add `--iterations N` CLI flag (default: 1, with --refine: 2)
- [ ] Update progress manager to handle N iterations
- [ ] Update display manager iteration colors (1=yellow, 2+=blue, 3+=cyan, etc.)
- [ ] Run plugin pipeline N times, each using previous output
- [ ] Track quality improvement across iterations
- [ ] Add early stopping: if quality doesn't improve, stop refining
- [ ] Test with 3, 4, 5 iterations on complex file
- [ ] Document diminishing returns (iteration 3+ usually minimal improvement)
- [ ] Add warning if user requests >3 iterations (cost, time)

#### Technical Notes

**Implementation**:
```typescript
const iterations = opts.iterations || (opts.refine ? 2 : 1);

for (let i = 1; i <= iterations; i++) {
  displayManager.showIterationHeader(i, iterations);
  progressManager.startIteration(i);

  const inputFile = i === 1 ? filename : getIterationOutput(outputDir, i - 1);

  await unminify(inputFile, outputDir, plugins, {
    skipWebcrack: i > 1, // Only webcrack on first iteration
    progressManager,
    displayManager
  });

  // Track quality
  if (i > 1) {
    const improvement = calculateQualityImprovement(
      getIterationOutput(outputDir, i - 1),
      getIterationOutput(outputDir, i)
    );

    if (improvement < 0.05) { // <5% improvement
      console.log(colorize(`\n‚ö†Ô∏è Minimal improvement in iteration ${i}, stopping early`, "yellow"));
      break;
    }
  }
}
```

**Early Stopping Logic**:
- Compare quality scores between iterations
- If improvement < 5%, stop (diminishing returns)
- Log reasoning to user
- Save cost and time

**Color Coding** (beyond iteration 2):
- Iteration 1: Yellow
- Iteration 2: Blue
- Iteration 3+: Cyan or magenta (visually distinct)

---

### P3-3: Add Performance Benchmarking Suite

**Status**: Not Started
**Effort**: Medium (4-5 hours)
**Dependencies**: P1-3 (Manual testing)
**Spec Reference**: CLAUDE.md lines 259-281

#### Description

Create automated benchmarking suite to track performance over time and compare different configurations (turbo vs sequential, different models, different concurrency levels).

#### Acceptance Criteria

- [ ] Create `benchmark/` directory with sample files (small, medium, large)
- [ ] Create benchmark runner script
- [ ] Measure: Total time, API calls, memory peak, cost estimate
- [ ] Compare: Sequential vs turbo mode
- [ ] Compare: Different concurrency levels (1, 5, 10, 20, 50)
- [ ] Compare: Different models (gpt-4o-mini, gpt-4o)
- [ ] Generate performance report (markdown or HTML)
- [ ] Add CI/CD integration (track performance over commits)
- [ ] Document baseline performance in CLAUDE.md
- [ ] Add `npm run benchmark` command

#### Technical Notes

**Benchmark Files**:
- `small.min.js`: 10KB, ~100 identifiers
- `medium.min.js`: 100KB, ~1,000 identifiers
- `large.min.js`: 1MB, ~10,000 identifiers
- `xlarge.min.js`: 10MB, ~100,000 identifiers

**Metrics to Track**:
```typescript
interface BenchmarkResult {
  file: string;
  fileSize: number;
  identifiers: number;
  configuration: {
    provider: string;
    model: string;
    turbo: boolean;
    maxConcurrent: number;
    refine: boolean;
  };
  performance: {
    totalTime: number; // seconds
    apiCalls: number;
    tokensProcessed: number;
    peakMemory: number; // MB
    estimatedCost: number; // USD
  };
  quality: {
    inputSingleLetterVars: number;
    outputSingleLetterVars: number;
    qualityScore: number;
  };
}
```

**Benchmark Report Example**:
```markdown
# HumanifyJS Performance Benchmarks

Date: 2025-11-17
Commit: abc123

## Small File (10KB, 100 identifiers)

| Config | Time | API Calls | Memory | Cost | Quality |
|--------|------|-----------|--------|------|---------|
| Sequential | 45s | 100 | 120MB | $0.05 | 85/100 |
| Turbo (10) | 12s | 100 | 180MB | $0.05 | 87/100 |
| Turbo (20) | 8s | 100 | 220MB | $0.05 | 87/100 |

**Recommendation**: Turbo mode with 10-20 concurrency

## Large File (1MB, 10K identifiers)

| Config | Time | API Calls | Memory | Cost | Quality |
|--------|------|-----------|--------|------|---------|
| Sequential | 2h 15m | 10,000 | 450MB | $5.00 | 82/100 |
| Turbo (10) | 18m | 10,000 | 890MB | $5.00 | 85/100 |
| Turbo (20) | 12m | 10,000 | 1.2GB | $5.00 | 85/100 |

**Recommendation**: Turbo mode with 20 concurrency (7.5x speedup)
```

---

## DEPENDENCY GRAPH

```
P0-1 (Investigate E2E test)
  ‚îî‚îÄ> P1-2 (Add output quality validation)
       ‚îî‚îÄ> P2-2 (Quality metrics and reports)
  ‚îî‚îÄ> P1-3 (Manual testing)
       ‚îî‚îÄ> P2-1 (Improve prompts)
       ‚îî‚îÄ> P2-3 (Document limitations)
       ‚îî‚îÄ> P3-3 (Performance benchmarking)

P0-2 (Fix refinement filename)
  ‚îî‚îÄ> P1-1 (Verify refinement chaining)
       ‚îî‚îÄ> P3-2 (Iterative refinement)

P3-1 (Improve chunking) - Independent
```

---

## RECOMMENDED SPRINT PLANNING

### Sprint 1: Critical Bug Fixes (1-2 days)

**Goal**: Fix blocking issues and verify core functionality works

**Tasks**:
- P0-1: Investigate E2E test failure (3-5 hours)
- P0-2: Fix refinement filename bug (2-3 hours)

**Deliverables**:
- Investigation report on E2E test failure
- Root cause identified (test wrong OR implementation broken)
- Refinement chaining works for multi-file bundles
- All tests passing (137/137)

**Risk**: If investigation reveals deep implementation bugs, may take longer

---

### Sprint 2: Verification and Quality (2-3 days)

**Goal**: Verify tool works end-to-end and provides value

**Tasks**:
- P1-1: Verify refinement chaining (2 hours)
- P1-2: Add output quality validation (3-4 hours)
- P1-3: Manual testing with real files (3-4 hours)

**Deliverables**:
- Comprehensive test suite for output quality
- Manual test report showing actual deobfuscation results
- Confidence in core functionality (either validated or bugs identified)

**Success Criteria**: Can confidently say "the tool produces semantic variable names"

---

### Sprint 3: Polish and Documentation (1-2 days)

**Goal**: Improve user experience and document behavior

**Tasks**:
- P2-1: Improve LLM prompts (3-4 hours) - if needed based on manual testing
- P2-2: Quality metrics and reports (4-5 hours)
- P2-3: Document limitations (2 hours)

**Deliverables**:
- Better LLM prompt engineering
- Quality comparison reports
- Comprehensive troubleshooting guide

**Success Criteria**: Users can understand output quality and troubleshoot issues

---

### Sprint 4: Enhancements (Optional, 2-3 days)

**Goal**: Nice-to-have improvements

**Tasks**:
- P3-1: Improve chunking reassembly (4-5 hours)
- P3-2: Iterative refinement (2-3 hours)
- P3-3: Performance benchmarking (4-5 hours)

**Deliverables**:
- Better chunking for cross-chunk references
- Support for 3+ refinement passes
- Automated performance benchmarks

**Priority**: Lower - only if time permits

---

## RISK ASSESSMENT

### High-Risk Items

**P0-1: E2E Test Investigation** (Risk: HIGH)
- **Uncertainty**: Root cause unknown (test vs implementation)
- **Impact**: If core pipeline broken, major rework needed
- **Mitigation**: Investigate first, create detailed action plan based on findings
- **Time Box**: 5 hours maximum - if no clear answer, escalate to user

**P1-3: Manual Testing** (Risk: MEDIUM)
- **Uncertainty**: May discover unexpected bugs or quality issues
- **Impact**: Could invalidate assumptions about implementation
- **Mitigation**: Start with simple cases, gradually increase complexity
- **Requirement**: Valid OpenAI API key (check .env file)

### Medium-Risk Items

**P0-2: Refinement Filename Fix** (Risk: MEDIUM)
- **Uncertainty**: May have unexpected edge cases
- **Impact**: Changes affect all 3 providers (OpenAI, Gemini, Local)
- **Mitigation**: Test thoroughly with single-file and multi-file bundles

**P2-1: Prompt Improvement** (Risk: MEDIUM)
- **Uncertainty**: May not significantly improve quality
- **Impact**: Cost of testing with multiple prompts
- **Mitigation**: Use small test files for iteration, document what works

### Low-Risk Items

**P1-1, P1-2, P2-2, P2-3**: Low risk - well-defined scope, clear acceptance criteria
**P3-1, P3-2, P3-3**: Very low risk - nice-to-have enhancements

---

## PLANNING FILE MAINTENANCE

### Files to Archive

Move to `.agent_planning/archive/`:
- STATUS-2025-11-17-064847.md (superseded by 074619)
- STATUS-2025-11-17-063410.md (superseded by 074619)
- PLAN-2025-11-17-150000.md (superseded by this file)
- PLAN-2025-11-17-134100.md (superseded by this file)
- PLANNING-SUMMARY-2025-11-17-150000.md (superseded)
- PLANNING-SUMMARY-2025-11-17-134100.md (superseded)

### Keep Active (Max 4 per prefix)

**STATUS files** (keep 4 most recent):
1. STATUS-2025-11-17-074619.md (latest evaluation)
2. STATUS-2025-11-17-115400.md (investigation report)
3. (Delete older ones)

**PLAN files** (keep 4 most recent):
1. PLAN-2025-11-17-075145.md (THIS FILE)
2. PLAN-2025-11-17-120000.md (refinement fix details)
3. (Delete older ones)

**PLANNING-SUMMARY files** (keep 4 most recent):
1. PLANNING-SUMMARY-2025-11-17-075145.md (will be created alongside this file)
2. PLANNING-SUMMARY-2025-11-17-120000.md
3. (Delete older ones)

---

## CONCLUSION

### Project Status: PROGRESS TRACKING COMPLETE, CORE FUNCTIONALITY NEEDS VERIFICATION

**What's Done** (High Confidence):
- ‚úÖ Work estimation before API calls (complete)
- ‚úÖ Global progress bar with ETA (complete)
- ‚úÖ Iteration display with color coding (complete)
- ‚úÖ No overlapping progress bars (complete)
- ‚úÖ Checkpoint system (fixed, working)

**What's Critical** (Needs Immediate Attention):
- ‚ùå E2E test failing (10/10 single-letter variables)
- ‚ùå Refinement chaining broken (hardcoded filename)
- ‚ùì Core deobfuscation quality (unverified)

**Recommendation**: Mark progress tracking work as COMPLETE and EXCELLENT. However, we cannot declare overall project success until:
1. E2E test failure investigated and resolved
2. Refinement chaining verified working
3. Manual testing confirms tool produces semantic variable names

**Total Estimated Effort**:
- Sprint 1 (Critical): 5-8 hours
- Sprint 2 (Verification): 8-10 hours
- Sprint 3 (Polish): 9-11 hours
- Sprint 4 (Optional): 10-13 hours

**Minimum to Production**: 13-18 hours (Sprints 1-2)
**Full Feature Complete**: 32-42 hours (All sprints)

---

**Next Action**: Investigate E2E test failure (P0-1) - this is the most critical unknown that affects confidence in core functionality.
