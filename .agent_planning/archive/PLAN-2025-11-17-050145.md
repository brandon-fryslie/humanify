# IMPLEMENTATION PLAN: Output Quality & Progress Tracking
**Generated**: 2025-11-17 05:01:45
**Source STATUS**: STATUS-2025-11-17-115400.md
**Spec Version**: PROJECT_SPEC.md + CLAUDE.md (last modified 2025-11-17)
**Purpose**: Address user's critical concerns about output quality and progress tracking

---

## EXECUTIVE SUMMARY

### Current State (from STATUS-2025-11-17-115400.md)

**What Works**:
- Core architecture is sound
- Plugin pipeline correctly implemented
- Data flow from API → AST → file is correct
- Checkpoint system fixed (commit 3eebd17)
- 96% test pass rate (223/227)

**What's Broken**:
- Cannot verify LLM provides semantic names (no API key testing yet)
- User reports output contains single-letter variables
- 4 turbo mode tests failing (return type mismatch)
- No global progress tracking across all work
- Unclear if refinement uses previous iteration output

**Critical Unknown**: Does the LLM actually provide semantic variable names?

---

## USER'S PRIMARY CONCERNS (from PROJECT_SPEC.md)

1. **"Output did not seem to be using the output of the deobfuscation in any way"**
   - Output contains single-letter variables after processing
   - Suspect LLM suggestions aren't being applied OR LLM provides poor quality names

2. **"There should be no single letter variables whatsoever when we are done"**
   - Explicit requirement: ZERO single-letter variables
   - Current output fails this test

3. **"Improving the estimation of work remaining" (global progress tracking)**
   - No indication of overall progress across all files/iterations
   - Only see per-batch progress (e.g., "Batch 3/47" without context)
   - Need to calculate total work BEFORE any API calls

4. **"Not deleting the checkpoints"**
   - **STATUS**: FIXED in commit 3eebd17 ✓

5. **Progress output quality issues**
   - Flickering between progress displays
   - Overlapping text
   - Hard to read
   - No color coding

6. **Refinement stage concerns**
   - May be re-running on original code instead of previous output
   - Need verification

---

## GAP ANALYSIS

### Architecture: COMPLETE ✓
- All components from spec are implemented
- Plugin chain works correctly
- AST transformations are sound

### Core Functionality: UNKNOWN ❓
- **Critical Gap**: Cannot verify LLM provides semantic names without testing
- **Evidence Missing**: No test shows LLM actually works end-to-end
- **Test Gap**: Tests verify API calls happen, not that output is improved

### Output Quality: BROKEN ❌
- User reports single-letter variables persist
- No quality validation in tests
- No metrics tracking variable improvement
- Prompts may be too generic

### Progress Tracking: INCOMPLETE ⚠️
- Per-batch progress exists
- Global progress missing
- No iteration tracking
- Display has flickering issues
- No ETA calculation

### Refinement: UNKNOWN ❓
- Code exists but behavior unclear
- May have hardcoded filename bug (brandon-fryslie_humanify-e7c)
- Need to verify it uses previous output

---

## PRIORITIZED BACKLOG

All work items tracked in beads (bd) system. Use `bd list` to see current status.

### P0 (CRITICAL) - Ship Blockers

These items MUST be completed before claiming the tool works. They address the fundamental question: "Does deobfuscation actually work?"

#### brandon-fryslie_humanify-ui2: Create diagnostic test to verify LLM provides semantic names
**Status**: Open | **Effort**: Medium (4-6 hours) | **Priority**: P0
**Dependencies**: Requires OpenAI API key

**Why P0**: This is the CRITICAL UNKNOWN. Without verifying the LLM provides semantic names, we cannot confirm the core value proposition works. Everything else is irrelevant if this doesn't work.

**Description**:
Create comprehensive diagnostic test that definitively answers: "Does the LLM provide semantic variable names?"

**Implementation**:
1. Create `src/diagnostic-llm-quality.llmtest.ts`
2. Test 1: Simple code (`function a(b,c){return b+c;}`) → verify LLM suggests semantic names
3. Test 2: Count single-letter variables before/after → must improve by >80%
4. Test 3: Log all LLM requests/responses to `diagnostic-llm-responses.json`
5. Test 4: FAIL if output has >30% single-letter variables

**Acceptance Criteria**:
- Test runs with `OPENAI_API_KEY=xxx npm run test:openai -- src/diagnostic-llm-quality.llmtest.ts`
- Console shows actual LLM responses
- Creates diagnostic file with all API calls
- Provides clear YES/NO answer: "Does LLM provide semantic names?"
- FAILS if output quality is poor (>30% single-letter vars)

**Spec Reference**: PROJECT_SPEC.md issues #2,#4 • **STATUS Reference**: STATUS-2025-11-17-115400.md lines 189-199, 360-402

---

#### brandon-fryslie_humanify-7kq: Fix 4 turbo mode test failures
**Status**: Open | **Effort**: Small (1-2 hours) | **Priority**: P0
**Dependencies**: None

**Why P0**: Test suite must show 100% pass rate before claiming stability. Easy fix that restores confidence.

**Description**:
4 tests fail with "result.includes is not a function" because turbo mode returns `{code, checkpointId}` object instead of string.

**Failed Tests**:
- src/plugins/local-llm-rename/turbo-mode.test.ts:16
- src/plugins/local-llm-rename/turbo-mode.test.ts:36
- src/plugins/openai/openai-turbo.test.ts:87
- src/plugins/openai/openai-turbo.test.ts:170

**Implementation**:
Update each test to extract `.code` property:
```typescript
// Before
const result = await visitAllIdentifiers(code, visitor, 200, undefined, { turbo: true });
assert.ok(result.includes("a_renamed"));

// After
const result = await visitAllIdentifiers(code, visitor, 200, undefined, { turbo: true });
const codeStr = typeof result === 'string' ? result : result.code;
assert.ok(codeStr.includes("a_renamed"));
```

**Acceptance Criteria**:
- All 4 tests pass
- Test suite shows 100% pass rate (227/227 → 231/231 after new tests)
- No changes to production code

**Spec Reference**: CLAUDE.md Testing section • **STATUS Reference**: STATUS-2025-11-17-115400.md lines 60-138

---

#### brandon-fryslie_humanify-6lh: Implement global progress tracking with iteration display
**Status**: Open | **Effort**: Large (1-2 weeks) | **Priority**: P0
**Dependencies**: brandon-fryslie_humanify-8jo (fix progress display chaos)

**Why P0**: User explicitly requested this and it's a major UX issue. "There is no indication whatsoever how much progress we are making."

**Description**:
Calculate total work upfront (before ANY API calls) and show:
- Iteration number (1 for first pass, 2+ for refinement) with color coding (yellow/blue)
- Global progress bar: 0-100% across ALL work
- Current item count: "15,000 / 20,000 identifiers"
- ETA in minutes
- Summary stats after each batch

**Implementation**:

**Phase 1: Calculate total work upfront**
```typescript
// In unminify.ts, after webcrack but BEFORE plugins:
const totalIdentifiers = await calculateTotalIdentifiers(extractedFiles);
const totalIterations = opts.refine ? 2 : 1;
const globalTotalWork = totalIdentifiers * totalIterations;

globalProgress.start(globalTotalWork, totalIterations);
```

**Phase 2: Create GlobalProgressTracker**
```typescript
// src/global-progress-tracker.ts
class GlobalProgressTracker {
  startIteration(num: number, total: number) // Yellow for 1, blue for 2+
  updateProgress(completed: number, message: string) // Update global bar
  finishBatch(stats: BatchStats) // Print summary
  getETA(): string // Calculate based on completion rate
}
```

**Phase 3: Integrate with existing progress**
- Replace `showPercentage()` calls with `globalProgress.updateProgress()`
- Report to global tracker from `visit-all-identifiers.ts`
- Add batch summary stats (tokens, avg time, size)

**Phase 4: Fix display issues**
- Use cli-progress MultiBar (global on top, batch on bottom)
- No flickering or overlap
- Clear formatting

**Acceptance Criteria**:
- Global progress shows BEFORE any API calls
- Displays "Iteration: N" with color (1=yellow, 2+=blue)
- Shows overall percentage and count ("15,000 / 20,000 identifiers")
- Shows ETA after first batch completes
- Per-batch progress on separate line (no overlap)
- Summary stats after each batch
- No flickering
- Manual test: Run large file, verify display is clean and readable

**Spec Reference**: PROJECT_SPEC.md issues #3,#5,#6 • **STATUS Reference**: STATUS-2025-11-17-115400.md lines 420-422

---

#### brandon-fryslie_humanify-cpx: Verify refinement uses previous iteration output
**Status**: Open | **Effort**: Medium (4-6 hours) | **Priority**: P0
**Dependencies**: brandon-fryslie_humanify-e7c (fix refinement hardcoded filename)

**Why P0**: User explicitly concerned about this. If refinement doesn't use previous output, it's completely broken and wasting API calls.

**Description**:
Verify (and fix if needed) that refinement pass 2 uses pass 1 output as input, not the original obfuscated code.

**Implementation**:

**Phase 1: Investigate**
- Read openai.ts lines 244-276
- Check what file is used as input for pass 2
- Current code: `const pass1OutputFile = ${opts.outputDir}/deobfuscated.js` (WRONG - hardcoded)

**Phase 2: Create verification test**
```typescript
// src/refinement-pipeline.test.ts
test("Pass 2 uses pass 1 output", async () => {
  // Pass 1: a → firstPass_a
  // Pass 2: firstPass_a → secondPass_firstPass_a (proves it used pass 1 output)
  // If broken: secondPass_a (proves it used original input)
});
```

**Phase 3: Fix if broken**
- Update openai.ts to read actual output files (not hardcoded name)
- Ensure refinement processes already-deobfuscated code
- Related to brandon-fryslie_humanify-e7c (multi-file refinement bug)

**Phase 4: Add visual indicator**
```
=== Pass 2: Refinement (processing PREVIOUS OUTPUT) ===
Input: deobfuscated code from pass 1
Files to refine: index.js, bundle_1.js, node_modules/1/index.js
```

**Acceptance Criteria**:
- Test proves pass 2 uses pass 1 output
- Test shows variable names CHANGE between passes
- Test proves refinement doesn't revert to original
- Clear log message shows what's being refined
- Manual test: Run with --refine --verbose, verify logs

**Spec Reference**: PROJECT_SPEC.md issue #3 • **STATUS Reference**: STATUS-2025-11-17-115400.md lines 464-478

---

### P1 (HIGH) - Quality Improvements

These items significantly improve output quality and user experience but don't block basic functionality.

#### brandon-fryslie_humanify-40s: Improve LLM prompts to eliminate single-letter variables
**Status**: Open | **Effort**: Medium (6-8 hours) | **Priority**: P1
**Dependencies**: brandon-fryslie_humanify-ui2 (diagnostic test must run first to confirm issue)

**Why P1**: Addresses user's core complaint ("There should be NO single letter variables"). However, must verify LLM works first (P0) before improving prompts.

**Description**:
Enhance LLM prompts with explicit requirements, examples, and post-processing validation to eliminate single-letter variables.

**Current Prompt** (too generic):
```
Rename Javascript variables/function `${name}` to have descriptive name based on their usage in the code.
```

**Enhanced Prompt**:
```typescript
const enhancedPrompt = `
You are an expert JavaScript developer analyzing obfuscated code to suggest semantic variable names.

CONTEXT:
${surroundingCode}

TASK: Rename the identifier "${name}" to a clear, descriptive name.

REQUIREMENTS:
1. Name must be camelCase (e.g., userData, processConfig, calculateTotal)
2. Name must describe the PURPOSE, not the type (prefer "userAge" over "numberValue")
3. Name must be 2-50 characters (NO single letters!)
4. Name must follow JavaScript naming conventions
5. If purpose is unclear, use descriptive placeholder (e.g., "unknownFunction1" not "a")

GOOD EXAMPLES:
- "x" → "coordinateX" (if used for x-axis)
- "a" → "addNumbers" (if function adds two numbers)
- "t" → "timestamp" (if stores Date.now())
- "cb" → "callback" or "onComplete"
- "cfg" → "configuration" or "userConfig"

BAD EXAMPLES:
- "x" → "x" (no change)
- "a" → "a1" (still not semantic)
- "callback" → "cb" (abbreviating is wrong direction)

PROVIDE:
{
  "newName": "descriptiveNameHere",
  "reasoning": "Brief explanation of why this name fits"
}
`;
```

**Additional Improvements**:
1. **Model upgrade**: Default to gpt-4o (full) instead of gpt-4o-mini
2. **Context window**: Increase from 100K to 200K for complex code
3. **Post-processing validation**: Reject single-letter names (retry up to 2x)
4. **Logging**: Save rejections to diagnostic output

**Implementation**:
- Update `toRenamePrompt()` in openai-rename.ts
- Apply same improvements to gemini-rename.ts and local-llm-rename.ts
- Add validation logic to reject poor suggestions
- Update CLAUDE.md with prompt strategy

**Acceptance Criteria**:
- Prompt explicitly forbids single-letter names
- Prompt provides 5+ good examples, 3+ bad examples
- Post-processing rejects single-letter suggestions
- Default model is gpt-4o (not mini)
- CLAUDE.md documents prompt strategy
- Manual test: Run diagnostic test, verify <10% single-letter vars

**Spec Reference**: PROJECT_SPEC.md issues #2,#4 • **STATUS Reference**: STATUS-2025-11-17-115400.md lines 447-458

---

#### brandon-fryslie_humanify-e7c: Fix refinement hardcoded filename
**Status**: Open | **Effort**: Large (12-17 hours, 2 days) | **Priority**: P1
**Dependencies**: None

**Why P1**: Refinement is completely broken for bundled files. High impact but not blocking basic functionality (can run without --refine).

**Description**:
Refinement assumes output file is 'deobfuscated.js' but webcrack creates files like 'bundle_1.js', 'index.js', etc. Need to discover actual files and process each one.

**Current (BROKEN)**:
```typescript
const pass1OutputFile = `${opts.outputDir}/deobfuscated.js`; // ← WRONG
await unminify(pass1OutputFile, opts.outputDir, [...]);
```

**Fixed**:
```typescript
const jsFiles = await findJsFilesRecursive(opts.outputDir);
for (const jsFile of jsFiles) {
  await unminify(jsFile, opts.outputDir, [...], { skipWebcrack: true });
}
```

**Implementation** (detailed design in issue):
- Phase 2a: Discover actual output files (2-3 hours)
- Phase 2b: Add skipWebcrack option (2-3 hours)
- Phase 2c: Apply fix to Gemini (1-2 hours)
- Phase 2d: Verify Local command (1 hour)
- Phase 2e: Fix validation for multi-file bundles (2-3 hours)
- Testing: E2E + manual tests (4-5 hours)

**Acceptance Criteria**:
- No hardcoded filename
- Discovers all .js files (including nested directories)
- Processes each file separately
- skipWebcrack option prevents re-bundling
- Validation checks all files
- E2E test verifies multi-file refinement
- Manual test with TensorFlow/Babylon bundles

**Spec Reference**: CLAUDE.md Refinement section • **STATUS Reference**: STATUS-2025-11-17-010000.md lines 76-123

---

#### brandon-fryslie_humanify-7dp: Fix checkpoint deletion timing
**Status**: Open | **Effort**: Large (15-21 hours, 2-3 days) | **Priority**: P1
**Dependencies**: None

**Why P1**: Data loss risk if any downstream plugin fails. Not immediately critical because the basic fix (3eebd17) prevents deletion during batch processing. This is about making it perfect.

**Description**:
Move checkpoint deletion from `visit-all-identifiers.ts` to `unminify.ts` AFTER successful file write. Currently checkpoints are deleted before prettier runs, risking data loss if write fails.

**Implementation** (detailed design in issue):
- Phase 1a: Modify plugin interface to support `{code, checkpointId}` return (1-2 hours)
- Phase 1b: Update visitAllIdentifiers return type (2-3 hours)
- Phase 1c: Update all rename plugins (3-4 hours)
- Phase 1d: Update unminify.ts to delete after write (3-4 hours)
- Phase 1e: Error handling & edge cases (2-3 hours)
- Testing: Unit + E2E + manual (4-5 hours)

**Acceptance Criteria**:
- Checkpoints NOT deleted in visit-all-identifiers.ts
- Checkpoints collected in unminify.ts
- Deletion happens AFTER fs.writeFile succeeds
- If write fails, checkpoints persist with helpful error
- E2E test verifies checkpoint persistence on failure
- Manual test: Kill process, verify checkpoint exists, resume works

**Spec Reference**: CLAUDE.md Checkpoint section • **STATUS Reference**: STATUS-2025-11-17-010000.md lines 27-73

---

### P2 (MEDIUM) - Polish & UX

These items improve user experience but don't affect core functionality.

#### brandon-fryslie_humanify-8jo: Fix progress display chaos
**Status**: Open | **Effort**: Medium (6-8 hours) | **Priority**: P2
**Dependencies**: None

**Why P2**: UX issue but doesn't affect functionality. User mentioned it but it's secondary to quality issues.

**Description**:
Multiple progress bars created simultaneously (webcrack, babel, rename per batch, prettier, repeated per chunk) causing overlapping, unreadable output.

**Implementation**:
1. Create `src/progress-manager.ts` with ProgressManager class
2. Use cli-progress MultiBar for single display
3. Three levels: Global → File → Batch
4. Auto-remove old bars when starting new ones
5. Support --no-progress flag

**Acceptance Criteria**:
- Only ONE multi-bar display shown
- Three levels visible: Global, File, Batch
- Old bars removed when new ones start
- Terminal display is clean
- No overlapping or flickering
- Manual test: Run large file, verify clean display

**Spec Reference**: CLAUDE.md Progress section • **STATUS Reference**: STATUS-2025-11-17-010000.md lines 270-332

---

#### brandon-fryslie_humanify-12m: Add global progress tracking with ETA
**Status**: Open | **Effort**: Medium (6-8 hours) | **Priority**: P3
**Dependencies**: brandon-fryslie_humanify-8jo (progress display must be fixed first)

**Why P3**: Nice to have but less critical than global progress bar (P0). ETA is a bonus feature.

**Description**:
Enhance ProgressManager with ETA calculation based on elapsed time and completion rate.

**Implementation**:
1. Add startTimes map to track when processing started
2. Calculate ETA: `(elapsedTime / completed) * remaining`
3. Show "File X of Y (ETA: Zm)" in global progress bar
4. Update ETA as work progresses

**Acceptance Criteria**:
- Global progress shows "File X of Y"
- Shows ETA in minutes
- ETA is reasonably accurate (within 20%)
- Updates as work progresses

**Spec Reference**: CLAUDE.md Progress section • **STATUS Reference**: STATUS-2025-11-17-010000.md lines 315-332

---

#### brandon-fryslie_humanify-ajh: Add E2E verification test
**Status**: Open | **Effort**: Medium (6-8 hours) | **Priority**: P2
**Dependencies**: None

**Why P2**: Would catch integration issues but current unit tests provide good coverage.

**Description**:
Create comprehensive E2E test that runs full CLI and verifies output quality (not just implementation details).

**Implementation**:
- Create `src/e2e-verification.e2etest.ts`
- Test 1: Full pipeline with bundled file
- Test 2: Webcrack bundle splitting
- Test 3: Checkpoint creation
- FAIL if output has >30% single-letter variables

**Acceptance Criteria**:
- Test runs full CLI (not mocked)
- Verifies output has semantic names
- Verifies webcrack works
- Verifies checkpoints created
- Runs in CI without API keys (local provider)
- Completes in <2 minutes

**Spec Reference**: CLAUDE.md Testing section • **STATUS Reference**: STATUS-2025-11-17-010000.md lines 410-457

---

### P3 (LOW) - Nice to Have

These items are improvements but not urgent.

#### brandon-fryslie_humanify-njm: Run LLM integration tests (OPTIONAL)
**Status**: Open | **Effort**: Small (1-2 hours setup) | **Priority**: P3
**Dependencies**: OpenAI/Gemini API keys or local model

**Description**: Run full integration test suite with actual API calls for end-to-end confidence.

**Acceptance Criteria**:
- Set up API keys
- Download local model
- Run `npm run test:llm`, `npm run test:openai`, `npm run test:gemini`
- All pass

---

#### brandon-fryslie_humanify-arj: Download and test very large files (OPTIONAL)
**Status**: Open | **Effort**: Medium (4 hours) | **Priority**: P3
**Dependencies**: None

**Description**: Test with TensorFlow.js (1.4MB) and Babylon.js (7.2MB) to verify production-scale performance.

**Acceptance Criteria**:
- Download via `just download-tensorflow` and `just download-babylon`
- Process both files successfully
- Memory stays <8GB
- Document performance

---

#### brandon-fryslie_humanify-s9s: Fix cache directory initialization edge case
**Status**: Open | **Effort**: Small (2-3 hours) | **Priority**: P3
**Dependencies**: None

**Description**: Cache fails on cloud-synced filesystems. Add mkdir -p logic.

**Acceptance Criteria**:
- Test passes: dependency-graph.test.ts:489
- Works on iCloud, Dropbox, OneDrive

---

#### brandon-fryslie_humanify-wiv: Adjust file splitter performance threshold
**Status**: Open | **Effort**: Trivial (30 min) | **Priority**: P3
**Dependencies**: None

**Description**: Update test threshold from 700% to 1500% to match actual performance.

**Acceptance Criteria**:
- Test passes: src/file-splitter.test.ts:323
- No changes to splitting logic

---

## DEPENDENCY GRAPH

```
P0 Items (Critical Path):
├─ ui2: Diagnostic test (no deps)
├─ 7kq: Fix 4 test failures (no deps)
├─ cpx: Verify refinement
│  └─ depends on: e7c (refinement filename fix)
└─ 6lh: Global progress tracking
   └─ depends on: 8jo (progress display fix)

P1 Items:
├─ 40s: Improve prompts
│  └─ should run after: ui2 (verify issue first)
├─ e7c: Fix refinement filename (no deps)
├─ 7dp: Checkpoint deletion timing (no deps)

P2 Items:
├─ 8jo: Fix progress display (no deps)
├─ 12m: ETA calculation
│  └─ depends on: 8jo
└─ ajh: E2E verification (no deps)

P3 Items:
└─ (all optional, no critical dependencies)
```

**Critical Path to Minimal Viable Quality**:
1. ui2 (Diagnostic test) - MUST VERIFY LLM WORKS
2. 7kq (Fix tests) - Quick win for 100% pass rate
3. 40s (Improve prompts) - Core quality issue
4. e7c (Fix refinement) - Makes refinement usable
5. cpx (Verify refinement) - Proves refinement works
6. 6lh (Global progress) - Major UX improvement

---

## RECOMMENDED SPRINT PLANNING

### Sprint 1: "Verify Core Functionality" (Week 1)
**Goal**: Definitively answer "Does the tool work?"

**Items**:
- brandon-fryslie_humanify-ui2 (Diagnostic test) - 4-6 hours
- brandon-fryslie_humanify-7kq (Fix 4 tests) - 1-2 hours
- **Deliverable**: Clear YES/NO on whether LLM provides semantic names
- **Decision Point**: If NO, must improve prompts before anything else

**Total Effort**: 5-8 hours (1 day)

---

### Sprint 2: "Quality Improvements" (Week 1-2)
**Goal**: Achieve user's quality bar (zero single-letter variables)

**Items**:
- brandon-fryslie_humanify-40s (Improve prompts) - 6-8 hours
- Run diagnostic test again to measure improvement
- **Deliverable**: <10% single-letter variables in output

**Total Effort**: 6-8 hours (1 day)

---

### Sprint 3: "Fix Refinement" (Week 2)
**Goal**: Make refinement feature work correctly

**Items**:
- brandon-fryslie_humanify-e7c (Fix hardcoded filename) - 12-17 hours
- brandon-fryslie_humanify-cpx (Verify refinement flow) - 4-6 hours
- **Deliverable**: Refinement processes previous output, handles multi-file bundles

**Total Effort**: 16-23 hours (2-3 days)

---

### Sprint 4: "Global Progress Tracking" (Week 3)
**Goal**: Give user visibility into overall progress

**Items**:
- brandon-fryslie_humanify-8jo (Fix display chaos) - 6-8 hours
- brandon-fryslie_humanify-6lh (Global progress) - 8-12 hours
- **Deliverable**: Clean progress display with iteration tracking and global percentage

**Total Effort**: 14-20 hours (2-3 days)

---

### Sprint 5: "Polish" (Week 3-4)
**Goal**: Address remaining issues

**Items**:
- brandon-fryslie_humanify-7dp (Checkpoint timing) - 15-21 hours
- brandon-fryslie_humanify-ajh (E2E verification) - 6-8 hours
- brandon-fryslie_humanify-12m (ETA calculation) - 6-8 hours
- **Deliverable**: Production-ready tool with excellent UX

**Total Effort**: 27-37 hours (3-5 days)

---

## RISK ASSESSMENT

### High-Risk Items

**1. Diagnostic Test May Reveal LLM Doesn't Work (brandon-fryslie_humanify-ui2)**
- **Risk**: LLM provides poor quality names even with good prompts
- **Impact**: Core value proposition broken
- **Mitigation**:
  - Test with multiple models (gpt-4o, gpt-4o-mini, Claude)
  - Increase context window
  - Try different prompt strategies
  - May need to implement post-processing heuristics
- **Contingency**: If LLM quality is fundamentally poor, may need to:
  - Add manual override/hints file
  - Implement local symbol analysis for better context
  - Use multiple LLM passes with voting

**2. Refinement May Need Major Rework (brandon-fryslie_humanify-e7c)**
- **Risk**: Refinement architecture may have deeper issues than just hardcoded filename
- **Impact**: Large time sink (could balloon to 30+ hours)
- **Mitigation**:
  - Break into smaller phases with validation at each step
  - Add comprehensive tests before making changes
  - Document current behavior thoroughly
- **Contingency**: If refinement is too broken, consider removing feature temporarily

**3. Global Progress Calculation May Be Expensive (brandon-fryslie_humanify-6lh)**
- **Risk**: Parsing all ASTs upfront may add significant startup time (>5s for large files)
- **Impact**: Poor UX (long pause before processing starts)
- **Mitigation**:
  - Use lightweight AST parser for counting only
  - Cache identifier counts per file
  - Show "Analyzing..." progress during calculation
- **Contingency**: Estimate instead of calculate (e.g., 500 bytes per identifier heuristic)

---

## TESTING STRATEGY

### For Each Work Item

1. **Unit Tests**: Test individual functions in isolation
2. **Integration Tests**: Test component interactions
3. **E2E Tests**: Test full CLI pipeline
4. **Manual Tests**: Run on real-world examples

### Critical Test Cases

**Quality Verification**:
- Input with 100% single-letter variables
- Output must have <10% single-letter variables
- Measure improvement score (semantic names / total names)

**Refinement Verification**:
- Pass 1 output has names like "renamed_var1"
- Pass 2 uses pass 1 output (not original)
- Pass 2 output has different names (proves refinement happened)

**Progress Tracking Verification**:
- Global progress calculated before API calls
- Progress updates correctly as batches complete
- ETA becomes more accurate over time
- Display is clean (no flickering or overlap)

---

## QUALITY STANDARDS

### Output Quality
- **Target**: 0% single-letter variables (user requirement)
- **Acceptable**: <10% single-letter variables
- **Unacceptable**: >30% single-letter variables

### Test Coverage
- **Target**: 100% test pass rate
- **Current**: 96% (223/227)
- **Minimum**: 100% (all tests must pass)

### Progress Display
- **Target**: Clean, readable, informative
- **Must Have**: No flickering, no overlap, color coding
- **Nice to Have**: ETA, batch stats, global percentage

### Documentation
- **Target**: All features documented in CLAUDE.md
- **Must Have**: Code comments explain complex logic
- **Nice to Have**: Troubleshooting guide for poor output

---

## FILE MANAGEMENT PLAN

### Planning Files to Keep (Max 4 per prefix)
- PLAN-2025-11-17-050145.md (this file)
- PLAN-2025-11-17-120000.md
- PLAN-2025-11-17-022500.md
- *Delete oldest if more exist*

### Planning Files to Archive
- PLAN-2025-11-16-* (older than 2 days)

### STATUS Files to Keep (Max 4)
- STATUS-2025-11-17-115400.md (latest)
- STATUS-2025-11-17-021529.md
- STATUS-2025-11-17-010000.md
- *Delete oldest if more exist*

### Work Tracking
- All active work items tracked in beads (bd) system
- Use `bd show <issue-id>` for detailed design docs
- Use `bd list --status open` to see current work
- Use `bd stats` to see overall progress

---

## CONCLUSION

### Project Health: YELLOW (Functional but Quality Unverified)

**What Must Happen Before Shipping**:
1. ✅ Verify LLM provides semantic names (brandon-fryslie_humanify-ui2)
2. ✅ Achieve <10% single-letter variables (brandon-fryslie_humanify-40s)
3. ✅ Fix refinement to use previous output (brandon-fryslie_humanify-cpx)
4. ✅ Add global progress tracking (brandon-fryslie_humanify-6lh)

**Critical Question**:
**"Does the LLM actually provide semantic variable names?"**

Until we answer this with a definitive YES (via brandon-fryslie_humanify-ui2), we cannot claim the tool works. Everything else is secondary.

**Confidence Assessment**:
- **Implementation Quality**: 95% (code is well-structured)
- **Functionality**: 30% (cannot verify without testing)
- **Output Quality**: 10% (user reports poor results)

**Next Actions** (in order):
1. Get OpenAI API key
2. Run diagnostic test (brandon-fryslie_humanify-ui2)
3. If PASS: Improve prompts (brandon-fryslie_humanify-40s)
4. If FAIL: Investigate root cause (prompt? model? context? architecture?)
5. Fix test failures (brandon-fryslie_humanify-7kq)
6. Implement global progress (brandon-fryslie_humanify-6lh)
7. Fix refinement (brandon-fryslie_humanify-e7c + brandon-fryslie_humanify-cpx)

**Timeline Estimate**: 2-4 weeks to address all P0 and P1 items.

---

**Report Generated**: 2025-11-17 05:01:45
**Status**: PLANNING COMPLETE - READY FOR IMPLEMENTATION
**Recommendation**: BEGIN WITH DIAGNOSTIC TEST (brandon-fryslie_humanify-ui2) IMMEDIATELY
