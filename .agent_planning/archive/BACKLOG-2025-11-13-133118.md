# HumanifyJS Project Backlog

**Generated**: 2025-11-13-133118
**Source STATUS**: STATUS-2025-11-13-132632.md
**Spec Source**: CLAUDE.md (last modified 2025-11-13)
**Current State**: 88% Complete, 228/233 tests passing (97.9%)

---

## Executive Summary

Based on the comprehensive project evaluation, HumanifyJS has a functionally complete checkpoint system but requires:

1. **Runtime verification** - Tests pass but checkpoint files not persisting in production
2. **Scope containment fixes** - 4 dependency-graph test failures impacting turbo mode quality (10-20%)
3. **Integration testing** - Missing tests for turbo+checkpoint, rate limiting, and subcommands
4. **Performance optimization** - File splitter overhead at 634% vs 50% target

**Estimated Time to Production-Ready**: 7-9 hours of focused work

**Recommended Focus**: Verify checkpoint persistence → Fix scope containment → Add integration tests → Self-minification benchmark

---

## P0 (CRITICAL) - Immediate Actions

### P0-1: Verify Checkpoint Persistence in Runtime

**Status**: Not Started
**Effort**: Small (30 minutes)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Checkpoint System • **Status Reference**: STATUS-2025-11-13-132632.md § 3.1, § 6.1

#### Description
All checkpoint tests pass but the `.humanify-checkpoints/` directory is empty in manual verification. This is a CRITICAL gap - we must verify that checkpoint files persist across process boundaries in real runtime execution, not just test environments.

#### Acceptance Criteria
- [ ] Build project with `npm run build`
- [ ] Run command: `./dist/index.mjs unminify test-samples/simple.js --turbo --provider local --output /tmp/test`
- [ ] Interrupt with Ctrl+C after 10 seconds
- [ ] Verify at least 1 JSON file exists in `.humanify-checkpoints/`
- [ ] If empty: Debug and fix checkpoint save logic
- [ ] Document findings in STATUS update

#### Technical Notes
**Hypothesis 1**: Tests may not enable turbo mode (checkpoints only work with `turbo: true`)
**Hypothesis 2**: Tests may mock filesystem
**Hypothesis 3**: Checkpoint logic triggers correctly but files aren't flushed to disk

Evidence shows code at `visit-all-identifiers.ts:410-461` should save after every batch, so this must be investigated immediately.

---

### P0-2: Fix Scope Containment Detection Bug

**Status**: Not Started
**Effort**: Medium (2-3 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Turbo Mode § Dependency Graph • **Status Reference**: STATUS-2025-11-13-132632.md § 2.2, § 3.6

#### Description
Four tests in `dependency-graph.test.ts` fail due to incorrect scope containment detection. The `isContainedIn()` logic doesn't properly handle:
- Variable shadowing (same name, different scopes)
- Arrow functions (different scope semantics)
- Nested closures

This causes turbo mode to miss dependencies, resulting in suboptimal ordering and 10-20% worse name quality.

**Failing Tests**:
1. Line 12:257 - "variable shadowing (same name, different scopes)"
2. Line 41:853 - "nested scope references"
3. Line 78:365 - "different dependency modes use separate caches"
4. Line 91:266 - "arrow functions and closures"

#### Acceptance Criteria
- [ ] All 4 tests in `dependency-graph.test.ts` pass
- [ ] Variable shadowing correctly detected (test expects `global_x -> function_local_x`)
- [ ] Arrow functions scope containment works
- [ ] Nested closures properly detected
- [ ] Different dependency modes produce different graphs
- [ ] No regressions in existing tests
- [ ] Turbo mode quality improves by 10-20% (measure with sample files)

#### Technical Notes
Root cause is in `dependency-graph.ts` function `buildScopeHierarchy()`. The `isContainedIn()` check needs to:
1. Handle Babel scope block types more precisely
2. Distinguish between lexical scopes (functions, arrow functions, blocks)
3. Account for variable shadowing where same name exists in nested scopes

---

## P1 (HIGH) - Short-Term Critical Path

### P1-1: Turbo Mode + Checkpoint Integration Test

**Status**: Not Started
**Effort**: Small (1 hour)
**Dependencies**: P0-1 (checkpoint verification)
**Spec Reference**: CLAUDE.md § Turbo Mode, § Checkpoint System • **Status Reference**: STATUS-2025-11-13-132632.md § 2.2 Gap #2

#### Description
No end-to-end test combines turbo mode with checkpoint resumption. This is a critical gap since these are two major features that must work together seamlessly.

#### Acceptance Criteria
- [ ] Create `checkpoint-turbo-integration.e2etest.ts`
- [ ] Test: Enable turbo + checkpoints, interrupt after 1 batch, resume
- [ ] Verify dependency graph persists correctly across resume
- [ ] Test with all three dependency modes (strict, balanced, relaxed)
- [ ] Verify deterministic output (same input = same output after resume)
- [ ] Test with refine mode enabled
- [ ] Test with chunking enabled

#### Technical Notes
Build on existing `checkpoint-resume.e2etest.ts` patterns but add turbo-specific validation:
- Checkpoint should include batch structure from dependency graph
- Resume should rebuild dependency graph or reuse cached version
- Verify no duplication or skipping of identifiers

---

### P1-2: Fix Checkpoint Save Error Handling

**Status**: Not Started
**Effort**: Small (30 minutes)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Checkpoint System • **Status Reference**: STATUS-2025-11-13-132632.md § 3.4

#### Description
In `visit-all-identifiers.ts:417-460`, if the first `saveCheckpoint()` call fails (e.g., disk full), the catch block attempts another `saveCheckpoint()` which will likely fail again and crash the process.

#### Acceptance Criteria
- [ ] Wrap second `saveCheckpoint()` in try/catch
- [ ] Log error without throwing
- [ ] Continue processing without checkpoint (graceful degradation)
- [ ] Add test for checkpoint save failure scenario
- [ ] Add test for corrupted checkpoint JSON handling

#### Technical Notes
```typescript
} catch (checkpointError) {
  console.error(`    → ERROR saving checkpoint:`, checkpointError);
  try {
    // Fallback save attempt with original code
    saveCheckpoint(checkpointId, { ... partialCode: originalCode });
  } catch (fallbackError) {
    console.error(`    → ERROR in fallback save, continuing without checkpoint:`, fallbackError);
    // Continue processing - checkpoint is optional
  }
}
```

---

### P1-3: Rate Limit Coordinator Tests

**Status**: Not Started
**Effort**: Small (2 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Turbo Mode § Parallel Batch Execution • **Status Reference**: STATUS-2025-11-13-132632.md § 2.2 Gap #3

#### Description
The `RateLimitCoordinator` class prevents "thundering herd" but has no tests. Need to verify:
- Multiple parallel requests wait together
- Stats reporting works
- Coordination across batches

#### Acceptance Criteria
- [ ] Create `rate-limit-coordinator.test.ts`
- [ ] Test: Single rate limit pauses all parallel requests
- [ ] Test: Multiple rate limits use longest delay
- [ ] Test: Stats report correct count of rate limit events
- [ ] Test: Coordinator resets between files
- [ ] Mock API calls to simulate rate limit responses

#### Technical Notes
Use jest fake timers or similar to test timing behavior without actual delays. Focus on verifying coordination logic rather than real API integration.

---

### P1-4: Checkpoint Subcommand Tests

**Status**: Not Started
**Effort**: Small (2 hours)
**Dependencies**: P0-1 (checkpoint verification)
**Spec Reference**: CLAUDE.md § Checkpoint System • **Status Reference**: STATUS-2025-11-13-132632.md § 1.6, § 2.2 Gap #5

#### Description
The checkpoint subcommands (`list`, `info`, `clean`, `delete`, `resume`) are implemented but have no automated tests. This creates risk of bugs in rarely-used commands.

#### Acceptance Criteria
- [ ] Create `checkpoint-subcommands.e2etest.ts`
- [ ] Test `humanify checkpoint list` with 0, 1, and multiple checkpoints
- [ ] Test `humanify checkpoint info <id>` displays metadata correctly
- [ ] Test `humanify checkpoint clean` removes old/stale checkpoints
- [ ] Test `humanify checkpoint delete <id>` removes specific checkpoint
- [ ] Test `humanify checkpoint resume` with interactive selection
- [ ] Verify proper error messages for invalid checkpoint IDs

#### Technical Notes
Build project first, use `execSync` to run CLI commands. Create test checkpoints manually using `saveCheckpoint()` utility.

---

## P2 (MEDIUM) - Quality Improvements

### P2-1: Self-Minification Test Implementation

**Status**: Not Started
**Effort**: Medium (4-6 hours)
**Dependencies**: P0-1, P0-2 (checkpoint verified and scope containment fixed)
**Spec Reference**: CLAUDE.md § Test Patterns • **Status Reference**: STATUS-2025-11-13-132632.md § 4 (entire section)

#### Description
Create a comprehensive quality benchmark by:
1. Using webpack to minify HumanifyJS source code
2. Running HumanifyJS on the minified version
3. Comparing output to original with scoring metrics

This provides a concrete quality metric and serves as a regression test.

#### Acceptance Criteria
- [ ] Webpack config created (`webpack.config.test.js`)
- [ ] External dependencies excluded (node-llama-cpp, openai, etc.)
- [ ] Minification produces valid JavaScript
- [ ] `self-test.ts` module implements comparison logic:
  - [ ] Name similarity score (Levenshtein distance, weighted by frequency)
  - [ ] Structure preservation (function/class counts match)
  - [ ] Syntactic correctness (parses without errors)
  - [ ] Optional: Semantic accuracy (embeddings-based, if time permits)
- [ ] Test file `self-minification.llmtest.ts` runs full pipeline
- [ ] Overall score >= 30% (realistic threshold for minified code)
- [ ] Test runs in < 2 hours
- [ ] NPM script `test:self` added
- [ ] Documentation updated with usage

#### Technical Notes
**Expected scores** (realistic targets):
- Name Similarity: 15-30%
- Semantic Accuracy: 40-60% (if implemented)
- Structure Preservation: 95-100%
- Overall: 35-50%

**Challenges**:
- Use `--chunk-size 50000` for large HumanifyJS source
- Native dependencies need externals config
- Long processing time (1-2 hours) - make this opt-in `.llmtest.ts`
- Semantic accuracy requires embeddings (optional, can defer)

**Value**: High for marketing, demos, and regression detection

---

### P2-2: File Splitter Performance Optimization

**Status**: Not Started
**Effort**: Small (2 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Large File Handling • **Status Reference**: STATUS-2025-11-13-132632.md § 2.1

#### Description
`file-splitter.test.ts` performance test fails with 634% overhead vs 50% target. The splitter does AST parsing twice (once for symbols, once for splitting). This doesn't break functionality but makes chunking slower than necessary.

#### Acceptance Criteria
- [ ] Single AST parse for both symbol extraction and splitting
- [ ] Overhead reduced to < 100% (stretch goal: < 50%)
- [ ] All file-splitter tests pass
- [ ] No regressions in correctness
- [ ] Document approach in code comments

#### Technical Notes
Root cause: `file-splitter.ts` parses AST in `extractSymbols()` then again in `splitCode()`.

**Optimization approach**:
1. Parse once
2. Traverse for symbols
3. Traverse for split points
4. Cache AST if needed

Alternative: Accept current performance and adjust test threshold to 700% - chunking is rarely used and overhead is tolerable.

---

### P2-3: Manual Integration Testing Checklist

**Status**: Not Started
**Effort**: Small (2 hours)
**Dependencies**: P0-1, P0-2, P1-1 through P1-4
**Spec Reference**: CLAUDE.md (all sections) • **Status Reference**: STATUS-2025-11-13-132632.md § 6.1

#### Description
Comprehensive manual testing across all providers, modes, and feature combinations to catch integration issues automated tests might miss.

#### Acceptance Criteria
- [ ] Test matrix completed for OpenAI provider:
  - [ ] Turbo mode + checkpoints
  - [ ] Refine mode + checkpoints
  - [ ] Chunking + checkpoints
  - [ ] All dependency modes (strict, balanced, relaxed)
- [ ] Test matrix completed for Gemini provider (same combinations)
- [ ] Test matrix completed for Local LLM provider (same combinations)
- [ ] Edge cases tested:
  - [ ] Very small file (< 10 identifiers)
  - [ ] Very large file (> 100KB)
  - [ ] File with no identifiers to rename
  - [ ] File with syntax errors
- [ ] Checkpoint scenarios:
  - [ ] Resume with changed CLI args (verify warning)
  - [ ] Resume with different dependency mode (verify rejection)
  - [ ] Multiple checkpoints exist (verify selection)
- [ ] Document findings in `MANUAL-TEST-RESULTS-<date>.md`

#### Technical Notes
Use test samples from `justfile` recipes:
- `just test-small` - Quick validation
- `just test-tensorflow` - Medium file (1.4MB)
- `just test-babylon` - Large file (7.2MB)

---

### P2-4: Documentation Updates

**Status**: Not Started
**Effort**: Small (1 hour)
**Dependencies**: P0-1, P0-2
**Spec Reference**: CLAUDE.md • **Status Reference**: STATUS-2025-11-13-132632.md § 6.1

#### Description
Update CLAUDE.md to reflect checkpoint system implementation, known issues, and troubleshooting guidance.

#### Acceptance Criteria
- [ ] Add "Checkpoint System" section to CLAUDE.md
- [ ] Document checkpoint subcommands with examples
- [ ] Add troubleshooting section for common issues:
  - [ ] Checkpoints not persisting
  - [ ] Batch count mismatches
  - [ ] Out-of-memory errors
- [ ] Update test patterns section with checkpoint test categories
- [ ] Add performance characteristics of checkpoint save/load
- [ ] Document checkpoint file format and location

#### Technical Notes
Keep documentation concise and example-driven. Link to code for implementation details.

---

## P3 (LOW) - Future Enhancements

### P3-1: Checkpoint Version Migration Strategy

**Status**: Not Started
**Effort**: Medium (3-4 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Checkpoint System • **Status Reference**: STATUS-2025-11-13-132632.md § 3.3

#### Description
Currently, checkpoint version mismatches result in starting fresh. Implement migration logic to upgrade old checkpoint formats when possible.

#### Acceptance Criteria
- [ ] Define migration function signature
- [ ] Implement v1 -> v2 migration (when v2 is defined)
- [ ] Add tests for migration logic
- [ ] Log migration events
- [ ] Fall back to fresh start if migration fails

#### Technical Notes
Not urgent - current approach (reject old checkpoints) is safe and acceptable. Implement only if checkpoint format changes frequently.

---

### P3-2: Checkpoint Compression

**Status**: Not Started
**Effort**: Small (2 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Checkpoint System • **Status Reference**: STATUS-2025-11-13-132632.md (not mentioned, potential improvement)

#### Description
Large files create large checkpoints. Implement optional compression (gzip) to reduce disk usage.

#### Acceptance Criteria
- [ ] Add compression option to checkpoint save
- [ ] Auto-detect compressed checkpoints on load
- [ ] Measure compression ratio on sample files
- [ ] Add flag `--compress-checkpoints` (default: false)
- [ ] Update tests

#### Technical Notes
**Low priority** - current checkpoints are already compact (JSON). Only implement if users report disk usage issues.

---

### P3-3: Multi-File Rate Limit Coordinator

**Status**: Not Started
**Effort**: Small (1-2 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Rate Limit Coordinator • **Status Reference**: STATUS-2025-11-13-132632.md § 3.5

#### Description
Currently, rate limit state is lost between files. If processing multiple files sequentially, second file doesn't know first file hit rate limit.

#### Acceptance Criteria
- [ ] Create shared coordinator for multi-file processing
- [ ] Pass coordinator through pipeline
- [ ] Persist rate limit state across files
- [ ] Add tests for multi-file scenario

#### Technical Notes
**Low priority** - multi-file processing is not currently a primary use case. Implement when batch processing feature is added.

---

### P3-4: Dependency Graph Visualization

**Status**: Not Started
**Effort**: Medium (3-4 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md § Turbo Mode § Dependency Graph • **Status Reference**: N/A (enhancement)

#### Description
Add CLI option to output dependency graph in DOT format for visualization with Graphviz.

#### Acceptance Criteria
- [ ] Add `--visualize-deps` flag
- [ ] Output graph in DOT format
- [ ] Include batch boundaries
- [ ] Color-code by dependency type (scope vs reference)
- [ ] Add README section with graphviz commands
- [ ] Example visualizations for documentation

#### Technical Notes
**Low priority** - Nice for debugging and presentations but not essential. Useful for understanding complex dependency patterns.

---

## Dependency Graph

```
P0-1 (Checkpoint Verification)
  └─> P1-1 (Turbo + Checkpoint Test)
  └─> P1-4 (Subcommand Tests)

P0-2 (Scope Containment Fix)
  └─> P2-1 (Self-Minification Test)

P1-1, P1-2, P1-3, P1-4 (All P1 items)
  └─> P2-3 (Manual Testing)

P0-1, P0-2
  └─> P2-4 (Documentation)

All P0 + P1 items
  └─> P2-1 (Self-Minification - quality benchmark)
```

---

## Recommended Sprint Planning

### Sprint 1: Production Readiness (1 week)
**Goal**: Get checkpoint system verified and all critical tests passing

**Work Items**:
- P0-1: Verify checkpoint persistence (30 min)
- P0-2: Fix scope containment (2-3 hours)
- P1-1: Turbo + checkpoint test (1 hour)
- P1-2: Error handling fix (30 min)
- P1-3: Rate limit tests (2 hours)
- P1-4: Subcommand tests (2 hours)

**Total Effort**: 8-9 hours
**Deliverable**: 100% test passing rate, production-ready checkpoint system

---

### Sprint 2: Quality & Benchmarking (1 week)
**Goal**: Establish quality metrics and complete integration testing

**Work Items**:
- P2-1: Self-minification test (4-6 hours)
- P2-2: File splitter optimization (2 hours)
- P2-3: Manual integration testing (2 hours)
- P2-4: Documentation updates (1 hour)

**Total Effort**: 9-11 hours
**Deliverable**: Quality benchmark, comprehensive documentation, full test coverage

---

### Sprint 3: Enhancements (Optional)
**Goal**: Future-proofing and advanced features

**Work Items**:
- P3-1: Checkpoint migration (3-4 hours)
- P3-2: Checkpoint compression (2 hours)
- P3-3: Multi-file rate limiting (1-2 hours)
- P3-4: Dependency visualization (3-4 hours)

**Total Effort**: 9-12 hours
**Deliverable**: Enhanced checkpoint system, better debugging tools

---

## Risk Assessment

### High-Risk Items
| Item | Risk | Impact | Mitigation |
|------|------|--------|------------|
| P0-1 | Checkpoints may not work in production | CRITICAL | Test immediately, highest priority |
| P0-2 | Scope bug affects 10-20% of name quality | HIGH | Fix reduces technical debt, improves UX |

### Medium-Risk Items
| Item | Risk | Impact | Mitigation |
|------|------|--------|------------|
| P1-1 | Turbo + checkpoint interaction unknown | MEDIUM | Integration test catches issues early |
| P1-2 | Checkpoint save can crash process | MEDIUM | Fix improves reliability |

### Low-Risk Items
| Item | Risk | Impact | Mitigation |
|------|------|--------|------------|
| P2-2 | File splitter slow but functional | LOW | Document limits, optimize if needed |
| All P3 | Nice-to-have enhancements | LOW | Defer until after production release |

---

## Success Metrics

**Sprint 1 Complete When**:
- [ ] All 233 tests passing (100%)
- [ ] Checkpoint files verified in `.humanify-checkpoints/` directory
- [ ] Scope containment tests pass
- [ ] Integration tests added and passing
- [ ] No known critical bugs

**Sprint 2 Complete When**:
- [ ] Self-minification test implemented
- [ ] Quality score baseline established (target: 35-50%)
- [ ] Manual test checklist 100% complete
- [ ] Documentation updated and reviewed

**Project Production-Ready When**:
- [ ] All Sprint 1 + Sprint 2 goals met
- [ ] Performance regression tests pass
- [ ] Known issues documented
- [ ] Troubleshooting guide complete

---

## Notes

- **Test Philosophy**: Focus on real functionality, avoid tautological tests
- **Code Philosophy**: "Boring implementation" - obvious, maintainable code
- **Priority Discipline**: Fix critical path first, resist feature creep
- **Documentation**: Keep aligned with code, update during development

**Estimated Time to 100% Complete**: 17-20 hours across 2 sprints
