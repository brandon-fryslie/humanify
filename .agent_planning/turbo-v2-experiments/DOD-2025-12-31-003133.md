# Definition of Done: Turbo-V2 Experiment Registry (MVP)

**Generated**: 2025-12-31-003133
**Plan**: PLAN-2025-12-31-003133.md
**Sprint Goal**: Deliver TypeScript-based experiment system replacing ad-hoc bash scripts

---

## Acceptance Criteria

### Deliverable 1: Experiment Registry & Storage (P0-1)

- [ ] TypeScript types defined for `ExperimentConfig` and `ExperimentResult` in `src/experiments/types.ts`
- [ ] Registry storage implemented in `src/experiments/registry.ts` with JSON persistence
- [ ] Registry supports create, read, list operations
- [ ] Experiments have unique IDs: `exp-{name}-{timestamp}` format
- [ ] Storage structure follows:
  ```
  .humanify-experiments/
    registry.json              # All experiment configs
    results/
      exp-baseline-*/
        {sample}-result.json   # Per-sample results
  ```
- [ ] Unit tests verify create/read/list operations (no duplicates, proper validation)
- [ ] Registry handles missing/corrupted files gracefully (error messages, no crash)

---

### Deliverable 2: Experiment Runner (P0-2)

- [ ] Experiment runner implemented in `src/experiments/runner.ts`
- [ ] Runner accepts experiment ID, resolves config from registry
- [ ] For each sample in experiment config:
  - [ ] Invokes turbo-v2 with correct preset/passes
  - [ ] Captures output path
  - [ ] Triggers semantic scoring via `scripts/score-semantic.ts`
  - [ ] Parses score JSON and stores in result
  - [ ] Handles errors gracefully (partial failures don't crash)
- [ ] Results written to `.humanify-experiments/results/{exp-id}/{sample}-result.json`
- [ ] Status updates persisted to registry (pending → running → complete/failed)
- [ ] Unit tests verify correct turbo-v2 invocation and result storage
- [ ] E2E test: run experiment on `tiny-qs` sample, verify output and score exist

---

### Deliverable 3: CLI Integration (P0-3)

#### `experiment define` Command
- [ ] CLI command `humanify experiment define` implemented with flags:
  - [ ] `--name` (required): User-friendly name
  - [ ] `--preset` (required): Turbo-v2 preset (fast/balanced/quality/anchor)
  - [ ] `--samples` (required): Comma-separated list or "all"
  - [ ] `--description` (optional): Text description
  - [ ] `--tags` (optional): Comma-separated tags
  - [ ] Validates preset exists in turbo-v2 presets
  - [ ] Validates samples exist in test-samples/canonical/
  - [ ] Outputs experiment ID on success

#### `experiment run` Command
- [ ] CLI command `humanify experiment run <experiment-id>` implemented:
  - [ ] Accepts experiment ID (exact match or prefix match if unique)
  - [ ] Shows experiment config summary before running
  - [ ] Displays progress (current sample X/Y)
  - [ ] Shows per-sample results (score, time, cost) as they complete
  - [ ] Outputs summary on completion (total time, avg score, total cost)

#### `experiment list` Command
- [ ] CLI command `humanify experiment list` implemented:
  - [ ] Shows table: ID | Name | Preset | Samples | Status | Avg Score
  - [ ] Supports `--tag` filter
  - [ ] Supports `--status` filter (pending/complete/failed)

#### `experiment compare` Command
- [ ] CLI command `humanify experiment compare <id1> <id2> [id3...]` implemented:
  - [ ] Validates all experiment IDs exist and are complete
  - [ ] Generates Markdown comparison table
  - [ ] Writes to `.humanify-experiments/reports/comparison-{timestamp}.md`
  - [ ] Outputs report path on success

#### General CLI Requirements
- [ ] Help text for all commands is clear and includes examples
- [ ] E2E test: full workflow (define → run → list → compare) on tiny-qs

---

### Deliverable 4: Result Aggregation (P1-1)

- [ ] Aggregator computes per-experiment statistics:
  - [ ] Average semantic score across samples
  - [ ] Average processing time
  - [ ] Total cost (sum of all samples)
  - [ ] Success rate (completed / total samples)
- [ ] Aggregator handles partial results (some samples failed)
- [ ] Comparator computes deltas between experiments:
  - [ ] Score delta (absolute and percentage)
  - [ ] Time delta (absolute and percentage)
  - [ ] Cost delta (absolute and percentage)
- [ ] Unit tests verify aggregation logic with sample data
- [ ] Comparison tables show deltas clearly (+3 pts, -15s, +$0.05)

---

### Deliverable 5: Error Handling (P1-2)

- [ ] Clear error messages for common failures:
  - [ ] Experiment not found: "Experiment 'xyz' not found. Run 'humanify experiment list' to see all experiments."
  - [ ] Invalid preset: "Preset 'xyz' not found. Valid presets: fast, balanced, quality, anchor."
  - [ ] Sample not found: "Sample 'xyz' not found in test-samples/canonical/."
  - [ ] Experiment already exists: "Experiment name 'baseline' already exists (exp-baseline-*). Use a different name."
- [ ] Progress indicators for long operations:
  - [ ] Running experiment shows current sample (2/3: small-axios)
  - [ ] Scoring shows spinner or progress bar
- [ ] Graceful degradation:
  - [ ] If scoring fails, still save output (mark score as null)
  - [ ] If one sample fails, continue with remaining samples
- [ ] User confirmation for destructive operations (none in MVP, but prepare for future)
- [ ] Unit tests verify error messages are user-friendly

---

## Sprint Scope

### This sprint delivers:
1. **Experiment Registry System** (TypeScript, not bash scripts)
2. **CLI Commands** (define, run, list, compare)
3. **Result Aggregation** (automatic scoring and comparison)

### Deferred to future sprints:
- Multi-run statistical analysis (`--runs N`)
- Advanced visualization (charts/graphs)
- Migration of existing outputs
- Custom preset definitions
- Concurrent experiment execution
- Cleanup/pruning commands
- Detailed provenance tracking

---

## Success Validation

### Demo Script (Must Complete Successfully):
```bash
# 1. Define baseline experiment
humanify experiment define --name baseline --preset fast --samples tiny-qs --tags baseline

# 2. Define variant experiment
humanify experiment define --name multi-prompt --preset balanced --samples tiny-qs --tags variant

# 3. Run both
humanify experiment run baseline
humanify experiment run multi-prompt

# 4. List experiments
humanify experiment list

# 5. Compare
humanify experiment compare baseline multi-prompt

# 6. View report
cat .humanify-experiments/reports/comparison-*.md
```

### Expected Outcomes:
- [ ] Two experiments created with unique IDs
- [ ] Both experiments run successfully with semantic scores
- [ ] List command shows both experiments with status=complete
- [ ] Comparison report shows score/time/cost deltas in Markdown table
- [ ] All operations complete without errors
- [ ] No bash scripts used (pure TypeScript CLI)

---

## Test Coverage Requirements

### Unit Tests:
- [ ] Registry create/read/list operations
- [ ] Runner turbo-v2 invocation logic
- [ ] Comparator aggregation and delta calculations
- [ ] Error message formatting

### E2E Tests:
- [ ] Full experiment lifecycle (define → run → compare)
- [ ] Partial failure scenarios (one sample fails, others succeed)
- [ ] CLI command parsing and validation

### Test Files Created:
- [ ] `src/experiments/registry.test.ts`
- [ ] `src/experiments/runner.test.ts`
- [ ] `src/experiments/runner.e2etest.ts`
- [ ] `src/experiments/comparator.test.ts`
- [ ] `src/experiments/cli.e2etest.ts`

---

## File Deliverables

### New Files (13 total):
1. `src/experiments/types.ts` - TypeScript interfaces
2. `src/experiments/registry.ts` - Experiment storage
3. `src/experiments/registry.test.ts` - Registry unit tests
4. `src/experiments/runner.ts` - Experiment execution
5. `src/experiments/runner.test.ts` - Runner unit tests
6. `src/experiments/runner.e2etest.ts` - Runner e2e tests
7. `src/experiments/cli.ts` - CLI commands
8. `src/experiments/comparator.ts` - Result aggregation
9. `src/experiments/reporter.ts` - Markdown report generation
10. `src/experiments/comparator.test.ts` - Comparator unit tests
11. `src/experiments/cli.e2etest.ts` - CLI e2e tests
12. `.humanify-experiments/registry.json` - Experiment storage (runtime)
13. `.humanify-experiments/reports/` - Generated reports (runtime)

### Modified Files:
- [ ] `src/cli/index.ts` (or equivalent) - Add experiment command group

---

## Quality Gates

### Code Quality:
- [ ] All TypeScript files have proper types (no `any` except where justified)
- [ ] All functions have JSDoc comments
- [ ] Code follows existing HumanifyJS conventions
- [ ] No linting errors (`npm run lint`)

### Functionality:
- [ ] All acceptance criteria met (100%)
- [ ] Demo script completes successfully
- [ ] All unit tests pass (`npm run test:unit`)
- [ ] All e2e tests pass (`npm run test:e2e`)

### Documentation:
- [ ] CLI help text is clear and includes examples
- [ ] Error messages are user-friendly
- [ ] README updated with experiment workflow (if needed)

---

**Sprint is DONE when all acceptance criteria are checked and demo script succeeds.**
