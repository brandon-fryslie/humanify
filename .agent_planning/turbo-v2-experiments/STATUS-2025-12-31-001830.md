# Status Report: Turbo-V2 Experiment/Benchmarking Infrastructure
**Timestamp**: 2025-12-31-001830
**Scope**: Experiment tracking and comparison system for turbo-v2 configurations
**Evaluator**: project-evaluator
**Confidence**: FRESH

---

## Executive Summary

**Overall Assessment**: STUB - Ad-hoc bash scripts exist, no systematic experiment tracking
**Implementation Status**: 15% - Individual pieces work, but no cohesive system
**User Vision Gap**: CRITICAL - User explicitly rejects "another bash script with arbitrary arguments"
**Recommendation**: BUILD - Need structured experiment framework with TypeScript implementation

**Critical Findings**:
1. **Current state is exactly what user doesn't want**: Bash scripts with positional arguments
2. **No experiment tracking**: No way to compare configurations systematically
3. **No result aggregation**: Scores scattered across directories, manual comparison required
4. **No configuration management**: Turbo-v2 configs exist as hardcoded presets, not experiments
5. **Scoring infrastructure exists but isn't orchestrated**: Individual pieces work, no workflow

---

## What Exists (Infrastructure Assessment)

### ✅ Working Components (Can Reuse)

#### 1. Sample Management System
**Location**: `test-samples/canonical/`
**Status**: COMPLETE
**Quality**: GOOD

```
test-samples/canonical/
  {sample}/
    original.js           # Clean source code
    minified.js           # Minified version
    output-{mode}/        # Output directories per mode
      output.js           # Deobfuscated output
      semantic-score.json # LLM-as-judge score
```

**Samples Available**:
- `tiny-qs`: ~150 identifiers, 356 lines - fast iteration
- `small-axios`: ~800 identifiers, 3K lines - medium tests
- `medium-chart`: ~5000 identifiers, 11K lines - stress tests

**Evidence**: All samples have original.js, minified.js, and baseline scores recorded.

#### 2. Semantic Scoring Tool
**Location**: `scripts/score-semantic.ts`
**Status**: COMPLETE
**Quality**: EXCELLENT

**What it does**:
- Extracts identifiers from original and unminified files
- Uses GPT-4o as judge to score semantic quality (0-100)
- Returns structured JSON with score, explanation, token usage, cost

**Output format** (validated):
```json
{
  "originalFile": "path/to/original.js",
  "unminifiedFile": "path/to/unminified.js",
  "originalIdentifierCount": 60,
  "unminifiedIdentifierCount": 66,
  "score": 75,
  "explanation": "...",
  "tokensUsed": 898,
  "cost": 0.00449
}
```

**Evidence**: Examined `test-samples/canonical/tiny-qs/output-turbo-v2/semantic-score.json` - structure is clean and consistent.

#### 3. Turbo-V2 Implementation
**Location**: `src/turbo-v2/`
**Status**: COMPLETE (12 sprints implemented)
**Quality**: PRODUCTION-READY

**Key components verified**:
- `cli/presets.ts`: Fast, balanced, quality, anchor presets defined
- `cli/pass-parser.ts`: Parse custom pass configurations
- `cli/turbo-v2-command.ts`: CLI integration complete
- `orchestrator/`: Multi-pass execution engine
- `checkpoint/`: Ledger + snapshot system
- `processors/`: LLM rename, refinement, conflict detection

**Evidence**: Git history shows 12 sequential feature commits completing all planned sprints.

#### 4. Baseline Data Collection
**Location**: `scripts/measure-baseline.ts`, `scripts/compare-unminified.ts`
**Status**: COMPLETE
**Quality**: GOOD

**What exists**:
- `measure-baseline.ts`: Runs humanify in sequential mode, records metrics
- `compare-unminified.ts`: Identifier-level diff between original and output
- Baseline scores recorded in `baseline-scores.json`

**Evidence**: `test-samples/canonical/tiny-qs/baseline-scores.json` shows structured baseline data from sequential mode.

---

### ⚠️ Partial/Incomplete Components

#### 5. Justfile Recipes
**Location**: `justfile`
**Status**: PARTIAL
**Quality**: FUNCTIONAL but not experiment-oriented

**What it provides**:
```bash
just run-seq <sample>              # Run sequential mode
just run-turbo <sample> <concurrency>   # Run turbo mode
just run-turbo-v2 <sample> <preset>     # Run turbo-v2 mode
just score <sample> <mode>              # Score a specific run
just score-all                          # Score all samples (aggregated)
```

**Problems**:
1. **No experiment tracking**: Running `just run-turbo-v2 tiny-qs fast` doesn't record that this was an experiment
2. **No comparison support**: Can't say "compare all experiments for tiny-qs"
3. **Hardcoded modes**: Can't create arbitrary experiment configurations
4. **No result aggregation**: `score-all` exists but doesn't organize by experiment

**Evidence**: Examined justfile lines 108-276 - recipes exist but are mode-specific, not experiment-generic.

#### 6. Experiment Runner Script
**Location**: `scripts/experiment-run.sh`
**Status**: STUB
**Quality**: EXACTLY WHAT USER DOESN'T WANT

**What it does**:
```bash
./scripts/experiment-run.sh <variant> [sample] [preset]
```

Output goes to: `test-samples/canonical/{sample}/output-turbo-v2-{variant}/`

**Problems**:
1. **Bash script with positional args**: User explicitly said "DO NOT want another bash script with arbitrary arguments"
2. **No configuration storage**: Variant name is just a string, no metadata about what makes it different
3. **No result tracking**: Outputs files but doesn't record experiment metadata
4. **No comparison support**: Just runs, doesn't analyze
5. **Hardcoded to turbo-v2**: Can't compare against sequential or turbo-v1

**Evidence**: Lines 1-101 of `scripts/experiment-run.sh` - classic ad-hoc bash script pattern.

---

### ❌ Missing Critical Infrastructure

#### 7. Experiment Configuration System
**Status**: DOES NOT EXIST
**Impact**: BLOCKING for systematic comparison

**What's missing**:
- No way to define an "experiment" as a configuration object
- No experiment ID or naming convention
- No metadata storage (timestamp, git commit, CLI args used)
- No way to list/search past experiments

**Why it matters**: Can't answer "What were the settings for experiment X?" or "Show me all experiments that used preset=quality"

#### 8. Result Aggregation & Comparison
**Status**: DOES NOT EXIST
**Impact**: BLOCKING for answering "Which config is best?"

**What's missing**:
- No cross-experiment comparison tables
- No way to aggregate scores across samples
- No statistical analysis (mean, variance, confidence intervals)
- No visualization or report generation

**Current state**: Scores are in `{sample}/output-{mode}/semantic-score.json` but there's no code to aggregate them.

**Example of what user wants**:
```
Experiment Comparison: tiny-qs

| Experiment ID | Config | Score | Time | Cost | Notes |
|---------------|--------|-------|------|------|-------|
| exp-001 | preset=fast | 75/100 | 45s | $0.12 | Baseline |
| exp-002 | preset=balanced | 78/100 | 67s | $0.18 | +3 pts |
| exp-003 | custom-2pass | 81/100 | 52s | $0.15 | Best quality/cost |
```

This doesn't exist.

#### 9. Experiment Lifecycle Management
**Status**: DOES NOT EXIST
**Impact**: BLOCKING for "easy, dynamic, intuitive" workflow

**What's missing**:
- `humanify experiment create <name>`: Define new experiment configuration
- `humanify experiment run <id>`: Run experiment on all samples (or specific sample)
- `humanify experiment list`: Show all experiments
- `humanify experiment compare <id1> <id2>`: Side-by-side comparison
- `humanify experiment report`: Generate summary across all experiments

**Current workflow**:
1. Edit bash script or justfile
2. Run script manually
3. Manually check output directories
4. Manually run scoring
5. Manually compare JSON files

**User wants**: Declarative experiment definitions, automated execution, automatic comparison.

---

## Architectural Approaches (Non-Ad-Hoc Solutions)

### Approach 1: Experiment Registry (TypeScript)

**Core Idea**: Treat experiments as first-class entities with metadata, config, and results.

**Structure**:
```typescript
interface ExperimentConfig {
  id: string;                    // Auto-generated: exp-2025-12-31-001
  name: string;                  // User-provided: "multi-prompt-test"
  description?: string;
  created: string;               // ISO timestamp
  gitCommit: string;             // Git SHA for reproducibility

  // Turbo-v2 configuration
  turboV2: {
    preset?: string;             // "fast" | "balanced" | etc.
    passes?: PassConfig[];       // Custom passes
    iterations?: number;
    maxConcurrent?: number;
    // ... other turbo-v2 options
  };

  // Execution config
  samples: string[];             // ["tiny-qs", "small-axios"] or ["all"]
  provider: string;              // "openai"

  // Metadata
  tags?: string[];               // ["baseline", "ablation-study"]
  parent?: string;               // exp-id of parent (for variants)
}

interface ExperimentResult {
  experimentId: string;
  sample: string;
  status: 'pending' | 'running' | 'complete' | 'failed';

  // Outputs
  outputPath: string;            // Where deobfuscated.js is
  semanticScore?: number;        // 0-100
  scoreExplanation?: string;

  // Metrics
  processingTime?: number;       // seconds
  tokensUsed?: number;
  cost?: number;

  // Errors
  error?: string;

  // Timestamps
  startedAt?: string;
  completedAt?: string;
}
```

**Storage**:
```
.humanify-experiments/
  registry.json              # All experiment configs
  results/
    exp-001/
      tiny-qs-result.json
      small-axios-result.json
      medium-chart-result.json
    exp-002/
      ...
  reports/
    comparison-2025-12-31.md
```

**CLI**:
```bash
# Define experiment
humanify experiment create \
  --name "baseline-fast" \
  --preset fast \
  --samples all \
  --description "Baseline using fast preset"

# Run experiment
humanify experiment run exp-001

# List experiments
humanify experiment list
humanify experiment list --tag baseline

# Compare
humanify experiment compare exp-001 exp-002 exp-003

# Generate report
humanify experiment report --format markdown > comparison.md
```

**Advantages**:
- TypeScript = type-safe, testable, maintainable
- Structured data = easy to query and aggregate
- Registry pattern = standard, proven approach
- Fits existing HumanifyJS architecture

**Disadvantages**:
- New code to write (3-5 files)
- Need to design schema carefully
- Potential over-engineering for small use case

**Recommendation**: STRONG CANDIDATE - Aligns with user's "standardized, better, not ad hoc" requirement.

---

### Approach 2: Configuration-Driven Workflow (YAML/JSON + TypeScript)

**Core Idea**: Experiments defined in config files, orchestrated by TypeScript runner.

**Example config** (`experiments/baseline-fast.yaml`):
```yaml
name: baseline-fast
description: Turbo-v2 fast preset baseline
config:
  preset: fast
  samples: [tiny-qs, small-axios, medium-chart]
  provider: openai
tags: [baseline, fast-preset]
```

**Example config** (`experiments/multi-prompt-ablation.yaml`):
```yaml
name: multi-prompt-ablation
description: Test impact of multiple prompts in pass 1
config:
  passes:
    - rename:parallel:50:model=gpt-4o-mini
    - rename:parallel:50:model=gpt-4o-mini:prompt-variant=2
  samples: [tiny-qs]
tags: [ablation, multi-prompt]
```

**Runner**:
```bash
# Run all experiments in experiments/
humanify experiment run-all

# Run specific experiment
humanify experiment run experiments/baseline-fast.yaml

# Compare experiments
humanify experiment compare \
  experiments/baseline-fast.yaml \
  experiments/multi-prompt-ablation.yaml
```

**Advantages**:
- Declarative = easy to version control, review, share
- Separation of config and code
- Easy to define many experiments (just add YAML files)
- Fits research workflow (common in ML experiments)

**Disadvantages**:
- YAML parsing overhead
- Need to validate schemas
- Harder to programmatically create experiments

**Recommendation**: GOOD CANDIDATE - Especially if user wants to define many experiment variants.

---

### Approach 3: Embedded Experiment Tracking (Extend Turbo-V2 Ledger)

**Core Idea**: Turbo-v2 already has ledger system - extend it to track experiment metadata.

**What changes**:
1. Add `--experiment-id` flag to turbo-v2 command
2. Ledger records experiment metadata in `JOB_STARTED` event
3. Add `humanify experiments report` command that reads ledgers

**Example**:
```bash
# Run with experiment ID
humanify unminify input.js --turbo-v2 --preset fast --experiment-id baseline-fast

# Later, report aggregates all runs with same experiment-id
humanify experiments report baseline-fast
```

**Advantages**:
- Minimal new code (reuse ledger infrastructure)
- No separate storage needed
- Automatic tracking of ALL turbo-v2 runs

**Disadvantages**:
- Couples experiment tracking to turbo-v2 (can't track sequential mode easily)
- Experiment ID is just a tag, not a structured config
- No way to define experiment BEFORE running

**Recommendation**: WEAK CANDIDATE - Too limited, doesn't solve comparison problem.

---

### Approach 4: Benchmark Suite Pattern (TypeScript + JSON)

**Core Idea**: Treat this as a benchmark suite, similar to typical performance testing frameworks.

**Structure**:
```typescript
// benchmarks/suite.ts
export const experimentSuite = {
  samples: ['tiny-qs', 'small-axios', 'medium-chart'],

  experiments: [
    {
      id: 'baseline-sequential',
      name: 'Sequential baseline',
      run: (sample) => runSequential(sample),
    },
    {
      id: 'baseline-turbo-v1',
      name: 'Turbo V1 baseline',
      run: (sample) => runTurboV1(sample),
    },
    {
      id: 'turbo-v2-fast',
      name: 'Turbo V2 fast preset',
      run: (sample) => runTurboV2(sample, { preset: 'fast' }),
    },
    {
      id: 'turbo-v2-balanced',
      name: 'Turbo V2 balanced preset',
      run: (sample) => runTurboV2(sample, { preset: 'balanced' }),
    },
    // ... more experiments
  ],
};

// Run all experiments
npm run benchmark

// Run specific experiment
npm run benchmark -- --experiment turbo-v2-fast

// Compare
npm run benchmark -- --compare turbo-v2-fast,turbo-v2-balanced
```

**Advantages**:
- Familiar pattern (similar to Jest, Mocha benchmarks)
- TypeScript = full control over execution
- Easy to add new experiments (just add to array)
- Can programmatically generate experiment variants

**Disadvantages**:
- Experiments are code, not data (harder for non-programmers to define)
- Need to design good abstraction for "run" function
- Potential for tight coupling

**Recommendation**: MODERATE CANDIDATE - Good for developer-centric workflows.

---

## User Requirements Analysis

From user context:
> They want to compare different turbo-v2 configurations (baseline, multi-prompt, adaptive context)
> They explicitly DO NOT want another bash script with arbitrary arguments
> They want a "dynamic system" that is "standardized", "better", "not ad hoc"
> They want it to be "easy, dynamic, and intuitive"

**Interpretation**:
1. **"Standardized"**: Consistent structure, not one-off scripts
2. **"Better"**: Type-safe, maintainable, testable
3. **"Not ad hoc"**: First-class experiments, not script hacks
4. **"Easy"**: Simple CLI commands
5. **"Dynamic"**: Easy to add new experiments without code changes
6. **"Intuitive"**: Clear workflow, discoverable commands

**Anti-pattern** (what user DOESN'T want):
```bash
./run-experiment.sh baseline tiny-qs fast
./run-experiment.sh multi-prompt tiny-qs "rename:parallel:50 rename:parallel:50:prompt=v2"
./run-experiment.sh adaptive all balanced
# Now manually compare output-turbo-v2-baseline, output-turbo-v2-multi-prompt, etc.
```

**What user DOES want** (best guess):
```bash
# Define experiment once
humanify experiment create baseline --preset fast --samples all

# Run it
humanify experiment run baseline

# Define variant
humanify experiment create multi-prompt --parent baseline --passes "..." --samples tiny-qs

# Run it
humanify experiment run multi-prompt

# Compare all experiments
humanify experiment compare --all

# Generate report
humanify experiment report > results.md
```

---

## Dependencies and Technical Considerations

### Integration Points

#### 1. Turbo-V2 CLI
**Location**: `src/turbo-v2/cli/turbo-v2-command.ts`
**Current interface**: Accepts flags like `--preset`, `--pass`, `--turbo-v2`

**Integration requirement**: Experiment runner needs to:
- Construct CLI args from experiment config
- Invoke turbo-v2 programmatically (not via shell)
- Capture results (output path, metrics)

**Options**:
- **Option A**: Shell out to `humanify` CLI (simple, but loses type safety)
- **Option B**: Import turbo-v2 orchestrator directly (clean, but requires refactor)

**Recommendation**: Option B - expose turbo-v2 orchestrator as programmatic API.

#### 2. Semantic Scoring
**Location**: `scripts/score-semantic.ts`
**Current interface**: CLI script, outputs JSON

**Integration requirement**: Experiment runner needs to:
- Trigger scoring after each run
- Parse JSON output
- Store in experiment result

**Options**:
- **Option A**: Shell out to `npx tsx scripts/score-semantic.ts` (works, but fragile)
- **Option B**: Refactor to importable function (cleaner)

**Recommendation**: Option B - make scoring a library function.

#### 3. Result Storage
**Current state**: Scores scattered in `test-samples/canonical/{sample}/output-{mode}/semantic-score.json`

**Experiment system needs**:
- Centralized result storage (`.humanify-experiments/results/`)
- Ability to query results by experiment ID, sample, timestamp
- Aggregation support (avg score across samples, etc.)

**Technical choice**: JSON files vs. SQLite vs. in-memory

**Recommendation**: JSON files initially (simple, human-readable), SQLite if querying becomes complex.

---

### Implementation Estimate

**For Approach 1 (Experiment Registry)**:

| Component | Effort | Files | LOC |
|-----------|--------|-------|-----|
| Experiment config types | 2 hours | 1 | 150 |
| Registry storage (JSON) | 3 hours | 1 | 200 |
| CLI commands (create/run/list/compare) | 8 hours | 4 | 600 |
| Result aggregation | 4 hours | 1 | 250 |
| Report generation (Markdown) | 3 hours | 1 | 200 |
| Tests | 4 hours | 5 | 400 |
| **Total** | **24 hours** | **13** | **~1800** |

**Dependencies**:
- Refactor turbo-v2 to expose programmatic API (4 hours)
- Refactor scoring to library function (2 hours)
- Total: **30 hours** (~4 days)

---

### Technical Risks

#### Risk 1: Turbo-V2 Coupling
**Problem**: Experiment system needs to invoke turbo-v2, but current CLI is not designed for programmatic use.

**Mitigation**: Refactor `turbo-v2-command.ts` to separate CLI parsing from orchestration logic. Expose `runTurboV2(config)` function.

**Impact**: 4 hours additional work.

#### Risk 2: Result Schema Evolution
**Problem**: As turbo-v2 evolves, experiment results may need new fields.

**Mitigation**: Version experiment result schema. Support migration/backward compatibility.

**Impact**: Add versioning to ExperimentResult type, handle in storage layer.

#### Risk 3: Sample Variability
**Problem**: LLM scoring has variance. Same config run twice may yield different scores.

**Mitigation**: Support multiple runs per experiment, report mean ± stddev.

**Impact**: Add `runs` field to experiment config, aggregate results.

---

## Data Flow Verification

### Current Flow (Ad-Hoc)
```
User runs: just run-turbo-v2 tiny-qs fast
  ↓
justfile calls: ./scripts/run-turbo-v2.sh tiny-qs fast
  ↓
Bash script calls: node dist/index.mjs unminify --turbo-v2 --preset fast ...
  ↓
Output written to: test-samples/canonical/tiny-qs/output-turbo-v2/output.js
  ↓
User manually runs: just score-turbo-v2 tiny-qs
  ↓
Score written to: test-samples/canonical/tiny-qs/output-turbo-v2/semantic-score.json
  ↓
User manually opens semantic-score.json to see result
```

**Data loss points**:
- No record of WHY this run happened (was it an experiment? which one?)
- No record of WHEN it ran
- No record of git commit (can't reproduce)
- No aggregation across samples

### Proposed Flow (Experiment Registry)
```
User runs: humanify experiment create baseline --preset fast --samples all
  ↓
Registry writes: .humanify-experiments/registry.json (exp-001)
  ↓
User runs: humanify experiment run exp-001
  ↓
For each sample in [tiny-qs, small-axios, medium-chart]:
  ├─ Invoke turboV2Orchestrator({ preset: 'fast', sample })
  ├─ Output to: .humanify-experiments/results/exp-001/{sample}/output.js
  ├─ Run semantic scoring
  ├─ Write result: .humanify-experiments/results/exp-001/{sample}/result.json
  └─ Update status: registry.json (exp-001 status = complete)
  ↓
User runs: humanify experiment compare exp-001 exp-002
  ↓
Aggregator reads: .humanify-experiments/results/{exp-001,exp-002}/**/result.json
  ↓
Generates comparison table and writes: .humanify-experiments/reports/comparison-{timestamp}.md
```

**Data preservation**:
- Experiment config stored in registry (full reproducibility)
- All results centralized
- Git commit recorded in experiment metadata
- Timestamps for all operations
- Can regenerate reports anytime

---

## Ambiguities Found

| Area | Question | How It Would Be Guessed | Impact |
|------|----------|-------------------------|--------|
| **Experiment naming** | How are experiment IDs generated? User-provided name? Auto-increment? Timestamp-based? | Likely guess: timestamp-based like `exp-2025-12-31-001` | Naming conflicts if multiple experiments created at same time |
| **Sample selection** | When user specifies `--samples all`, does "all" mean current samples or a dynamic list? | Likely guess: hardcoded list of tiny-qs, small-axios, medium-chart | Breaks when new samples added |
| **Result retention** | How long are experiment results kept? Are old results auto-deleted? | Likely guess: keep forever, manual cleanup | Disk space bloat over time |
| **Concurrent experiments** | Can multiple experiments run simultaneously on different samples? | Likely guess: no locking, race conditions possible | Corrupted results if two experiments write to same output dir |
| **Comparison baseline** | When comparing experiments, what's the reference? First experiment? Best score? Sequential baseline? | Likely guess: first in list | Comparison is meaningless without defined baseline |
| **Statistical significance** | LLM scoring has variance. How many runs needed to determine if improvement is real? | Likely guess: single run per experiment | False positives (lucky run) or false negatives (unlucky run) |

---

## Recommendations

### Priority 1: Resolve Ambiguities
**Before implementation**:
1. Define experiment ID convention (recommendation: `exp-{YYYY-MM-DD}-{NNN}`)
2. Define sample list source (recommendation: read from `test-samples/canonical/` dynamically)
3. Define result retention policy (recommendation: keep all, add `humanify experiment prune --older-than 30d`)
4. Define concurrency model (recommendation: file locking per sample, allow concurrent experiments on different samples)
5. Define comparison baseline (recommendation: always compare against experiment tagged `baseline`)
6. Define statistical rigor (recommendation: support `--runs N` flag, report mean ± stddev)

### Priority 2: Choose Architecture
**Recommended**: **Approach 1 (Experiment Registry)** for these reasons:
1. Aligns with user's "standardized, not ad hoc" requirement
2. TypeScript = fits HumanifyJS architecture, type-safe
3. Structured data = easy to extend, query, visualize
4. First-class experiments = discoverable, manageable
5. Proven pattern = similar to test frameworks, experiment tracking tools

**Alternative**: **Approach 2 (Config files)** if user prefers declarative definitions over programmatic creation.

### Priority 3: Implementation Phases

#### Phase 1: Core Registry (8 hours)
- [x] Define TypeScript types for ExperimentConfig, ExperimentResult
- [x] Implement registry storage (JSON file)
- [x] CLI command: `humanify experiment create`
- [x] CLI command: `humanify experiment list`

**Deliverable**: Can define and list experiments.

#### Phase 2: Execution (12 hours)
- [x] Refactor turbo-v2 to expose programmatic API
- [x] Implement experiment runner
- [x] CLI command: `humanify experiment run <id>`
- [x] Result storage

**Deliverable**: Can run experiments and collect results.

#### Phase 3: Comparison (6 hours)
- [x] Result aggregation logic
- [x] CLI command: `humanify experiment compare <id1> <id2> ...`
- [x] Markdown report generation

**Deliverable**: Can compare experiments and generate reports.

#### Phase 4: Quality of Life (4 hours)
- [x] CLI command: `humanify experiment delete <id>`
- [x] CLI command: `humanify experiment show <id>` (detailed view)
- [x] CLI command: `humanify experiment prune` (cleanup old results)
- [x] Add `--runs N` support for statistical rigor

**Total**: 30 hours (4-5 days)

---

## Missing Checks (Implementer Should Create)

### Persistent Test Suite
Currently no automated tests for experiment workflow. Implementer should create:

**Test suite**: `src/experiments/experiments.test.ts`

**Required test cases**:
1. **Experiment creation**:
   - Create experiment with valid config → succeeds
   - Create experiment with duplicate ID → fails
   - Create experiment with invalid preset → fails

2. **Experiment execution**:
   - Run experiment on single sample → produces result
   - Run experiment on all samples → produces N results
   - Run experiment with missing sample → reports error

3. **Result aggregation**:
   - Compare 2 experiments → produces correct diff
   - Compare experiment with missing results → handles gracefully
   - Generate report with 0 experiments → empty report

4. **Data integrity**:
   - Registry survives crash (atomic writes)
   - Results survive crash (atomic writes)
   - Concurrent experiment runs don't corrupt data

5. **End-to-end smoke test**:
   - Create experiment → run → score → compare → verify output

---

## Verdict

### Implementation Readiness: NOT READY

**Blockers**:
1. User requirements need clarification (6 ambiguities listed above)
2. Architecture choice needs approval (Approach 1 vs. Approach 2)
3. Turbo-v2 refactor needed (expose programmatic API)

### Workflow Recommendation: **PAUSE**

**Clarification Needed Before Proceeding**:

#### Question 1: Experiment Identification
**Context**: Need convention for experiment IDs and naming.
**Options**:
- Option A: Auto-generated ID (`exp-001`), user provides name
- Option B: User provides ID directly (risk of conflicts)
- Option C: Timestamp-based (`exp-2025-12-31-143022`)
**Impact**: Affects CLI UX and storage structure.

#### Question 2: Experiment Definition Workflow
**Context**: How should users define experiments?
**Options**:
- Option A: Programmatic (CLI flags): `humanify experiment create --preset fast`
- Option B: Declarative (YAML files): `experiments/baseline.yaml`
- Option C: Hybrid (CLI creates skeleton, user edits JSON)
**Impact**: Determines whether Approach 1 or Approach 2 is better fit.

#### Question 3: Statistical Rigor
**Context**: LLM scoring has variance. Single run or multiple runs?
**How it was guessed**: Current scripts run once, assume result is stable.
**Options**:
- Option A: Single run (fast, may be unreliable)
- Option B: Multiple runs by default (slow, expensive)
- Option C: Single run with `--runs N` flag for critical experiments
**Impact**: Affects cost (more runs = more API calls) and reporting complexity.

#### Question 4: Comparison Baseline
**Context**: When comparing experiments, what's the reference point?
**Options**:
- Option A: First experiment in comparison list
- Option B: Experiment tagged as `baseline`
- Option C: Sequential mode (always compare against non-turbo)
- Option D: User specifies via `--baseline <id>`
**Impact**: Determines whether improvements are measured absolutely or relatively.

#### Question 5: Result Organization
**Context**: Where should experiment outputs live?
**Options**:
- Option A: Centralized (`.humanify-experiments/results/exp-001/...`)
- Option B: Co-located with samples (`test-samples/canonical/{sample}/experiments/exp-001/...`)
- Option C: Hybrid (results centralized, link to outputs in sample dirs)
**Impact**: Affects discoverability and cleanup.

#### Question 6: Existing Integration
**Context**: What happens to existing output directories?
**How it was guessed**: Current workflow uses `output-sequential`, `output-turbo`, `output-turbo-v2`.
**Options**:
- Option A: Deprecate old dirs, migrate to experiment system
- Option B: Parallel systems (old bash scripts + new experiment system)
- Option C: Experiment system can import existing results as "legacy experiments"
**Impact**: Determines migration path and backward compatibility.

---

## File Index

**Evaluated**:
- `justfile`: Recipe-based workflow (functional but not experiment-oriented)
- `scripts/experiment-run.sh`: Bash script (ad-hoc, user explicitly rejects)
- `scripts/score-semantic.ts`: LLM-as-judge scoring (complete, reusable)
- `scripts/measure-baseline.ts`: Baseline measurement (complete)
- `scripts/compare-unminified.ts`: Identifier diff (complete)
- `src/turbo-v2/cli/turbo-v2-command.ts`: Turbo-v2 CLI (needs refactor for programmatic use)
- `test-samples/canonical/`: Sample structure (good, standardized)

**Missing** (needs creation):
- `src/experiments/registry.ts`: Experiment registry storage
- `src/experiments/runner.ts`: Experiment execution orchestrator
- `src/experiments/aggregator.ts`: Result aggregation and comparison
- `src/experiments/reporter.ts`: Markdown report generation
- `src/experiments/cli.ts`: CLI commands for experiment management
- `src/experiments/types.ts`: TypeScript types for experiments and results

---

*This evaluation identifies a critical gap: the current ad-hoc bash script approach is exactly what the user doesn't want. A structured TypeScript-based experiment system is needed, but requires architectural decisions and user input on 6 key ambiguities before implementation can proceed.*
