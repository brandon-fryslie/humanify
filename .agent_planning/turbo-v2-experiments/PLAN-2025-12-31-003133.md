# Sprint Plan: Turbo-V2 Experiment Registry (MVP)

**Generated**: 2025-12-31-003133
**Source**: STATUS-2025-12-31-001830.md
**Sprint Goal**: Build a TypeScript-based experiment tracking system that replaces ad-hoc bash scripts with a standardized, dynamic, intuitive CLI workflow

---

## Executive Summary

### Current State
- Individual validation pieces work (scoring, samples, turbo-v2)
- Ad-hoc bash scripts with positional arguments (exactly what user doesn't want)
- No experiment tracking, no systematic comparison, scattered results
- Implementation Status: 15% (infrastructure exists, no cohesive system)

### Target State
- TypeScript experiment registry with metadata, config, and results
- CLI commands: `experiment define`, `experiment run`, `experiment compare`
- Centralized storage in `.humanify-experiments/`
- Easy comparison of turbo-v2 variants (baseline, multi-prompt, adaptive)

### Sprint Scope (MVP Focus)
This sprint delivers **3 deliverables** for the core workflow: **Define → Run → Compare**

**In Scope**:
1. Experiment registry system (TypeScript)
2. CLI integration for define/run/compare
3. Result aggregation and comparison reports

**Deferred**:
- Multi-run statistical analysis (single run per experiment for now)
- Advanced visualization (Markdown tables for now)
- Migration of existing outputs (clean slate)
- Custom presets beyond turbo-v2 presets (use existing presets)

---

## Priority 0 (Critical): Foundation

### P0-1: Experiment Registry & Storage

**Status**: Not Started
**Effort**: Medium (1 day)
**Dependencies**: None
**Spec Reference**: FINAL_PROJECT_SPEC.md §3 (Storage Layout), §7 (CLI Interface) • **Status Reference**: STATUS-2025-12-31-001830.md §7 (Missing Critical Infrastructure)

#### Description
Create TypeScript-based experiment registry to replace ad-hoc bash scripts. This provides structured storage for experiment configurations and results.

**Key Design Decisions** (simplest viable):
- **Experiment IDs**: User provides name (e.g., "baseline", "multi-prompt"), system auto-generates ID as `exp-{name}-{YYYY-MM-DD-HHmmss}`
- **Storage**: JSON files in `.humanify-experiments/` (simple, human-readable)
- **Sample Selection**: `--samples all` reads from `test-samples/canonical/` dynamically
- **Concurrency**: Single experiment at a time (no locking needed for MVP)
- **Baseline**: Sequential mode always available for comparison via tag `baseline`

#### Acceptance Criteria
- [ ] TypeScript types defined for `ExperimentConfig` and `ExperimentResult` in `src/experiments/types.ts`
- [ ] Registry storage implemented in `src/experiments/registry.ts` with JSON persistence
- [ ] Registry supports create, read, list operations
- [ ] Experiments have unique IDs: `exp-{name}-{timestamp}` format
- [ ] Storage structure follows:
  ```
  .humanify-experiments/
    registry.json              # All experiment configs
    results/
      exp-baseline-*/
        {sample}-result.json   # Per-sample results
  ```
- [ ] Unit tests verify create/read/list operations (no duplicates, proper validation)
- [ ] Registry handles missing/corrupted files gracefully (error messages, no crash)

#### Technical Notes
**TypeScript interfaces** (minimal viable):
```typescript
interface ExperimentConfig {
  id: string;                    // exp-baseline-2025-12-31-003133
  name: string;                  // "baseline"
  description?: string;
  created: string;               // ISO timestamp
  gitCommit: string;             // Current git SHA

  turboV2Config: {
    preset?: string;             // "fast" | "balanced" | "quality" | "anchor"
    passes?: string[];           // Custom pass definitions (optional)
  };

  samples: string[];             // ["tiny-qs", "small-axios"] or ["all"]
  provider: string;              // "openai"
  tags?: string[];               // ["baseline", "ablation"]
}

interface ExperimentResult {
  experimentId: string;
  sample: string;
  status: 'pending' | 'running' | 'complete' | 'failed';

  outputPath?: string;           // Where output.js is
  semanticScore?: number;        // 0-100
  scoreExplanation?: string;

  processingTime?: number;       // seconds
  tokensUsed?: number;
  cost?: number;

  error?: string;
  startedAt?: string;
  completedAt?: string;
}
```

**Files to create**:
- `src/experiments/types.ts` (~100 LOC)
- `src/experiments/registry.ts` (~200 LOC)
- `src/experiments/registry.test.ts` (~150 LOC)

---

### P0-2: Experiment Runner (Execute & Score)

**Status**: Not Started
**Effort**: Medium (1 day)
**Dependencies**: P0-1
**Spec Reference**: FINAL_PROJECT_SPEC.md §7 (CLI), §5 (Multi-Pass) • **Status Reference**: STATUS-2025-12-31-001830.md §3 (Working Components), §6 (Integration Points)

#### Description
Build orchestrator that runs experiments by invoking turbo-v2 programmatically (not via shell) and automatically triggers semantic scoring. This is the "Run" step of Define → Run → Compare.

**Integration Strategy**:
- Import turbo-v2 orchestrator directly (avoid shell commands)
- Reuse existing `scripts/score-semantic.ts` for now (invoke via shell as interim solution)
- Future sprint: refactor scoring to importable library function

#### Acceptance Criteria
- [ ] Experiment runner implemented in `src/experiments/runner.ts`
- [ ] Runner accepts experiment ID, resolves config from registry
- [ ] For each sample in experiment config:
  - [ ] Invokes turbo-v2 with correct preset/passes
  - [ ] Captures output path
  - [ ] Triggers semantic scoring via `scripts/score-semantic.ts`
  - [ ] Parses score JSON and stores in result
  - [ ] Handles errors gracefully (partial failures don't crash)
- [ ] Results written to `.humanify-experiments/results/{exp-id}/{sample}-result.json`
- [ ] Status updates persisted to registry (pending → running → complete/failed)
- [ ] Unit tests verify correct turbo-v2 invocation and result storage
- [ ] E2E test: run experiment on `tiny-qs` sample, verify output and score exist

#### Technical Notes
**Turbo-V2 Integration**:
- STATUS report notes turbo-v2 CLI needs refactor for programmatic use
- For MVP, use simplified approach: construct CLI args and invoke via child_process
- Future sprint: expose turbo-v2 orchestrator as importable function

**Scoring Integration**:
- Shell out to `npx tsx scripts/score-semantic.ts {original} {output}`
- Parse stdout JSON
- Store in result

**Error Handling**:
- If one sample fails, continue with remaining samples
- Record error in result with status='failed'
- Log detailed error for debugging

**Files to create**:
- `src/experiments/runner.ts` (~300 LOC)
- `src/experiments/runner.test.ts` (~200 LOC)
- `src/experiments/runner.e2etest.ts` (~150 LOC)

---

### P0-3: CLI Integration (Define, Run, Compare)

**Status**: Not Started
**Effort**: Large (1.5 days)
**Dependencies**: P0-1, P0-2
**Spec Reference**: FINAL_PROJECT_SPEC.md §7 (CLI Interface) • **Status Reference**: STATUS-2025-12-31-001830.md §7.9 (Missing Lifecycle Management)

#### Description
Add CLI commands to humanify for the core experiment workflow. This makes the system "easy, dynamic, and intuitive" per user requirements.

**Commands to implement**:
1. `humanify experiment define` - Create experiment config
2. `humanify experiment run` - Execute experiment
3. `humanify experiment list` - Show all experiments
4. `humanify experiment compare` - Generate comparison report

#### Acceptance Criteria
- [ ] CLI command `humanify experiment define` implemented with flags:
  - [ ] `--name` (required): User-friendly name
  - [ ] `--preset` (required): Turbo-v2 preset (fast/balanced/quality/anchor)
  - [ ] `--samples` (required): Comma-separated list or "all"
  - [ ] `--description` (optional): Text description
  - [ ] `--tags` (optional): Comma-separated tags
  - [ ] Validates preset exists in turbo-v2 presets
  - [ ] Validates samples exist in test-samples/canonical/
  - [ ] Outputs experiment ID on success
- [ ] CLI command `humanify experiment run <experiment-id>` implemented:
  - [ ] Accepts experiment ID (exact match or prefix match if unique)
  - [ ] Shows experiment config summary before running
  - [ ] Displays progress (current sample X/Y)
  - [ ] Shows per-sample results (score, time, cost) as they complete
  - [ ] Outputs summary on completion (total time, avg score, total cost)
- [ ] CLI command `humanify experiment list` implemented:
  - [ ] Shows table: ID | Name | Preset | Samples | Status | Avg Score
  - [ ] Supports `--tag` filter
  - [ ] Supports `--status` filter (pending/complete/failed)
- [ ] CLI command `humanify experiment compare <id1> <id2> [id3...]` implemented:
  - [ ] Validates all experiment IDs exist and are complete
  - [ ] Generates Markdown comparison table
  - [ ] Writes to `.humanify-experiments/reports/comparison-{timestamp}.md`
  - [ ] Outputs report path on success
- [ ] Help text for all commands is clear and includes examples
- [ ] E2E test: full workflow (define → run → list → compare) on tiny-qs

#### Technical Notes
**CLI Framework**:
- Use existing HumanifyJS CLI structure (Commander.js or similar)
- Add new command group `experiment` with subcommands

**Example Usage**:
```bash
# Define baseline experiment
humanify experiment define \
  --name baseline \
  --preset fast \
  --samples all \
  --description "Fast preset baseline" \
  --tags baseline

# Output: Created experiment: exp-baseline-2025-12-31-003133

# Run it
humanify experiment run exp-baseline-2025-12-31-003133

# Or use name if unique
humanify experiment run baseline

# List all experiments
humanify experiment list

# Compare experiments
humanify experiment compare exp-baseline-* exp-multi-prompt-*
```

**Comparison Report Format** (Markdown):
```markdown
# Experiment Comparison
Generated: 2025-12-31-003133

## Summary
| Experiment ID | Name | Preset | Samples | Avg Score | Avg Time | Total Cost |
|---------------|------|--------|---------|-----------|----------|------------|
| exp-baseline-* | baseline | fast | 3 | 75/100 | 45s | $0.12 |
| exp-multi-prompt-* | multi-prompt | custom | 3 | 78/100 | 67s | $0.18 |

## Per-Sample Breakdown

### tiny-qs
| Experiment | Score | Time | Cost | Notes |
|------------|-------|------|------|-------|
| baseline | 75/100 | 15s | $0.04 | - |
| multi-prompt | 78/100 | 22s | $0.06 | +3 pts, +7s |

### small-axios
...
```

**Files to create**:
- `src/experiments/cli.ts` (~400 LOC)
- `src/experiments/comparator.ts` (~250 LOC)
- `src/experiments/reporter.ts` (~200 LOC)
- `src/experiments/cli.e2etest.ts` (~250 LOC)

---

## Priority 1 (High): Quality & Usability

### P1-1: Result Aggregation & Validation

**Status**: Not Started
**Effort**: Small (0.5 day)
**Dependencies**: P0-2
**Spec Reference**: FINAL_PROJECT_SPEC.md §12 (Validation) • **Status Reference**: STATUS-2025-12-31-001830.md §7.8 (Missing Result Aggregation)

#### Description
Implement aggregation logic to compute summary statistics across samples and validate results meet quality thresholds.

#### Acceptance Criteria
- [ ] Aggregator computes per-experiment statistics:
  - [ ] Average semantic score across samples
  - [ ] Average processing time
  - [ ] Total cost (sum of all samples)
  - [ ] Success rate (completed / total samples)
- [ ] Aggregator handles partial results (some samples failed)
- [ ] Comparator computes deltas between experiments:
  - [ ] Score delta (absolute and percentage)
  - [ ] Time delta (absolute and percentage)
  - [ ] Cost delta (absolute and percentage)
- [ ] Unit tests verify aggregation logic with sample data
- [ ] Comparison tables show deltas clearly (+3 pts, -15s, +$0.05)

#### Technical Notes
**Statistical Functions**:
```typescript
function computeAggregateStats(results: ExperimentResult[]): {
  avgScore: number;
  avgTime: number;
  totalCost: number;
  successRate: number;
}

function computeDelta(baseline: Stats, variant: Stats): {
  scoreDelta: number;
  timeDelta: number;
  costDelta: number;
}
```

**Files to modify**:
- `src/experiments/comparator.ts` (add aggregation logic)
- `src/experiments/comparator.test.ts` (~100 LOC)

---

### P1-2: Error Handling & User Feedback

**Status**: Not Started
**Effort**: Small (0.5 day)
**Dependencies**: P0-3
**Spec Reference**: FINAL_PROJECT_SPEC.md §10 (Error Handling) • **Status Reference**: STATUS-2025-12-31-001830.md §9 (Technical Considerations)

#### Description
Ensure robust error handling and clear user feedback throughout the experiment lifecycle.

#### Acceptance Criteria
- [ ] Clear error messages for common failures:
  - [ ] Experiment not found: "Experiment 'xyz' not found. Run 'humanify experiment list' to see all experiments."
  - [ ] Invalid preset: "Preset 'xyz' not found. Valid presets: fast, balanced, quality, anchor."
  - [ ] Sample not found: "Sample 'xyz' not found in test-samples/canonical/."
  - [ ] Experiment already exists: "Experiment name 'baseline' already exists (exp-baseline-*). Use a different name."
- [ ] Progress indicators for long operations:
  - [ ] Running experiment shows current sample (2/3: small-axios)
  - [ ] Scoring shows spinner or progress bar
- [ ] Graceful degradation:
  - [ ] If scoring fails, still save output (mark score as null)
  - [ ] If one sample fails, continue with remaining samples
- [ ] User confirmation for destructive operations (none in MVP, but prepare for future)
- [ ] Unit tests verify error messages are user-friendly

#### Technical Notes
**User Feedback Patterns**:
- Use existing HumanifyJS progress rendering if available
- Fallback to simple console output
- Color coding: green for success, yellow for warnings, red for errors

**Files to modify**:
- `src/experiments/cli.ts` (add error messages)
- `src/experiments/runner.ts` (improve error handling)

---

## Dependency Graph

```
P0-1 (Registry & Storage)
  ↓
P0-2 (Experiment Runner) ────→ P1-1 (Result Aggregation)
  ↓
P0-3 (CLI Integration) ────→ P1-2 (Error Handling)
```

**Critical Path**: P0-1 → P0-2 → P0-3 (3 days)
**Parallel Work**: P1-1 and P1-2 can be done concurrently after P0-2/P0-3 (0.5 day each)
**Total**: 4 days

---

## Recommended Sprint Execution

### Day 1: Foundation
- Morning: P0-1 (Registry & Storage)
- Afternoon: Start P0-2 (Runner - turbo-v2 integration)

### Day 2: Execution
- Morning: Finish P0-2 (Runner - scoring integration)
- Afternoon: Start P0-3 (CLI - define and list commands)

### Day 3: Integration
- Morning: Finish P0-3 (CLI - run and compare commands)
- Afternoon: E2E testing, bug fixes

### Day 4: Polish
- Morning: P1-1 (Aggregation) and P1-2 (Error Handling) in parallel
- Afternoon: Documentation, final testing, demo prep

---

## Risk Assessment

### High-Risk Items

#### Risk 1: Turbo-V2 Programmatic API
**Problem**: Turbo-v2 CLI may not expose clean programmatic interface
**Impact**: Runner implementation blocked
**Mitigation**: Use child_process to invoke CLI for MVP, refactor later
**Likelihood**: Medium
**Severity**: Medium

#### Risk 2: Scoring Script Integration
**Problem**: `scripts/score-semantic.ts` may be fragile via shell invocation
**Impact**: Results incomplete or unreliable
**Mitigation**: Add retry logic, validate JSON output, log errors
**Likelihood**: Low
**Severity**: Low

#### Risk 3: Sample Availability
**Problem**: Canonical samples might not exist or be incomplete
**Impact**: Can't run experiments
**Mitigation**: STATUS confirms samples exist (tiny-qs, small-axios, medium-chart), validate on startup
**Likelihood**: Low
**Severity**: Low

### Medium-Risk Items

#### Risk 4: ID Collisions
**Problem**: Timestamp-based IDs might collide if created rapidly
**Impact**: Overwrite existing experiments
**Mitigation**: Check for existence before creating, add milliseconds to timestamp
**Likelihood**: Low
**Severity**: Medium

#### Risk 5: JSON Corruption
**Problem**: Registry file might get corrupted if app crashes during write
**Impact**: Loss of experiment metadata
**Mitigation**: Atomic writes (write to temp, rename), keep backups
**Likelihood**: Low
**Severity**: Medium

---

## Success Criteria

### Sprint Complete When:
1. **All P0 items delivered** with acceptance criteria met
2. **E2E workflow works**: Define baseline → Run on tiny-qs → Compare against another experiment → Generate report
3. **Unit tests pass** for registry, runner, comparator
4. **E2E tests pass** for CLI commands
5. **No ad-hoc bash scripts needed** for experiment workflow

### Demo Script (Validation):
```bash
# 1. Define baseline experiment
humanify experiment define --name baseline --preset fast --samples tiny-qs --tags baseline

# 2. Define variant experiment
humanify experiment define --name multi-prompt --preset balanced --samples tiny-qs --tags variant

# 3. Run both
humanify experiment run baseline
humanify experiment run multi-prompt

# 4. List experiments
humanify experiment list

# 5. Compare
humanify experiment compare baseline multi-prompt

# 6. View report
cat .humanify-experiments/reports/comparison-*.md
```

**Expected Output**:
- Two experiments created with unique IDs
- Both run successfully with scores
- List shows both experiments with status=complete
- Comparison report shows score/time/cost deltas
- All operations complete without errors

---

## Out of Scope (Future Sprints)

### Deferred Features:
1. **Multi-run statistical analysis**: `--runs N` flag for variance analysis (§12 Validation)
2. **Advanced visualization**: Charts, graphs (Markdown tables sufficient for MVP)
3. **Migration tooling**: Import existing `output-*` directories as experiments (clean slate for now)
4. **Custom preset definition**: Use existing turbo-v2 presets only (§6 Presets)
5. **Concurrent experiments**: One at a time for MVP (no locking needed)
6. **Cleanup/pruning**: Manual deletion for now (no `--older-than` flag)
7. **Detailed provenance**: Git commit recorded but not used for comparison yet
8. **Anchor detection experiments**: Use existing anchor preset, don't customize

### Why Deferred:
- Not essential for core workflow (Define → Run → Compare)
- Add complexity without immediate value
- Can be added incrementally in future sprints

---

## File Index (What Gets Created)

### New Files (13 total):
1. `src/experiments/types.ts` - TypeScript interfaces
2. `src/experiments/registry.ts` - Experiment storage
3. `src/experiments/registry.test.ts` - Registry unit tests
4. `src/experiments/runner.ts` - Experiment execution
5. `src/experiments/runner.test.ts` - Runner unit tests
6. `src/experiments/runner.e2etest.ts` - Runner e2e tests
7. `src/experiments/cli.ts` - CLI commands
8. `src/experiments/comparator.ts` - Result aggregation
9. `src/experiments/reporter.ts` - Markdown report generation
10. `src/experiments/comparator.test.ts` - Comparator unit tests
11. `src/experiments/cli.e2etest.ts` - CLI e2e tests
12. `.humanify-experiments/registry.json` - Experiment storage (created at runtime)
13. `.humanify-experiments/reports/` - Generated reports (created at runtime)

### Modified Files:
- `src/cli/index.ts` or equivalent (add experiment command group)

### Total Estimated LOC: ~2150

---

## Blockers and Questions

### Resolved Assumptions (Made for MVP):
1. **Experiment naming**: User provides name, system generates `exp-{name}-{timestamp}` ID ✓
2. **Sample selection**: `--samples all` reads `test-samples/canonical/` dynamically ✓
3. **Baseline reference**: Sequential mode available via tag, compare against any experiment ✓
4. **Statistical rigor**: Single run per experiment for MVP (defer multi-run) ✓
5. **Result storage**: Centralized in `.humanify-experiments/` ✓
6. **Existing outputs**: Don't migrate, clean slate ✓

### Open Questions (Acceptable for MVP):
None - all decisions made to unblock implementation.

---

**End of Sprint Plan**
