# Turbo Mode V2: Fresh Implementation Plan

## Overview

A ground-up reimplementation of turbo mode that builds functionality incrementally, validating correctness and performance at each step. Based on lessons learned from v1, with cleaner architecture and better separation of concerns.

## Key Decisions

- **Location**: `src/turbo-v2/` - completely isolated directory
- **Code Reuse**: ZERO copy/paste from v1 turbo. Reference only. Original humanify code (pre-turbo) is acceptable to adapt.
- **Provider**: OpenAI only initially, others added later
- **Checkpoints**: New format, explicitly incompatible with v1
- **Priority**: Balanced quality and speed
- **Coexistence**: `--turbo-v2` flag during development, may replace `--turbo` when mature

## Design Principles

1. **Incremental Building**: Each phase produces a working system that can be validated
2. **Correctness First**: Every step includes automated tests before moving on
3. **Performance Baselines**: Measure before and after each optimization
4. **Clean Interfaces**: Each component has a simple, well-defined interface
5. **Debuggability**: Built-in observability from day one
6. **Fresh Code**: Write everything from scratch, no shortcuts

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                    TurboV2 Rename Pipeline                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────────┐ │
│  │  Identifier │───▶│  Batch      │───▶│  Parallel           │ │
│  │  Collector  │    │  Scheduler  │    │  Executor           │ │
│  └─────────────┘    └─────────────┘    └─────────────────────┘ │
│         │                 │                      │              │
│         ▼                 ▼                      ▼              │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────────┐ │
│  │  Context    │    │  Dependency │    │  AST                │ │
│  │  Extractor  │    │  Analyzer   │    │  Mutator            │ │
│  └─────────────┘    └─────────────┘    └─────────────────────┘ │
│                                                                 │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                   Checkpoint Manager                        │ │
│  └────────────────────────────────────────────────────────────┘ │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Phase 0: Validation Infrastructure

**Goal**: Establish ground truth for measuring unminification quality before writing any turbo code.

### Step 0.1: Find Canonical Libraries
- Identify 3 JavaScript libraries with accessible source:
  - **Tiny**: <500 lines (e.g., a utility library)
  - **Small**: <2000 lines (e.g., a focused single-purpose lib)
  - **Medium**: <5000 lines (e.g., a moderately complex lib)
- Requirements: original source available, well-named variables, good code quality
- Store in `test-samples/canonical/`

### Step 0.2: Generate Minified Samples
- Minify each canonical library using a standard minifier (terser/uglify)
- Store pairs: `original.js` + `minified.js`
- Verify minified code runs correctly (if testable)

### Step 0.3: Comparison Pipeline
- Script to compare unminified output against original
- Metrics:
  - **Name similarity**: How many identifiers match original names?
  - **Semantic similarity**: Are names meaningful even if different?
  - **Structural correctness**: Does output parse and run?
- Output: Score card for each sample

### Step 0.4: Baseline Measurement
- Run existing sequential humanify on minified samples
- Record baseline scores
- This is the bar to beat/match

**Phase 0 Deliverable**: Reproducible quality measurement pipeline with baseline scores.

---

## Phase 1: Foundation (No Dependencies, Sequential)

**Goal**: Parallel identifier processing WITHOUT dependency ordering. Establishes baseline and validates parallel execution is correct.

### Step 1.1: Identifier Collector
- Extract all binding identifiers from AST
- Return flat list with metadata: `{ path, name, scopeSize, location }`
- **Test**: Parse known JS files, verify identifier count matches expected
- **Metric**: Collection time vs file size

### Step 1.2: Context Extractor
- Given an identifier, extract surrounding code context
- Configurable context window size
- Handles truncation gracefully
- **Test**: Context extraction produces valid JS snippets
- **Metric**: Extraction time per identifier

### Step 1.3: Simple Batch Scheduler (No Dependencies)
- Split identifiers into fixed-size batches
- No ordering consideration yet
- **Test**: All identifiers appear in exactly one batch
- **Metric**: Batch creation time

### Step 1.4: Parallel Executor
- Execute visitor function on batch items in parallel
- Configurable concurrency limit
- Collect results in order
- **Test**: Results maintain order, concurrency limit respected
- **Metric**: Throughput (identifiers/second) at various concurrency levels

### Step 1.5: AST Mutator
- Apply rename operations to AST
- Handle name collisions with prefix strategy
- Sequential mutations only (AST not thread-safe)
- **Test**: Renamed code parses correctly, no duplicate names
- **Metric**: Mutation time per identifier

### Step 1.6: Integration - TurboV2 Basic
- Wire all components together
- Process file end-to-end
- **Correctness Test**: Output equivalent to sequential processing (same renames, different order)
- **Performance Test**: Speedup vs sequential baseline

**Phase 1 Deliverable**: Working parallel rename with no dependency ordering. May produce lower quality names than sequential (context doesn't include prior renames).

---

## Phase 2: Dependency Ordering

**Goal**: Add dependency analysis so identifiers are processed in optimal order for LLM context.

### Step 2.1: Scope Hierarchy Builder
- Build parent-child relationship map for scopes
- O(scopes × identifiers) but simpler than v1
- **Test**: Hierarchy is valid tree, all identifiers have scope
- **Metric**: Build time vs identifier count

### Step 2.2: Dependency Analyzer (Scope Only)
- Create edges: outer scope → inner scope identifiers
- No reference analysis yet (simpler, faster)
- **Test**: No cycles in dependency graph
- **Metric**: Edge creation time, edge count

### Step 2.3: Topological Batch Scheduler
- Replace fixed batches with dependency-ordered batches
- Identifiers in batch N only depend on batches 0..N-1
- **Test**: Dependency invariant holds for all batches
- **Metric**: Batch count, max batch size

### Step 2.4: Integration - TurboV2 with Scope Dependencies
- Wire dependency analyzer into pipeline
- **Correctness Test**: All dependencies satisfied before processing
- **Quality Test**: Compare LLM output quality vs Phase 1 (should be better)
- **Performance Test**: Overhead of dependency analysis

**Phase 2 Deliverable**: Parallel rename with scope-based dependency ordering. Quality should be close to sequential mode.

---

## Phase 3: Reference Dependencies (Optional Enhancement)

**Goal**: Add reference-based dependencies for additional context quality.

### Step 3.1: Reference Index Builder
- Build map: identifier → identifiers it references
- Uses Babel binding analysis
- **Test**: Index captures actual references correctly
- **Metric**: Build time, memory usage

### Step 3.2: Extended Dependency Analyzer
- Add reference edges to dependency graph
- Mode toggle: `scope-only` | `with-references`
- **Test**: Reference edges are valid, no spurious edges
- **Metric**: Additional edges, impact on batch structure

### Step 3.3: Integration - TurboV2 Full Dependencies
- **Quality Test**: Compare output quality vs Phase 2
- **Performance Test**: Reference analysis overhead
- **Decision Point**: Is quality improvement worth the cost?

**Phase 3 Deliverable**: Full dependency ordering including references. Highest quality, higher setup cost.

---

## Phase 4: Batch Optimization

**Goal**: Optimize batch structure for better throughput.

### Step 4.1: Batch Merger
- Merge small consecutive batches (< minBatchSize)
- Respect dependency constraints
- **Test**: Merged batches still satisfy dependencies
- **Metric**: Batch count reduction, throughput improvement

### Step 4.2: Batch Splitter
- Split large batches (> maxBatchSize) to prevent memory spikes
- Simple chunking (no dependency consideration within split)
- **Test**: Split batches don't exceed size limit
- **Metric**: Memory usage reduction, throughput impact

### Step 4.3: Integration - TurboV2 Optimized Batches
- **Performance Test**: End-to-end throughput vs Phase 2/3
- **Memory Test**: Peak memory usage on large files

**Phase 4 Deliverable**: Optimized batch structure balancing parallelism and memory.

---

## Phase 5: Checkpointing

**Goal**: Enable resume from interruption.

### Step 5.1: Checkpoint Serializer
- Serialize: completed batches, renames map, partial code
- Efficient format (consider binary for large files)
- **Test**: Serialize/deserialize round-trip is lossless
- **Metric**: Serialization time and size

### Step 5.2: Checkpoint Validator
- Validate checkpoint matches current input
- Detect stale/incompatible checkpoints
- **Test**: Rejects modified input, accepts identical input
- **Metric**: Validation time

### Step 5.3: Resume Logic
- Restore state from checkpoint
- Continue from next uncompleted batch
- **Test**: Resumed processing produces same output as uninterrupted
- **Metric**: Resume overhead

### Step 5.4: Integration - TurboV2 with Checkpoints
- **Correctness Test**: Interrupt at random points, resume produces valid output
- **Performance Test**: Checkpoint save overhead per batch

**Phase 5 Deliverable**: Fully resumable turbo mode processing.

---

## Phase 6: Caching (Optional)

**Goal**: Cache expensive computations for repeated runs.

### Step 6.1: Dependency Graph Cache
- Cache dependency graph by code hash
- Invalidation on code change
- **Test**: Cache hit returns same graph as fresh computation
- **Metric**: Cache hit rate, time savings

### Step 6.2: Cache Management
- Size limits, eviction policy
- Handle cloud storage (iCloud, OneDrive, Dropbox) edge cases
- **Test**: Cache doesn't grow unbounded, handles errors gracefully
- **Metric**: Disk usage over time

**Phase 6 Deliverable**: Cached dependency graphs for faster repeated runs.

---

## Implementation Strategy

### File Structure
```
src/turbo-v2/
├── index.ts                    # Main entry point, exports turboV2Rename plugin
├── identifier-collector.ts     # Phase 1.1 - Find all binding identifiers
├── context-extractor.ts        # Phase 1.2 - Extract surrounding code
├── batch-scheduler.ts          # Phase 1.3, 2.3, 4.1-4.2 - Create/optimize batches
├── parallel-executor.ts        # Phase 1.4 - Run tasks with concurrency limit
├── ast-mutator.ts              # Phase 1.5 - Apply renames to AST
├── scope-hierarchy.ts          # Phase 2.1 - Build scope parent/child map
├── dependency-analyzer.ts      # Phase 2.2, 3.1-3.2 - Build dependency graph
├── checkpoint-manager.ts       # Phase 5 - Save/restore progress
├── openai-visitor.ts           # OpenAI API integration (visitor function)
├── types.ts                    # Shared types for v2
└── __tests__/
    ├── identifier-collector.test.ts
    ├── context-extractor.test.ts
    ├── batch-scheduler.test.ts
    ├── parallel-executor.test.ts
    ├── ast-mutator.test.ts
    ├── scope-hierarchy.test.ts
    ├── dependency-analyzer.test.ts
    ├── checkpoint-manager.test.ts
    └── integration.test.ts
```

### CLI Integration
```
src/commands/turbo-v2.ts        # New command: humanify turbo-v2 <file>
```

This keeps v2 completely isolated. The existing `--turbo` flag and all v1 code remain untouched.

### Validation Gates

After each phase:
1. **Unit Tests Pass**: All new components have tests
2. **Integration Tests Pass**: End-to-end flow works
3. **Regression Tests Pass**: Existing tests still pass
4. **Performance Benchmarks**: Recorded and compared to baseline
5. **Quality Assessment**: LLM output quality measured (sample files)

### Quality Metrics

For each test file, measure:
- **Correctness**: Output parses, no duplicate names, semantic equivalence
- **Name Quality**: Manual review of sample renames (10 random identifiers)
- **Performance**: Wall-clock time, peak memory

### Test Files

Use existing test infrastructure:
- Small: Quick smoke tests
- TensorFlow.js (1.4MB, ~35K identifiers): Medium scale
- Babylon.js (7.2MB, ~82K identifiers): Large scale stress test

---

## Success Criteria

### Phase 1 Complete When:
- [ ] All components implemented with tests
- [ ] 5x speedup over sequential on 100+ identifier files
- [ ] Output is semantically equivalent (same renames possible)

### Phase 2 Complete When:
- [ ] Dependency ordering produces valid batches
- [ ] Quality matches or exceeds sequential mode
- [ ] <10% overhead from dependency analysis

### Phase 3 Complete When (Optional):
- [ ] Reference dependencies measurably improve quality
- [ ] Or: Decision to skip based on cost/benefit analysis

### Phase 4 Complete When:
- [ ] Batch optimization improves throughput 20%+
- [ ] Peak memory reduced for large files

### Phase 5 Complete When:
- [ ] Can resume from any batch
- [ ] Resumed output identical to uninterrupted

### Phase 6 Complete When (Optional):
- [ ] 90%+ cache hit rate on repeated runs
- [ ] No disk space issues after 100 runs

---

## Risk Mitigation

### Risk: Reference analysis too slow at scale
**Mitigation**: Phase 3 is optional, scope-only dependencies may be sufficient

### Risk: Checkpoint format breaks compatibility
**Mitigation**: Version checkpoints, auto-invalidate on version mismatch

### Risk: Quality regression vs sequential
**Mitigation**: Quality gates at each phase, can fall back to sequential

### Risk: Memory issues with large batches
**Mitigation**: Aggressive batch splitting, streaming context extraction

---

## Comparison to V1

| Aspect | V1 | V2 |
|--------|----|----|
| Providers | OpenAI, Gemini, Local | OpenAI only (initially) |
| Reference index | Complex O(n × refs) | Same algorithm, cleaner code |
| Batch scheduling | Monolithic | Separate scheduler component |
| Checkpoint format | JSON (compatible) | New format (incompatible) |
| Testing | Integration-heavy | Unit tests per component |
| Dependencies | All-or-nothing | Incremental (scope, then refs) |
| Code organization | Single large file | Modular components |
| Code origin | Evolved from sequential | Fresh from scratch |

---

## Estimated Effort

| Phase | Effort | Priority |
|-------|--------|----------|
| Phase 1: Foundation | 2-3 days | Required |
| Phase 2: Scope Dependencies | 1-2 days | Required |
| Phase 3: Reference Dependencies | 1 day | Optional |
| Phase 4: Batch Optimization | 1 day | Required |
| Phase 5: Checkpointing | 1-2 days | Required |
| Phase 6: Caching | 1 day | Optional |

**Total: 6-10 days** depending on optional phases
