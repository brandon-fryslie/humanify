# Sprint Plan: Critical Gaps Resolution
**Generated**: 2025-12-29-235952
**Source**: STATUS-2024-12-29-gaps.md
**Scope**: Pre-implementation readiness - resolve 6 critical gaps before Phase 1
**Sprint Goal**: Enable turbo-v2 Phase 1 implementation with validated assumptions and complete specifications

---

## Executive Summary

**Current State**: Architecture defined, 6 critical gaps blocking implementation
**Total Gaps**: 6 critical (must resolve), 3 deferred (tune later)
**Estimated Effort**: 12-16 hours across 3 deliverables
**Risk**: MEDIUM-HIGH - Core hypothesis unvalidated until baselines complete
**Strategy**: Resolve specification gaps NOW (Gaps 2-6), validate hypothesis AFTER baselines run overnight

**Sprint Deliverables**:
1. **Specification Cleanup** - Resolve Gaps 2, 3, 6 (can do NOW)
2. **Robustness Design** - Resolve Gaps 4, 5 (can do NOW)
3. **Hypothesis Validation** - Resolve Gap 1 (requires baseline data from overnight run)

---

## Timeline & Dependencies

```
Day 0 (Tonight):
  └─ Baselines running overnight (user action)

Day 1 (NOW - no baseline data required):
  ├─ Deliverable 1: Specification Cleanup (Gaps 2, 3, 6) ✓ Can start immediately
  └─ Deliverable 2: Robustness Design (Gaps 4, 5) ✓ Can start immediately

Day 2 (AFTER baselines complete):
  └─ Deliverable 3: Hypothesis Validation (Gap 1) ⚠ Blocked until baselines finish

Gate:
  └─ If hypothesis FAILS → Pause for architecture rethink
  └─ If hypothesis PASSES → Proceed to Phase 1 implementation
```

---

## Deliverable 1: Specification Cleanup

**Priority**: P0 (Critical)
**Status**: Not Started
**Effort**: Medium (5-6 hours)
**Dependencies**: None - can start immediately
**Blocks**: Phase 1 implementation
**Spec Reference**: PROJECT_SPEC.md (entire file needs updates) • **Status Reference**: STATUS-2024-12-29-gaps.md sections "Gap 2", "Gap 3", "Gap 6"

### Description

The PROJECT_SPEC.md file has three categories of ambiguity that will cause implementation errors:
1. **Undefined success metrics** (Gap 2) - No objective definition of "done"
2. **Inconsistent terminology** (Gap 3) - Same concepts defined multiple ways
3. **Missing processor specifications** (Gap 6) - Interfaces exist, implementation details missing

This deliverable resolves all specification ambiguities so Phase 1 implementation can proceed without guesswork.

### Sub-Tasks

#### 1.1: Define Success Metric (Gap 2)
**Location**: PROJECT_SPEC.md line 627, score-semantic.ts methodology
**Current problem**: "Semantic score ≥ 95% of sequential baseline" is ambiguous
**Resolution required**:
- Document exact semantic score calculation in PROJECT_SPEC.md
- Specify numerical success threshold (absolute or relative)
- Define sampling strategy (200 identifiers vs full-file scoring)
- Create examples showing threshold application

**Estimated**: 1 hour

#### 1.2: Standardize Terminology (Gap 3)
**Location**: Multiple files - types.ts, PROJECT_SPEC.md, CLI examples
**Current problems**:
- "Anchor" has 3 conflicting definitions
- Processor names vary (rename vs llm-rename vs LLMRenameProcessor)
- Filter composition undefined (AND vs OR)

**Resolution required**:
- Define "anchor" with precise threshold in types.ts
- Choose ONE processor naming convention and apply everywhere
- Document filter composition rules with truth tables
- Add edge case examples to PROJECT_SPEC.md

**Estimated**: 2 hours

#### 1.3: Specify LLMRenameProcessor (Gap 6 - Phase 1 only)
**Location**: processors/llm-rename.ts spec needed
**Current problem**: Interface exists, no implementation details
**Resolution required**:
- Exact LLM prompt template
- Model parameters (temperature, max_tokens, response_format)
- Response parsing strategy
- Difference from existing openai-rename.ts
- Error handling for API failures

**Why only LLMRenameProcessor now**: Phase 1 only needs this processor. Other processors (refinement, anchor-detect, conflict-detect, consistency) will be specified when their phases are implemented.

**Estimated**: 2-3 hours

### Acceptance Criteria

- [ ] **Success Metric Defined**: PROJECT_SPEC.md section "Success Criteria" includes:
  - Exact formula for semantic score calculation
  - Numerical threshold (e.g., "score ≥ 0.95 × baseline" OR "score ≥ 90")
  - Sampling strategy (200 identifiers with confidence intervals OR full-file)
  - 3+ worked examples showing threshold application
  - Link to score-semantic.ts implementation

- [ ] **Terminology Standardized**: All files use consistent naming:
  - types.ts defines "anchor" with numerical threshold (e.g., "referenceCount > 5 OR scopeSize > 100")
  - All code/docs use SAME processor name (choose: "rename" everywhere OR "llm-rename" everywhere)
  - Filter composition documented with truth table (e.g., "filters are AND-ed", "onlyAnchors=true + skipHighConfidence=true = anchors with lowConfidence")
  - PROJECT_SPEC.md section "Terminology" added with definitions

- [ ] **LLMRenameProcessor Fully Specified**: processors/llm-rename.ts specification includes:
  - Complete prompt template with variable placeholders
  - Model config: temperature, max_tokens, response_format (JSON schema if structured output)
  - Response parsing logic with error handling
  - Retry strategy (count, backoff)
  - Documented difference from openai-rename.ts (e.g., "uses ProcessorResult format", "supports confidence scoring")
  - At least 3 edge cases documented (e.g., "empty context", "API timeout", "malformed response")

- [ ] **Specification Cross-Validation**: All examples in PROJECT_SPEC.md are consistent:
  - CLI examples use standardized processor names
  - Pass configuration examples use standardized filter syntax
  - Preset definitions reference processors by standard names
  - No contradictions between sections (run grep for processor names, check consistency)

- [ ] **Documentation Complete**: New sections added to PROJECT_SPEC.md:
  - "Success Criteria" section (~200 words)
  - "Terminology" section (~300 words)
  - "Processor Specifications" section with LLMRenameProcessor details (~500 words)
  - All sections include examples and edge cases

### Technical Notes

**For Gap 2 (Success Metric)**:
- Review existing `score-semantic.ts` implementation - it samples 200 identifiers and uses GPT-4o as judge
- Decision needed: Is sampling sufficient or do we need full-file scoring?
- Consider statistical confidence: 200 samples gives ±7% confidence interval at 95% confidence level
- Recommend: Document sampling approach + confidence intervals in spec

**For Gap 3a (Anchor Definition)**:
- APPROACHES-RESEARCH.md has 3 definitions - pick the most specific one (reference count + scope size)
- Recommend threshold: `isAnchor = referenceCount >= 3 OR scopeSize >= 50 OR type === 'exported'`
- This gives ~10-20% anchors which matches "top 10-20%" heuristic

**For Gap 3b (Processor Naming)**:
- Current code uses `openai-rename`, `gemini-rename`, `local-llm-rename`
- Recommend: Use `llm-rename` for V2 (generic, supports multiple providers)
- Avoid `rename` alone (too generic, conflicts with Babel rename)

**For Gap 6 (LLMRenameProcessor)**:
- Can reuse most of openai-rename.ts logic
- Key difference: Must return `ProcessorResult` with confidence field
- Prompt should be optimized for multi-pass (e.g., "improve this name" not "name this identifier")

---

## Deliverable 2: Robustness Design

**Priority**: P0 (Critical)
**Status**: Not Started
**Effort**: Medium (4-5 hours)
**Dependencies**: None - can start immediately
**Blocks**: Phase 5 (checkpointing), multi-pass quality
**Spec Reference**: PROJECT_SPEC.md checkpointing section, orchestrator spec • **Status Reference**: STATUS-2024-12-29-gaps.md sections "Gap 4", "Gap 5"

### Description

The multi-pass architecture and checkpointing system will fail without proper error handling and quality regression policies. This deliverable designs the robustness mechanisms BEFORE implementation so developers know what to build.

**Gap 4** (Error Handling): No specification for API failures, checkpoint corruption, config changes, concurrent access
**Gap 5** (Quality Regression): No policy for when pass N produces worse names than pass N-1

### Sub-Tasks

#### 2.1: Error Handling Specification (Gap 4)
**Current problem**: Multiple failure scenarios undefined
**Resolution required**:

**API Failure Handling**:
- Retry strategy: 3 attempts with exponential backoff (1s, 2s, 4s)
- Per-identifier failure: Log error, mark identifier as `skipped`, continue batch
- Batch-wide failure: Checkpoint partial results, report count of skipped identifiers
- Rate limit handling: Backoff with jitter, respect Retry-After headers

**Checkpoint Atomicity**:
- Write to temporary file, then atomic rename
- Detect partial writes via checksum or JSON validation
- Recovery: If checkpoint corrupted, use previous checkpoint + warn user
- Frequency: After each pass completion + every 100 identifiers within pass

**Config Change on Resume**:
- Detect config changes (compare checkpoint.config vs current config)
- Interactive mode: Prompt user (keep checkpoint config OR use new config)
- Non-interactive mode: ERROR with message "Config mismatch - use --force-config to override"
- Document via CLI help: `--force-config new` or `--force-config checkpoint`

**Concurrent Access**:
- File locking via `fs.promises.open()` with `O_EXCL` flag
- Lock acquisition failure → Error: "Checkpoint in use by another process"
- Lock released on process exit (handle SIGINT, SIGTERM gracefully)

**Estimated**: 2-3 hours

#### 2.2: Quality Regression Policy (Gap 5)
**Current problem**: No mechanism to prevent pass N from degrading quality
**Resolution required**:

**Regression Detection**:
- Track per-identifier history: `[{pass: 1, name: "config", confidence: 0.7}, {pass: 2, name: "configuration", confidence: 0.6}]`
- Regression definition: `confidence[N] < confidence[N-1] * 0.9` (10% drop threshold)
- Measure per-pass: After pass N completes, count regressions

**Handling Policy** (three options - choose one):
1. **Conservative**: Always keep higher-confidence name (rollback on regression)
2. **Progressive**: Always take latest name (assume LLM improves even with lower confidence)
3. **Configurable**: CLI flag `--regression-policy [keep-best|keep-latest]`

**Recommendation**: Option 3 (configurable) with default `keep-best`

**Stability Metric**:
- Track name changes per pass: `stabilityRate = (unchangedCount / totalCount)`
- Target: stabilityRate > 0.8 by pass 3 (80% of names stabilize)
- If stabilityRate < 0.5, warn user "High churn detected - consider fewer passes"

**Estimated**: 2 hours

### Acceptance Criteria

- [ ] **Error Handling Spec Complete**: PROJECT_SPEC.md section "Error Handling" documents:
  - API retry strategy with code example (3 attempts, exponential backoff)
  - Batch failure handling (checkpoint partial, continue)
  - Rate limit handling (backoff, Retry-After)
  - At least 2 error scenarios with expected behavior described

- [ ] **Checkpoint Atomicity Spec**: PROJECT_SPEC.md section "Checkpointing" includes:
  - Write strategy (temp file + atomic rename)
  - Corruption detection (checksum or JSON validation)
  - Recovery procedure (fallback to previous checkpoint)
  - Checkpoint frequency (per-pass + every 100 identifiers)
  - File locking mechanism (O_EXCL flag)

- [ ] **Config Change Handling Spec**: PROJECT_SPEC.md section "Resume Behavior" includes:
  - Detection logic (compare configs)
  - Interactive mode behavior (prompt user)
  - Non-interactive mode behavior (error + --force-config flag)
  - CLI help text for --force-config option

- [ ] **Quality Regression Policy Spec**: PROJECT_SPEC.md section "Quality Assurance" includes:
  - Regression definition (confidence drop threshold: 10%)
  - Identifier history tracking (data structure spec)
  - Policy options (keep-best vs keep-latest)
  - CLI flag: `--regression-policy [keep-best|keep-latest]` with default
  - Stability metric definition and warning thresholds

- [ ] **Edge Cases Documented**: PROJECT_SPEC.md includes examples of:
  - Disk full during checkpoint write → error message + recovery steps
  - Crash during pass execution → resume from last checkpoint
  - Two processes try to resume same checkpoint → second process errors immediately
  - Pass 2 has 30% regressions → behavior depends on --regression-policy flag
  - All passes produce same name → stabilityRate = 1.0, optimal outcome

- [ ] **Interfaces Updated**: types.ts includes:
  - `IdentifierHistory` type with pass, name, confidence fields
  - `CheckpointMetadata` type with config hash, timestamp, identifierCount
  - `RegressionPolicy` enum: 'keep-best' | 'keep-latest'
  - `ErrorHandlingConfig` type with retryCount, backoffMs, maxConcurrent fields

### Technical Notes

**For Gap 4 (Error Handling)**:
- Existing checkpoint code at `src/checkpoint/` can be referenced but V2 checkpointing is separate
- Use Node.js `fs.promises.open()` with `wx` mode for atomic create-or-fail locking
- For rate limits, respect OpenAI's `Retry-After` header (documented in their API)

**For Gap 5 (Quality Regression)**:
- Confidence field already in ProcessorResult interface (PROJECT_SPEC.md line 407)
- History tracking needed in orchestrator state
- Rollback is cheap: Just use previous name from history (no API call)
- Consider logging regressions to help tune thresholds: `console.warn("Pass 2 regressed 45 identifiers (8%)")`

**Implementation Note**:
- These specs are DESIGN ONLY for now
- Actual code implementation happens in Phase 5 (checkpointing) and Phase 2-3 (quality regression)
- Goal: Developers know WHAT to build before they start coding

---

## Deliverable 3: Hypothesis Validation

**Priority**: P0 (Critical - BLOCKS ALL IMPLEMENTATION)
**Status**: Blocked (waiting for baselines to complete overnight)
**Effort**: Small (2-3 hours, mostly API wait time)
**Dependencies**: Baseline measurements must complete first
**Blocks**: Everything - if hypothesis fails, architecture changes needed
**Spec Reference**: PROJECT_SPEC.md lines 29-53 (core hypothesis) • **Status Reference**: STATUS-2024-12-29-gaps.md section "Gap 1"

### Description

The entire turbo-v2 architecture assumes: **"Two parallel passes can match or exceed sequential quality."**

This assumption is currently UNVALIDATED. The quality numbers in PROJECT_SPEC.md (85-95%) are guesses marked with `~`.

**Critical risk**: If the hypothesis is false, the entire multi-pass architecture may be the wrong solution. We'd discover this after 2-3 weeks of implementation effort.

**This deliverable MUST complete before Phase 1 coding starts.**

### Sub-Tasks

#### 3.1: Complete Baseline Measurements
**Status**: PARTIAL (tiny-qs done, small-axios and medium-chart pending)
**Action**: User is running overnight, no work needed NOW
**Verification**: After baselines complete, check for:
- `test-samples/canonical/small-axios/output-sequential/baseline-scores.json`
- `test-samples/canonical/medium-chart/output-sequential/baseline-scores.json`

**Estimated**: 0 hours (user action, already in progress)

#### 3.2: Run Semantic Scoring on Baselines
**Status**: NEVER RUN (score-semantic.ts exists but never executed on baseline outputs)
**Action**: After baselines complete, run:
```bash
for sample in tiny-qs small-axios medium-chart; do
  ./scripts/score-semantic.ts \
    test-samples/canonical/$sample/original.js \
    test-samples/canonical/$sample/output-sequential/deobfuscated.js \
    > test-samples/canonical/$sample/output-sequential/semantic-score.json
done
```

**Expected output**: JSON with `{ score: 0-100, samples: [...], confidence: 0-1 }`

**Estimated**: 2-3 hours (mostly GPT-4o API latency)

#### 3.3: Measure 1-Pass Parallel Quality
**Status**: NOT YET IMPLEMENTED
**Action**: Need to implement quick 1-pass parallel mode OR use existing turbo V1
**Options**:
- Option A: Use existing turbo V1 with `--max-concurrent 50` (measures ~60-70% quality)
- Option B: Implement minimal 1-pass runner (just parallel, no dependencies)

**Recommendation**: Option A (faster, V1 already exists)

**Run**:
```bash
for sample in tiny-qs small-axios medium-chart; do
  humanify unminify test-samples/canonical/$sample/minified.js \
    --turbo --max-concurrent 50 \
    -o test-samples/canonical/$sample/output-1pass/

  ./scripts/score-semantic.ts \
    test-samples/canonical/$sample/original.js \
    test-samples/canonical/$sample/output-1pass/deobfuscated.js \
    > test-samples/canonical/$sample/output-1pass/semantic-score.json
done
```

**Estimated**: 2-3 hours (API latency)

#### 3.4: Measure 2-Pass Parallel Quality (STUB - Can't Do Yet)
**Status**: BLOCKED until Phase 1 implements 2-pass orchestrator
**Action**: DEFER until Phase 1 complete
**Note**: This is OK - we can validate 1-pass quality now, measure 2-pass after Phase 1

**Estimated**: 0 hours (deferred)

#### 3.5: Document Results and Make Go/No-Go Decision
**Status**: Pending measurement results
**Action**: Update PROJECT_SPEC.md lines 29-53 with ACTUAL numbers:

Replace:
```
| 2-Pass Parallel | 2n | 2n/30 × latency | ~85-95% |
```

With:
```
| Sequential | n | n × latency | 87% (measured) |
| 1-Pass Parallel | n | n/30 × latency | 64% (measured) |
| 2-Pass Parallel | 2n | 2n/30 × latency | TBD (Phase 1) |
```

**Decision criteria**:
- If 1-pass quality ≥ 60%: Hypothesis plausible, proceed to Phase 1
- If 1-pass quality < 50%: Hypothesis FAILS, rethink architecture
- If sequential quality < 80%: Baseline too low, need better test samples

**Estimated**: 1 hour

### Acceptance Criteria

- [ ] **Baselines Complete**: All 3 samples have sequential baseline measurements:
  - tiny-qs: ✓ Already complete
  - small-axios: baseline-scores.json exists with structuralMatchRate
  - medium-chart: baseline-scores.json exists with structuralMatchRate

- [ ] **Semantic Scores Measured**: All 3 samples have semantic scoring:
  - tiny-qs: semantic-score.json with score (0-100 scale)
  - small-axios: semantic-score.json with score (0-100 scale)
  - medium-chart: semantic-score.json with score (0-100 scale)
  - All scores ≥ 70 (validates baseline quality)

- [ ] **1-Pass Quality Measured**: All 3 samples have 1-pass parallel measurements:
  - tiny-qs: output-1pass/semantic-score.json
  - small-axios: output-1pass/semantic-score.json
  - medium-chart: output-1pass/semantic-score.json
  - Average 1-pass quality is 60-75% of sequential baseline

- [ ] **Hypothesis Validated**: PROJECT_SPEC.md updated with empirical data:
  - Table at lines 29-53 shows MEASURED numbers (no `~` prefix)
  - "Expected Quality" changed to "Measured Quality"
  - Baseline semantic scores documented (e.g., "Sequential: 87%")
  - 1-pass semantic scores documented (e.g., "1-Pass Parallel: 64%")
  - 2-pass marked as "TBD (Phase 1)"

- [ ] **Go/No-Go Decision Made**: STATUS report updated with decision:
  - If hypothesis plausible (1-pass ≥ 60%): "✓ Hypothesis validated, proceed to Phase 1"
  - If hypothesis fails (1-pass < 50%): "✗ Hypothesis rejected, pause for architecture rethink"
  - Rationale documented (e.g., "1-pass achieved 64% quality, suggesting 2-pass can reach 85%+")

- [ ] **Validation Results Archived**: New file created:
  - `test-samples/canonical/VALIDATION-RESULTS.md`
  - Contains table with all measurements (structural + semantic)
  - Documents measurement methodology
  - Includes timestamp and model used (e.g., "GPT-4o, 2025-12-29")

### Technical Notes

**Why 1-pass is sufficient for validation**:
- If 1-pass achieves 60-70%, it validates that parallel processing preserves reasonable quality
- 2-pass quality can be estimated: If pass 1 gives 70% context and pass 2 gives 100% context, expect ~85-90% quality
- We can't measure 2-pass until we BUILD 2-pass (chicken-and-egg)
- But we CAN measure 1-pass NOW using existing turbo V1

**Baseline quality expectations**:
- Sequential should score 80-90% (not 100% - LLMs make mistakes)
- If sequential < 70%, test samples may be too small or poorly chosen
- If sequential > 95%, test samples may be too easy

**Statistical significance**:
- 3 samples is minimum for trend detection
- Each sample scores 200 identifiers (score-semantic.ts implementation)
- 200 samples gives ±7% confidence interval at 95% confidence
- This is sufficient for go/no-go decision (not publication-grade, but good enough)

**API costs**:
- Semantic scoring uses GPT-4o (expensive model)
- 200 identifiers × 3 samples × 2 modes (sequential, 1-pass) = 1200 API calls
- Estimated cost: ~$5-10 for all measurements
- This is acceptable for validation

---

## Deferred Items (Not in This Sprint)

The following gaps can be resolved during implementation and do NOT block Phase 1:

### Gap 7: Context Size Optimization
**Why defer**: Empirical tuning question, can start with existing defaults (5000 chars)
**When**: Phase 1 performance testing

### Gap 8: Confidence Threshold Tuning
**Why defer**: Optimization, can implement simple policy (always reprocess) initially
**When**: Phase 2-3 optimization

### Gap 9: Batch Size Tuning
**Why defer**: Can use conservative defaults (min=3, max=100), tune based on profiling
**When**: Phase 4 optimization

### Other Processor Specifications (Gap 6 partial)
**Why defer**: Phase 1 only needs LLMRenameProcessor
**When**:
- RefinementProcessor: Phase 2
- AnchorDetectionProcessor: When implementing "anchor" preset
- ConflictDetectionProcessor: When implementing "quality" preset
- ConsistencyProcessor: When implementing "quality" preset

---

## Sprint Success Criteria

### Definition of Done

This sprint is complete when:

1. **All specifications are unambiguous**:
   - Success metric has numerical definition
   - All terminology is standardized
   - LLMRenameProcessor has complete implementation spec

2. **Robustness is designed**:
   - Error handling spec covers all failure modes
   - Quality regression policy is defined
   - Checkpoint atomicity is specified

3. **Hypothesis is validated OR rejected**:
   - Baseline semantic scores measured
   - 1-pass quality measured
   - PROJECT_SPEC.md updated with empirical data
   - Go/no-go decision documented

4. **Implementation can begin**:
   - Developer can implement Phase 1 without guessing
   - All interfaces and behaviors are specified
   - No critical unknowns remaining

### Metrics

- **Documentation**: 3 new sections added to PROJECT_SPEC.md (Success Criteria, Error Handling, Quality Assurance)
- **Consistency**: 0 naming conflicts in code examples
- **Validation**: 3/3 samples measured with semantic scores
- **Decision confidence**: ≥95% (based on statistical sampling)

### Exit Criteria

**Proceed to Phase 1 IF**:
- ✓ Deliverable 1 complete (specifications clean)
- ✓ Deliverable 2 complete (robustness designed)
- ✓ Deliverable 3 complete AND hypothesis validated (1-pass ≥ 60%)

**Pause for rethinking IF**:
- ✗ Hypothesis fails (1-pass < 50%)
- ✗ Baseline quality too low (sequential < 70%)
- ✗ Semantic scoring shows high variance (confidence < 0.8)

---

## Risk Assessment

### High Risk: Hypothesis Failure
**Probability**: 30%
**Impact**: Complete architecture change required
**Mitigation**: Validate FIRST (Deliverable 3) before implementation
**Fallback**: If fails, investigate alternative approaches (streaming, anchor-first, hybrid)

### Medium Risk: Quality Regression in Practice
**Probability**: 50%
**Impact**: Need sophisticated regression handling
**Mitigation**: Design policy NOW (Deliverable 2) before encountering in code
**Fallback**: If policy insufficient, add confidence-based filtering

### Medium Risk: Processor Implementations Vary
**Probability**: 70%
**Impact**: Inconsistent quality across presets
**Mitigation**: Fully specify LLMRenameProcessor (Deliverable 1) as template
**Fallback**: Iterative refinement based on measurements

### Low Risk: Baseline Measurements Fail
**Probability**: 10%
**Impact**: Can't validate hypothesis
**Mitigation**: User already running baselines, likely to succeed
**Fallback**: Run smaller samples or use existing tiny-qs data

---

## Recommended Execution Order

### Day 1 (Can start NOW - no baseline data needed)

**Morning (3-4 hours)**:
1. Start with Deliverable 1.2 (Standardize Terminology) - easiest, unblocks examples
2. Continue with Deliverable 1.1 (Define Success Metric) - document semantic scoring
3. Review Deliverable 2.1 (Error Handling) - design API retry strategy

**Afternoon (3-4 hours)**:
4. Complete Deliverable 1.3 (Specify LLMRenameProcessor) - critical for Phase 1
5. Complete Deliverable 2.2 (Quality Regression Policy) - design rollback logic
6. Finish Deliverable 2.1 (Error Handling) - complete checkpoint spec

**End of Day 1**:
- ✓ Deliverable 1 complete (Specifications clean)
- ✓ Deliverable 2 complete (Robustness designed)
- ⏳ Deliverable 3 blocked (waiting for baselines)

### Day 2 (After baselines complete overnight)

**Morning (1 hour)**:
1. Verify baselines complete (check for baseline-scores.json files)
2. Start semantic scoring (Deliverable 3.2) - runs in background

**Afternoon (2-3 hours)**:
3. Run 1-pass parallel measurements (Deliverable 3.3)
4. Document results (Deliverable 3.5)
5. Make go/no-go decision

**End of Day 2**:
- ✓ Deliverable 3 complete (Hypothesis validated)
- ✓ Sprint complete
- → Ready for Phase 1 implementation OR architecture rethink

---

## Files to Create/Update

### Updates Required
1. **PROJECT_SPEC.md** (src/turbo-v2/docs/_for-review/):
   - Add "Success Criteria" section (Gap 2)
   - Add "Terminology" section (Gap 3)
   - Add "Processor Specifications" section with LLMRenameProcessor (Gap 6)
   - Add "Error Handling" section (Gap 4)
   - Add "Quality Assurance" section (Gap 5)
   - Update lines 29-53 with empirical data (Gap 1)

2. **types.ts** (src/turbo-v2/types.ts - may not exist yet):
   - Define `AnchorCriteria` type with thresholds
   - Define `IdentifierHistory` type for quality tracking
   - Define `CheckpointMetadata` type
   - Define `RegressionPolicy` enum
   - Define `ErrorHandlingConfig` type

### New Files Required
1. **test-samples/canonical/VALIDATION-RESULTS.md**:
   - Document all baseline measurements
   - Track semantic scores over time
   - Include measurement methodology

2. **.agent_planning/turbo-v2/PROCESSOR-SPECS.md** (optional):
   - Detailed specs for all processors
   - Can be merged into PROJECT_SPEC.md instead

---

## Summary

**Total Items**: 3 deliverables addressing 6 critical gaps
**Estimated Effort**: 12-16 hours (8-10 hours NOW, 2-3 hours after baselines, ~3 hours API wait)
**Blocking Dependencies**: Baseline measurements (running overnight)
**Risk Level**: MEDIUM-HIGH (hypothesis unvalidated until Day 2)
**Outcome**: All critical gaps resolved, ready for Phase 1 implementation OR informed architecture pivot

**Key Insight**: By splitting work into "can do NOW" (Deliverables 1-2) and "needs baseline data" (Deliverable 3), we maximize productivity while baselines run overnight. Day 1 work is completely independent and unblocks specification questions.
