# Definition of Done: Critical Gaps Resolution Sprint
**Generated**: 2025-12-29-235952
**Plan**: PLAN-2025-12-29-235952.md
**Source**: STATUS-2024-12-29-gaps.md

---

## Sprint Scope

**This sprint delivers**:
1. Clean, unambiguous specifications (Gaps 2, 3, 6)
2. Complete robustness design (Gaps 4, 5)
3. Validated or rejected core hypothesis (Gap 1)

**Deferred to implementation**:
- Context size tuning (Gap 7)
- Confidence threshold tuning (Gap 8)
- Batch size tuning (Gap 9)
- Processor specs beyond LLMRenameProcessor

---

## Acceptance Criteria

### Deliverable 1: Specification Cleanup

#### Success Metric Defined (Gap 2)
- [ ] PROJECT_SPEC.md has new "Success Criteria" section (~200 words)
- [ ] Semantic score calculation formula documented precisely
- [ ] Numerical success threshold specified (e.g., "score ≥ 0.95 × baseline" OR "score ≥ 90")
- [ ] Sampling strategy defined (200 identifiers with confidence intervals OR full-file)
- [ ] At least 3 worked examples showing threshold application
- [ ] Link to score-semantic.ts implementation included
- [ ] Examples show both passing and failing scenarios

#### Terminology Standardized (Gap 3)
- [ ] types.ts defines "anchor" with numerical threshold (e.g., "referenceCount > 5 OR scopeSize > 100")
- [ ] All code examples use consistent processor naming (all "rename" OR all "llm-rename", no mixing)
- [ ] All CLI examples use consistent processor naming
- [ ] Filter composition documented with truth table
- [ ] PROJECT_SPEC.md has new "Terminology" section (~300 words)
- [ ] Edge case documented: onlyAnchors=true + skipHighConfidence=true behavior specified
- [ ] Cross-validation complete: grep shows 0 naming inconsistencies in examples

#### LLMRenameProcessor Fully Specified (Gap 6)
- [ ] PROJECT_SPEC.md has new "Processor Specifications" section (~500 words)
- [ ] Complete prompt template with variable placeholders (e.g., `{identifierName}`, `{context}`)
- [ ] Model config specified: temperature, max_tokens, response_format
- [ ] Response parsing logic documented with error handling
- [ ] Retry strategy specified (count: 3, backoff: exponential)
- [ ] Difference from openai-rename.ts documented (at least 2 key differences)
- [ ] At least 3 edge cases documented:
  - [ ] Empty context handling
  - [ ] API timeout handling
  - [ ] Malformed response handling

#### Documentation Cross-Validation
- [ ] All CLI examples in PROJECT_SPEC.md use standardized names
- [ ] All pass configuration examples use standardized filter syntax
- [ ] All preset definitions reference processors by standard names
- [ ] No contradictions between sections (manual review confirms consistency)

---

### Deliverable 2: Robustness Design

#### Error Handling Specification (Gap 4)
- [ ] PROJECT_SPEC.md has new "Error Handling" section
- [ ] API retry strategy documented with code example (3 attempts, exponential backoff: 1s, 2s, 4s)
- [ ] Per-identifier failure behavior: log error, mark as skipped, continue batch
- [ ] Batch-wide failure behavior: checkpoint partial results, report skipped count
- [ ] Rate limit handling: backoff with jitter, respect Retry-After headers
- [ ] At least 2 error scenarios with expected behavior:
  - [ ] Scenario: "API returns 500 for identifier #50 of 100"
  - [ ] Scenario: "Rate limit exceeded mid-batch"

#### Checkpoint Atomicity Specification (Gap 4)
- [ ] PROJECT_SPEC.md "Checkpointing" section updated
- [ ] Write strategy specified: temp file + atomic rename
- [ ] Corruption detection method: checksum OR JSON validation
- [ ] Recovery procedure: fallback to previous checkpoint + warn user
- [ ] Checkpoint frequency: after each pass + every 100 identifiers
- [ ] File locking mechanism: fs.promises.open() with O_EXCL flag
- [ ] Concurrent access handling: "Checkpoint in use" error

#### Config Change Handling Specification (Gap 4)
- [ ] PROJECT_SPEC.md has "Resume Behavior" section
- [ ] Detection logic: compare checkpoint.config vs current config
- [ ] Interactive mode: prompt user with options (keep checkpoint config OR use new config)
- [ ] Non-interactive mode: ERROR with message "Config mismatch - use --force-config to override"
- [ ] CLI flag documented: --force-config [new|checkpoint]
- [ ] CLI help text includes --force-config explanation

#### Quality Regression Policy (Gap 5)
- [ ] PROJECT_SPEC.md has new "Quality Assurance" section
- [ ] Regression definition: confidence drop threshold ≥ 10%
- [ ] Identifier history tracking data structure specified
- [ ] Policy options documented: keep-best vs keep-latest
- [ ] CLI flag specified: --regression-policy [keep-best|keep-latest] with default
- [ ] Stability metric defined: stabilityRate = (unchangedCount / totalCount)
- [ ] Warning threshold: stabilityRate < 0.5 triggers "High churn detected" warning

#### Edge Cases Documented (Gaps 4 & 5)
- [ ] Disk full during checkpoint write → error message + recovery steps documented
- [ ] Crash during pass execution → resume from last checkpoint (documented)
- [ ] Two processes try to resume same checkpoint → second process errors immediately (documented)
- [ ] Pass 2 has 30% regressions → behavior depends on --regression-policy flag (documented)
- [ ] All passes produce same name → stabilityRate = 1.0 (optimal, documented)

#### Type Interfaces Updated (Gaps 4 & 5)
- [ ] types.ts includes IdentifierHistory type: `{ pass: number, name: string, confidence: number }`
- [ ] types.ts includes CheckpointMetadata type: `{ configHash: string, timestamp: number, identifierCount: number }`
- [ ] types.ts includes RegressionPolicy enum: `'keep-best' | 'keep-latest'`
- [ ] types.ts includes ErrorHandlingConfig type: `{ retryCount: number, backoffMs: number[], maxConcurrent: number }`

---

### Deliverable 3: Hypothesis Validation

#### Baselines Complete (Gap 1 - Step 3.1)
- [ ] tiny-qs: baseline-scores.json exists (already complete ✓)
- [ ] small-axios: baseline-scores.json exists with structuralMatchRate
- [ ] medium-chart: baseline-scores.json exists with structuralMatchRate

#### Semantic Scores Measured (Gap 1 - Step 3.2)
- [ ] tiny-qs: semantic-score.json exists with score (0-100 scale)
- [ ] small-axios: semantic-score.json exists with score (0-100 scale)
- [ ] medium-chart: semantic-score.json exists with score (0-100 scale)
- [ ] All semantic scores ≥ 70 (validates baseline quality is sufficient)
- [ ] Score confidence ≥ 0.8 (validates measurement reliability)

#### 1-Pass Parallel Quality Measured (Gap 1 - Step 3.3)
- [ ] tiny-qs: output-1pass/ directory with deobfuscated.js
- [ ] tiny-qs: output-1pass/semantic-score.json
- [ ] small-axios: output-1pass/ directory with deobfuscated.js
- [ ] small-axios: output-1pass/semantic-score.json
- [ ] medium-chart: output-1pass/ directory with deobfuscated.js
- [ ] medium-chart: output-1pass/semantic-score.json
- [ ] Average 1-pass quality is 60-75% of sequential baseline

#### Hypothesis Validated (Gap 1 - Step 3.5)
- [ ] PROJECT_SPEC.md lines 29-53 updated with empirical data
- [ ] Table shows MEASURED numbers (no `~` prefix on measured values)
- [ ] "Expected Quality" column renamed to "Measured Quality"
- [ ] Baseline semantic scores documented (e.g., "Sequential: 87% (measured)")
- [ ] 1-pass semantic scores documented (e.g., "1-Pass Parallel: 64% (measured)")
- [ ] 2-pass row marked "TBD (requires Phase 1 implementation)"

#### Go/No-Go Decision Made (Gap 1 - Step 3.5)
- [ ] Decision documented in STATUS report or PROJECT_SPEC.md
- [ ] IF 1-pass quality ≥ 60%: "✓ Hypothesis validated - proceed to Phase 1"
- [ ] IF 1-pass quality < 50%: "✗ Hypothesis rejected - pause for architecture rethink"
- [ ] Rationale included (e.g., "1-pass achieved 64%, suggesting 2-pass can reach 85%+")
- [ ] Alternative approaches considered if hypothesis fails (streaming, anchor-first, hybrid)

#### Validation Results Archived (Gap 1)
- [ ] File created: test-samples/canonical/VALIDATION-RESULTS.md
- [ ] Contains table with all measurements (structural + semantic)
- [ ] Documents measurement methodology:
  - [ ] Model used (e.g., "GPT-4o")
  - [ ] Timestamp (e.g., "2025-12-29")
  - [ ] Sample size (e.g., "200 identifiers per file")
  - [ ] Confidence intervals
- [ ] Includes both sequential and 1-pass results
- [ ] Formatted for future tracking (can append 2-pass results later)

---

## Sprint Exit Criteria

### PROCEED to Phase 1 Implementation IF:
- ✓ All Deliverable 1 criteria met (specifications clean and complete)
- ✓ All Deliverable 2 criteria met (robustness fully designed)
- ✓ All Deliverable 3 criteria met (hypothesis validated)
- ✓ 1-pass quality ≥ 60% of sequential baseline
- ✓ No critical ambiguities remain in PROJECT_SPEC.md

### PAUSE for Architecture Rethink IF:
- ✗ Hypothesis fails (1-pass < 50% of sequential)
- ✗ Baseline quality too low (sequential < 70%)
- ✗ High measurement variance (confidence < 0.8)
- ✗ Semantic scoring reveals fundamental issues

### ITERATE on Sprint IF:
- ⚠ Specifications have inconsistencies discovered during review
- ⚠ Error handling spec missing critical failure modes
- ⚠ Baseline measurements incomplete or inconclusive

---

## Verification Checklist

### Documentation Quality
- [ ] PROJECT_SPEC.md increased by ~1500 words (5 new sections)
- [ ] All new sections have examples
- [ ] All edge cases have documented behavior
- [ ] No TODOs or TBDs in specification sections (except 2-pass measurement)
- [ ] Cross-references between sections are accurate

### Consistency Validation
- [ ] Manual grep for processor names shows single consistent pattern
- [ ] All CLI examples can be parsed by same syntax
- [ ] Filter truth table covers all combinations
- [ ] Type definitions match specification prose

### Measurement Validity
- [ ] All 3 samples measured with same methodology
- [ ] Measurement timestamps within 24 hours of each other
- [ ] Same model used for all semantic scoring
- [ ] Results documented with uncertainty/confidence

### Completeness Check
- [ ] All 6 critical gaps addressed (Gaps 1-6)
- [ ] All 3 deferred gaps acknowledged (Gaps 7-9)
- [ ] All acceptance criteria have checkboxes
- [ ] All checkboxes are verifiable (specific, testable)

---

## Success Metrics

**Quantitative**:
- 6/6 critical gaps resolved
- 5 new sections in PROJECT_SPEC.md
- 3/3 baseline samples measured
- 0 naming inconsistencies in examples
- 3+ error scenarios documented
- 5+ edge cases documented

**Qualitative**:
- Developer can implement Phase 1 without guessing implementation details
- All "how should I...?" questions have answers in spec
- No contradictions or ambiguities in specification
- Hypothesis validation gives high confidence in approach

---

## Notes

**Priority**: All acceptance criteria are REQUIRED. None are optional.

**Validation**: Each checkbox should be verified by:
1. Reading the relevant file/section
2. Confirming the specific content exists
3. Checking examples/edge cases are present
4. Verifying consistency across references

**Timeline**:
- Deliverable 1 + 2: Can complete NOW (Day 1, 8-10 hours)
- Deliverable 3: Requires baselines to finish (Day 2, 2-3 hours + API wait)

**Cost**:
- Semantic scoring: ~$5-10 in API costs (GPT-4o)
- 1-pass measurements: ~$20-30 in API costs (renaming)
- Total: ~$25-40 for full validation

**Risk**: If hypothesis fails, sprint is still successful - we've validated the approach BEFORE spending weeks implementing the wrong architecture.
