# Sprint Plan: Experiment Dashboard Real Data Integration

**Generated**: 2025-12-31-031329
**Source STATUS**: STATUS-2025-12-31.md
**Sprint Duration**: 1 sprint (3-5 days)
**Priority**: P0 - Critical functionality blocker

---

## Executive Summary

**Current State**: Dashboard UI is complete and functional, but experiments show all zeros because the processing pipeline is not integrated. The turbo-v2 infrastructure (Sprints 1-9) exists and is tested, but the main CLI does not route to it.

**Root Causes**:
1. Main CLI (`src/cli.ts`) does NOT have `--turbo-v2` flag wired up
2. Sample file paths are incorrect (runner executes from webapp dir, samples at project root)
3. No bridge from turbo-v2 metrics output to dashboard storage format
4. Output files written to checkpoint directory, not dashboard expected location

**Sprint Goal**: Make experiments execute successfully and display real processing data (scores, tokens, costs, identifier counts).

**Total Gap**: 3 critical integration points + metrics extraction
- CLI Integration (BLOCKER)
- Path Configuration (HIGH)
- Metrics Bridge (HIGH)

**Recommended Focus**: Limit to 2 deliverables for this sprint:
1. **End-to-End Execution** (CLI integration + path fixes + output handling)
2. **Metrics Display** (token/cost tracking and extraction)

Deferred for future sprint:
- Console log capture (nice-to-have for debugging)
- Live progress tracking (requires WebSocket/SSE)
- Sample validation at creation time

---

## Sprint Backlog

### P0 (Critical) - Enable Basic Experiment Execution

---

## [P0] Deliverable 1: End-to-End Experiment Execution

**Status**: Not Started
**Effort**: Medium (4-6 hours)
**Dependencies**: None
**Spec Reference**: CLAUDE.md (Turbo Mode, Plugin Architecture) • **Status Reference**: STATUS-2025-12-31.md sections "Gap 1: CLI Integration Missing", "Gap 2: Sample File Path Mismatch", "Gap 4: Output Path Not Standardized"

### Description
Wire the turbo-v2 processing pipeline into the main CLI and configure paths so that dashboard experiments can successfully execute processing runs. Currently, the dashboard calls `npx tsx src/cli.ts unminify ... --turbo-v2` but this flag does not exist, causing all experiments to fail or fall back to sequential processing.

**Critical Changes**:
1. Add `--turbo-v2` flag to main CLI that routes to `executeTurboV2()` from `turbo-v2-command.ts`
2. Fix sample file paths in `run-experiment.ts` to use absolute paths from project root
3. Configure turbo-v2 to write final output to dashboard-expected location (`.humanify-experiments/outputs/{expId}/{sample}/output.js`)
4. Ensure scoring script receives correct file paths

**Technical Approach**:
- Main CLI routing: Import and call `executeTurboV2(options)` when `--turbo-v2` flag is present
- Path resolution: Use `path.resolve(__dirname, '../../../test-samples/canonical/...')` in `run-experiment.ts`
- Output handling: After turbo-v2 completes, copy final snapshot from `.humanify-checkpoints/{jobId}/snapshots/after-pass-N.js` to `.humanify-experiments/outputs/{expId}/{sample}/output.js`

### Acceptance Criteria
- [ ] Main CLI (`src/cli.ts`) accepts `--turbo-v2` flag and routes to `executeTurboV2()` function
- [ ] Running `npx tsx src/cli.ts unminify test-samples/canonical/tiny-qs/minified.js --turbo-v2 --preset fast --output /tmp/test-output.js` completes successfully and writes output file
- [ ] Dashboard runner (`run-experiment.ts`) resolves sample paths correctly (no "Sample file not found" errors)
- [ ] After experiment runs, output file exists at `.humanify-experiments/outputs/{expId}/{sample}/output.js`
- [ ] Scoring script (`score-semantic.ts`) executes successfully with original and output files
- [ ] Experiment result shows non-zero score (proves end-to-end execution worked)
- [ ] Unit test: CLI flag parsing includes `--turbo-v2` and passes correct options to executor
- [ ] E2E test: Create experiment via API, run it, verify output file created and score populated

### Technical Notes
- **Option B recommended**: Copy final snapshot after completion (maintains separation between checkpoint isolation and dashboard storage)
- Checkpoint snapshots are at `.humanify-checkpoints/{jobId}/snapshots/after-pass-{N}.js`
- Dashboard expects outputs at `.humanify-experiments/outputs/{experimentId}/{sampleName}/output.js`
- The `executeTurboV2()` function already exists and is fully implemented in `src/turbo-v2/cli/turbo-v2-command.ts`
- Sample files confirmed to exist at `/Users/bmf/code/brandon-fryslie_humanify/test-samples/canonical/{tiny-qs,small-axios,medium-chart}/minified.js`
- Runner executes from `src/turbo-v2/webapp/` so relative paths must account for this working directory

---

## [P0] Deliverable 2: Token/Cost Tracking and Metrics Display

**Status**: Not Started
**Effort**: Medium (3-4 hours)
**Dependencies**: Deliverable 1 (requires successful execution to generate metrics)
**Spec Reference**: CLAUDE.md (Observability & Performance) • **Status Reference**: STATUS-2025-12-31.md section "Gap 3: Token/Cost Tracking Not Captured"

### Description
Extract token usage and identifier statistics from turbo-v2 metrics output and display them in the dashboard. Currently, all experiments show `tokens: undefined, cost: undefined, identifiersProcessed: undefined` because there's no bridge from the `MetricsCollector` output (written to `logs/metrics.jsonl`) to the dashboard's `SampleResult` format.

**Critical Changes**:
1. Create `parseMetrics()` function to read and aggregate data from `logs/metrics.jsonl`
2. Implement `calculateCost()` function with model-specific pricing for OpenAI models
3. Update `run-experiment.ts` to call parseMetrics after execution completes
4. Populate all `SampleResult` fields: `tokens`, `cost`, `identifiersProcessed`, `identifiersRenamed`

**Metrics Format** (from MetricsCollector):
```json
{
  "type": "pass_completed",
  "stats": {
    "identifiersProcessed": 150,
    "identifiersRenamed": 142,
    "tokensUsed": { "prompt": 42000, "completion": 6000, "total": 48000 },
    "duration": 82100
  }
}
```

**Cost Calculation** (model pricing as of 2025-12-31):
| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| gpt-4o-mini | $0.150 | $0.600 |
| gpt-4o | $2.50 | $10.00 |

### Acceptance Criteria
- [ ] `parseMetrics(metricsFilePath)` function reads `logs/metrics.jsonl` and returns aggregated data with: `{ identifiersProcessed, identifiersRenamed, tokensUsed: { promptTokens, completionTokens, totalTokens } }`
- [ ] `calculateCost(tokens, modelName)` function returns `CostBreakdown` with: `{ inputCost, outputCost, totalCost }` using correct pricing for gpt-4o-mini and gpt-4o
- [ ] After experiment completes, `SampleResult.tokens` is populated with non-zero values
- [ ] After experiment completes, `SampleResult.cost` is populated with calculated cost breakdown
- [ ] After experiment completes, `SampleResult.identifiersProcessed` and `SampleResult.identifiersRenamed` are populated
- [ ] Dashboard UI displays token counts in experiment detail view
- [ ] Dashboard UI displays cost breakdown in experiment detail view
- [ ] Dashboard UI displays identifier statistics in experiment detail view
- [ ] Unit test: `calculateCost({ promptTokens: 1000, completionTokens: 500 }, 'gpt-4o-mini')` returns ~$0.000225
- [ ] Unit test: `parseMetrics()` correctly aggregates metrics from multi-pass experiments (sums tokens across all passes)
- [ ] E2E test: Run experiment, verify all metrics fields are non-zero and accurate

### Technical Notes
- **MetricsCollector location**: Metrics written to `logs/metrics.jsonl` by turbo-v2 pipeline
- **Multi-pass aggregation**: For N-pass experiments, sum all `pass_completed` events to get total tokens/identifiers
- **Error handling**: If metrics file doesn't exist or is malformed, return default values (zeros) and log warning
- **Cost precision**: Round to 6 decimal places for display (costs can be very small for tiny samples)
- **Model detection**: Extract model name from experiment config (dashboard stores preset, need to map preset → model)
- **Pricing updates**: Model pricing should be configurable (consider moving to config file for future updates)

---

## Priority Summary

| Priority | Deliverables | Total Effort | Items |
|----------|-------------|--------------|-------|
| P0 (Critical) | 2 | 7-10 hours | End-to-End Execution, Metrics Display |

---

## Dependency Graph

```
Deliverable 1: End-to-End Execution
  ├─ No dependencies (BLOCKER - start immediately)
  └─ Enables: Deliverable 2

Deliverable 2: Metrics Display
  └─ Depends on: Deliverable 1 (needs successful execution to generate metrics)
```

---

## Recommended Sprint Execution Order

### Day 1-2: Unblock Execution
1. **CLI Integration** (2-3 hours)
   - Edit `src/cli.ts` to add `--turbo-v2` flag
   - Import and route to `executeTurboV2()` from `turbo-v2-command.ts`
   - Test: `npx tsx src/cli.ts unminify test-samples/canonical/tiny-qs/minified.js --turbo-v2 --preset fast --output /tmp/test.js`

2. **Path Configuration** (1-2 hours)
   - Update `SAMPLES_DIR` in `src/turbo-v2/webapp/server/lib/run-experiment.ts` to use absolute paths
   - Test: Verify samples resolve correctly from webapp working directory

3. **Output Handling** (1-2 hours)
   - After turbo-v2 execution completes, copy final snapshot to dashboard output location
   - Implement copy logic in `run-experiment.ts`
   - Test: Verify output file exists at expected location after run

4. **End-to-End Validation** (1 hour)
   - Start webapp: `cd src/turbo-v2/webapp && npm start`
   - Create experiment via UI with preset="fast", samples=["tiny-qs"]
   - Run experiment
   - Verify: non-zero score appears in UI (proves scoring worked)

### Day 3: Metrics Integration
5. **Metrics Parsing** (2-3 hours)
   - Implement `parseMetrics()` in `src/turbo-v2/webapp/server/lib/metrics-parser.ts`
   - Handle multi-pass aggregation (sum across all `pass_completed` events)
   - Test: Parse real metrics.jsonl from completed experiment

6. **Cost Calculation** (1 hour)
   - Implement `calculateCost()` with model-specific pricing
   - Test: Verify calculations match OpenAI pricing

7. **Integration** (1 hour)
   - Update `run-experiment.ts` to call parseMetrics and calculateCost
   - Populate all SampleResult fields
   - Test: Run experiment, verify all fields populated in experiments.json

### Day 4-5: Testing & Polish
8. **Unit Tests** (2 hours)
   - CLI flag parsing test
   - calculateCost test with known inputs
   - parseMetrics test with mock metrics.jsonl

9. **E2E Tests** (2 hours)
   - Full experiment lifecycle test (create → run → verify results)
   - Multi-sample experiment test
   - Error handling test (missing files, malformed metrics)

10. **Documentation** (1 hour)
    - Update webapp README with setup instructions
    - Document metrics file format
    - Add troubleshooting section for common errors

---

## Out of Scope (Deferred)

The following items are NOT included in this sprint:

### Deferred to Future Sprint
1. **Console Log Capture** (2-3 hours)
   - Capturing stdout/stderr during execution
   - Structuring as `ConsoleLogEntry[]`
   - Storing in experiments.json
   - **Rationale**: Nice-to-have for debugging, not critical for MVP

2. **Live Progress Tracking** (4-6 hours)
   - Real-time progress updates via SSE/WebSocket
   - Streaming ProgressRenderer output to dashboard
   - Per-pass/batch progress display
   - **Rationale**: Requires significant infrastructure, low ROI for MVP

3. **Sample Validation at Creation** (1 hour)
   - Validate sample files exist before creating experiment
   - Show warnings in UI if samples missing
   - **Rationale**: Low priority since execution will fail fast anyway

4. **Advanced Error Handling** (2 hours)
   - Retry logic for transient failures
   - Partial result saving on crash
   - **Rationale**: Edge cases, not critical for happy path

---

## Risk Assessment

### High Risk Items
1. **Checkpoint-to-Output Copying**
   - **Risk**: Turbo-v2 checkpoint structure might change between runs
   - **Mitigation**: Use stable snapshot naming convention (`after-pass-N.js`), document dependency
   - **Contingency**: If structure changes, update copy logic in run-experiment.ts

2. **Metrics File Location**
   - **Risk**: MetricsCollector might write to different location based on working directory
   - **Mitigation**: Test with different working directories, use absolute paths
   - **Contingency**: Configure metrics output path explicitly in turbo-v2

### Medium Risk Items
3. **Cost Calculation Accuracy**
   - **Risk**: OpenAI pricing changes without notice
   - **Mitigation**: Move pricing to config file, add unit tests with known values
   - **Contingency**: Add "as of date" to cost display, link to pricing page

4. **Multi-Pass Token Aggregation**
   - **Risk**: Edge cases in summing tokens across passes (retries, errors)
   - **Mitigation**: Test with 2-pass and 3-pass experiments, handle missing events
   - **Contingency**: Fall back to last completed pass if aggregation fails

### Low Risk Items
5. **Sample Path Resolution**
   - **Risk**: Different environments have different project root locations
   - **Mitigation**: Use `__dirname` and relative navigation, test on different machines
   - **Contingency**: Make SAMPLES_DIR configurable via environment variable

---

## Success Criteria

### Sprint Complete When:
- [ ] User can create experiment with preset + samples via UI
- [ ] User can click "Run" and experiment executes without errors
- [ ] Experiment detail view shows:
  - Non-zero semantic similarity score
  - Token usage (promptTokens, completionTokens, totalTokens)
  - Cost breakdown (inputCost, outputCost, totalCost)
  - Identifier statistics (identifiersProcessed, identifiersRenamed)
  - Processing duration
- [ ] Compare view can chart metrics across multiple experiments
- [ ] All unit and E2E tests pass
- [ ] End-to-end smoke test succeeds: create → run → verify data

### Quality Gates
- [ ] No regressions in existing turbo-v2 functionality (all Sprint 1-9 tests still pass)
- [ ] No hardcoded paths (use path resolution from project root)
- [ ] Error handling for missing files, malformed metrics
- [ ] Code coverage >80% for new functions (parseMetrics, calculateCost)

---

## Testing Strategy

### Unit Tests (New)
1. **CLI Flag Parsing**
   ```typescript
   test('CLI accepts --turbo-v2 flag', () => {
     const args = parseArgs(['unminify', 'input.js', '--turbo-v2']);
     expect(args.turboV2).toBe(true);
   });
   ```

2. **Cost Calculation**
   ```typescript
   test('calculateCost for gpt-4o-mini', () => {
     const cost = calculateCost({ promptTokens: 1000, completionTokens: 500 }, 'gpt-4o-mini');
     expect(cost.totalCost).toBeCloseTo(0.000225, 6);
   });
   ```

3. **Metrics Parsing**
   ```typescript
   test('parseMetrics aggregates multi-pass data', () => {
     const metrics = parseMetrics('test-fixtures/metrics-2pass.jsonl');
     expect(metrics.identifiersProcessed).toBe(300); // 150 + 150
     expect(metrics.tokensUsed.totalTokens).toBe(96000); // 48000 + 48000
   });
   ```

### E2E Tests (New)
1. **Full Experiment Lifecycle**
   ```bash
   # Start server
   cd src/turbo-v2/webapp && npm start &

   # Create experiment
   curl -X POST http://localhost:3001/api/experiments \
     -H "Content-Type: application/json" \
     -d '{"preset":"fast","samples":["tiny-qs"]}'
   # Response: { "id": "exp-123", ... }

   # Run experiment
   curl -X POST http://localhost:3001/api/experiments/exp-123/run

   # Poll status until complete
   curl http://localhost:3001/api/experiments/exp-123/status
   # Wait for: { "status": "completed", "completedSamples": 1, "totalSamples": 1 }

   # Verify results
   curl http://localhost:3001/api/experiments/exp-123
   # Assert: score > 0, tokens.totalTokens > 0, cost.totalCost > 0
   ```

2. **Multi-Sample Experiment**
   ```bash
   # Create experiment with all 3 samples
   curl -X POST http://localhost:3001/api/experiments \
     -d '{"preset":"fast","samples":["tiny-qs","small-axios","medium-chart"]}'

   # Run and verify all samples have results
   ```

### Regression Tests
- [ ] Run existing turbo-v2 unit tests: `npm run test:unit`
- [ ] Run existing turbo-v2 e2e tests: `npm run test:e2e`
- [ ] Verify checkpoint/resume still works with new output copying

---

## Implementation Checklist

### Phase 1: CLI Integration (BLOCKER)
- [ ] Edit `src/cli.ts` to add `--turbo-v2` boolean flag to commander options
- [ ] Import `executeTurboV2` from `src/turbo-v2/cli/turbo-v2-command.ts`
- [ ] Add conditional: if `options.turboV2` then call `executeTurboV2(options)` else use existing logic
- [ ] Test manual execution: `npx tsx src/cli.ts unminify test-samples/canonical/tiny-qs/minified.js --turbo-v2 --preset fast --output /tmp/test.js`
- [ ] Verify output file created at /tmp/test.js
- [ ] Write unit test for flag parsing

### Phase 2: Path Configuration
- [ ] Update `SAMPLES_DIR` in `src/turbo-v2/webapp/server/lib/run-experiment.ts`:
  ```typescript
  const SAMPLES_DIR = path.resolve(__dirname, '../../../../test-samples/canonical');
  ```
- [ ] Update input path construction to use SAMPLES_DIR
- [ ] Test from webapp working directory: verify paths resolve correctly
- [ ] Handle case where samples don't exist (throw error early)

### Phase 3: Output Handling
- [ ] After `execSync` completes in `run-experiment.ts`, locate final snapshot:
  - Parse checkpoint directory (`.humanify-checkpoints/{jobId}/`)
  - Find `snapshots/after-pass-N.js` with highest N
- [ ] Create output directory: `.humanify-experiments/outputs/{experimentId}/{sampleName}/`
- [ ] Copy snapshot to: `.humanify-experiments/outputs/{experimentId}/{sampleName}/output.js`
- [ ] Verify scoring script receives correct paths (original + output)
- [ ] Test: Run experiment, check output file exists

### Phase 4: Metrics Extraction
- [ ] Create `src/turbo-v2/webapp/server/lib/metrics-parser.ts`:
  ```typescript
  export function parseMetrics(metricsPath: string): MetricsData {
    // Read logs/metrics.jsonl
    // Filter for type: "pass_completed"
    // Aggregate: sum tokensUsed, sum identifiers
    // Return: { identifiersProcessed, identifiersRenamed, tokensUsed }
  }
  ```
- [ ] Create `src/turbo-v2/webapp/server/lib/cost-calculator.ts`:
  ```typescript
  export function calculateCost(tokens: TokenUsage, model: string): CostBreakdown {
    // MODEL_PRICING = { 'gpt-4o-mini': { input: 0.150, output: 0.600 }, ... }
    // inputCost = (tokens.promptTokens / 1_000_000) * pricing.input
    // outputCost = (tokens.completionTokens / 1_000_000) * pricing.output
    // return { inputCost, outputCost, totalCost }
  }
  ```
- [ ] Update `run-experiment.ts` to call both after execution
- [ ] Populate `SampleResult` fields
- [ ] Test: Verify all fields non-zero after run

### Phase 5: Testing
- [ ] Write unit tests (see Testing Strategy section)
- [ ] Write E2E test (see Testing Strategy section)
- [ ] Run regression tests
- [ ] Manual smoke test via UI

### Phase 6: Documentation
- [ ] Update `src/turbo-v2/webapp/README.md` with setup steps
- [ ] Document metrics file format
- [ ] Add troubleshooting section

---

## Ambiguities and Questions

### Resolved (proceeding with recommended approach)
1. **Output File Location**
   - **Decision**: Copy final snapshot to dashboard output location (Option B)
   - **Rationale**: Maintains separation of concerns, checkpoint isolation preserved

### Open (implementer can decide)
2. **Metrics File Not Found**
   - **Question**: What if `logs/metrics.jsonl` doesn't exist after execution?
   - **Options**: (A) Return zeros and log warning, (B) Throw error and fail experiment, (C) Retry once
   - **Recommendation**: Option A (graceful degradation, log warning)

3. **Model Name Detection**
   - **Question**: How to detect which model was used (for cost calculation)?
   - **Context**: Dashboard stores preset, not model name
   - **Options**: (A) Hardcode preset → model mapping, (B) Read from CLI output, (C) Store model in config
   - **Recommendation**: Option A for MVP (fast → gpt-4o-mini, quality → gpt-4o)

---

## Definition of Done

See separate file: `DOD-2025-12-31-031329.md`

---

**Sprint Plan Ready For**: Implementation
**Blockers**: None (clear path forward)
**Risk Level**: MEDIUM (multiple integration points, but well-understood)
**Estimated Completion**: 3-5 days (single developer, full-time)
