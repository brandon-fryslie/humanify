# Definition of Done: Experiment Dashboard Real Data Integration

**Generated**: 2025-12-31-031329
**Plan**: PLAN-2025-12-31-031329.md
**Source STATUS**: STATUS-2025-12-31.md

---

## Sprint Scope

This sprint delivers: **2 critical deliverables**
1. End-to-End Experiment Execution (CLI integration + path fixes + output handling)
2. Token/Cost Tracking and Metrics Display

**Deferred to future sprint**:
- Console log capture
- Live progress tracking via WebSocket/SSE
- Sample validation at experiment creation
- Advanced error handling and retry logic

---

## Acceptance Criteria

### Deliverable 1: End-to-End Experiment Execution

#### AC-1.1: CLI Integration
- [ ] Main CLI (`src/cli.ts`) accepts `--turbo-v2` flag and routes to `executeTurboV2()` function
- [ ] Running `npx tsx src/cli.ts unminify test-samples/canonical/tiny-qs/minified.js --turbo-v2 --preset fast --output /tmp/test-output.js` completes successfully and writes output file
- [ ] Unit test: CLI flag parsing includes `--turbo-v2` and passes correct options to executor

#### AC-1.2: Path Resolution
- [ ] Dashboard runner (`run-experiment.ts`) resolves sample paths correctly using absolute paths from project root
- [ ] No "Sample file not found" errors when running experiments from webapp working directory
- [ ] All three samples accessible: tiny-qs, small-axios, medium-chart

#### AC-1.3: Output File Handling
- [ ] After experiment runs, output file exists at `.humanify-experiments/outputs/{expId}/{sample}/output.js`
- [ ] Final snapshot correctly copied from `.humanify-checkpoints/{jobId}/snapshots/after-pass-N.js` to dashboard output location
- [ ] Scoring script (`score-semantic.ts`) executes successfully with original and output files

#### AC-1.4: End-to-End Execution
- [ ] Experiment result shows non-zero score (proves scoring worked)
- [ ] E2E test: Create experiment via API, run it, verify output file created and score populated
- [ ] No errors in experiment execution logs

---

### Deliverable 2: Token/Cost Tracking and Metrics Display

#### AC-2.1: Metrics Parsing
- [ ] `parseMetrics(metricsFilePath)` function reads `logs/metrics.jsonl` and returns aggregated data with: `{ identifiersProcessed, identifiersRenamed, tokensUsed: { promptTokens, completionTokens, totalTokens } }`
- [ ] Function correctly aggregates metrics from multi-pass experiments (sums tokens across all `pass_completed` events)
- [ ] Unit test: `parseMetrics()` with mock metrics.jsonl returns expected aggregated values

#### AC-2.2: Cost Calculation
- [ ] `calculateCost(tokens, modelName)` function returns `CostBreakdown` with: `{ inputCost, outputCost, totalCost }`
- [ ] Correct pricing applied for gpt-4o-mini: $0.150/1M input, $0.600/1M output
- [ ] Correct pricing applied for gpt-4o: $2.50/1M input, $10.00/1M output
- [ ] Unit test: `calculateCost({ promptTokens: 1000, completionTokens: 500 }, 'gpt-4o-mini')` returns ~$0.000225

#### AC-2.3: Data Population
- [ ] After experiment completes, `SampleResult.tokens` is populated with non-zero values
- [ ] After experiment completes, `SampleResult.cost` is populated with calculated cost breakdown
- [ ] After experiment completes, `SampleResult.identifiersProcessed` and `SampleResult.identifiersRenamed` are populated
- [ ] E2E test: Run experiment, verify all metrics fields are non-zero and accurate

#### AC-2.4: UI Display
- [ ] Dashboard UI displays token counts (promptTokens, completionTokens, totalTokens) in experiment detail view
- [ ] Dashboard UI displays cost breakdown (inputCost, outputCost, totalCost) in experiment detail view
- [ ] Dashboard UI displays identifier statistics (identifiersProcessed, identifiersRenamed) in experiment detail view
- [ ] All metrics visible in compare view charts

---

## Sprint Complete When (Success Criteria)

### Functional Requirements
- [ ] User can create experiment with preset + samples via UI
- [ ] User can click "Run" and experiment executes without errors
- [ ] Experiment detail view shows:
  - Non-zero semantic similarity score
  - Token usage (promptTokens, completionTokens, totalTokens)
  - Cost breakdown (inputCost, outputCost, totalCost)
  - Identifier statistics (identifiersProcessed, identifiersRenamed)
  - Processing duration
- [ ] Compare view can chart metrics across multiple experiments

### Quality Requirements
- [ ] All unit tests pass (new + existing)
- [ ] All E2E tests pass (new + existing)
- [ ] No regressions in existing turbo-v2 functionality (all Sprint 1-9 tests still pass)
- [ ] Code coverage >80% for new functions (parseMetrics, calculateCost)
- [ ] No hardcoded paths (use path resolution from project root)
- [ ] Error handling for missing files and malformed metrics

### Smoke Test
- [ ] End-to-end smoke test succeeds:
  1. Start webapp: `cd src/turbo-v2/webapp && npm start`
  2. Create experiment via UI: preset="fast", samples=["tiny-qs"]
  3. Click "Run" button
  4. Wait for completion (status shows "completed")
  5. Verify experiment detail shows: score > 0, tokens > 0, cost > 0, identifiers > 0
  6. Verify compare view displays metrics correctly

---

## Manual Verification Steps

### Step 1: CLI Integration Test
```bash
cd /Users/bmf/code/brandon-fryslie_humanify
npx tsx src/cli.ts unminify test-samples/canonical/tiny-qs/minified.js \
  --turbo-v2 \
  --preset fast \
  --output /tmp/test-output.js \
  --provider openai

# Verify:
# - Command completes without errors
# - Output file exists at /tmp/test-output.js
# - Output file is valid JavaScript
```

### Step 2: Dashboard Execution Test
```bash
cd src/turbo-v2/webapp
npm run build
npm start  # Starts server on port 3001

# In browser, navigate to http://localhost:3001
# 1. Create new experiment
#    - Select preset: "fast"
#    - Select samples: ["tiny-qs"]
#    - Click "Create Experiment"
# 2. Run experiment
#    - Click "Run" button
#    - Wait for status to change to "completed"
# 3. Verify results
#    - Score > 0
#    - Duration > 0
#    - Tokens displayed (all > 0)
#    - Cost displayed (all > 0)
#    - Identifiers displayed (processed > 0, renamed > 0)
```

### Step 3: API Verification Test
```bash
# Create experiment
curl -X POST http://localhost:3001/api/experiments \
  -H "Content-Type: application/json" \
  -d '{"preset":"fast","samples":["tiny-qs"]}'
# Note the returned experiment ID

# Run experiment
curl -X POST http://localhost:3001/api/experiments/{id}/run

# Poll status
curl http://localhost:3001/api/experiments/{id}/status
# Wait until: {"status":"completed","completedSamples":1,"totalSamples":1}

# Get results
curl http://localhost:3001/api/experiments/{id} | jq '.results[0]'
# Verify all fields populated:
# {
#   "sample": "tiny-qs",
#   "score": <number > 0>,
#   "duration": <number > 0>,
#   "tokens": {
#     "promptTokens": <number > 0>,
#     "completionTokens": <number > 0>,
#     "totalTokens": <number > 0>
#   },
#   "cost": {
#     "inputCost": <number > 0>,
#     "outputCost": <number > 0>,
#     "totalCost": <number > 0>
#   },
#   "identifiersProcessed": <number > 0>,
#   "identifiersRenamed": <number > 0>
# }
```

### Step 4: File Verification Test
```bash
# After running experiment, verify files exist:
ls -lh .humanify-experiments/outputs/{experimentId}/tiny-qs/output.js
ls -lh .humanify-checkpoints/*/snapshots/after-pass-*.js

# Verify output is valid JavaScript:
node -c .humanify-experiments/outputs/{experimentId}/tiny-qs/output.js
# Should print nothing (no syntax errors)
```

---

## Automated Test Requirements

### Unit Tests (Minimum Coverage)
1. CLI flag parsing test
2. Path resolution test (absolute paths from different working directories)
3. Cost calculation tests (gpt-4o-mini, gpt-4o, edge cases)
4. Metrics parsing tests (single-pass, multi-pass, malformed file, missing file)

### E2E Tests (Minimum Coverage)
1. Full experiment lifecycle (create → run → verify)
2. Multi-sample experiment (all 3 samples)
3. Error handling (missing sample file, malformed metrics)

### Regression Tests
1. All existing turbo-v2 unit tests pass: `npm run test:unit`
2. All existing turbo-v2 e2e tests pass: `npm run test:e2e`
3. Checkpoint/resume functionality still works

---

## Performance Benchmarks

### Expected Results (Fast Preset, gpt-4o-mini)

| Sample | Duration | Tokens | Cost | Score |
|--------|----------|--------|------|-------|
| tiny-qs | 30-60s | ~15K | ~$0.01 | >70 |
| small-axios | 3-5min | ~80K | ~$0.05 | >70 |
| medium-chart | 15-25min | ~500K | ~$0.30 | >70 |

### Acceptance Thresholds
- [ ] tiny-qs completes in <90s
- [ ] All experiments show score >50 (semantic similarity)
- [ ] Token counts match expected ranges (±20%)
- [ ] Cost calculations accurate to 0.1 cent

---

## Rollback Criteria

If any of the following occur during sprint, rollback and reassess:

1. **Breaking Changes**: Existing turbo-v2 tests fail (regression)
2. **Data Loss**: Experiments.json corrupted or lost
3. **Severe Performance**: Experiments take >3x expected duration
4. **Cost Overruns**: Actual costs >2x calculated costs (pricing error)

---

## Documentation Requirements

- [ ] Update `src/turbo-v2/webapp/README.md` with:
  - Setup instructions (dependencies, build, start)
  - How to run experiments
  - Metrics file format documentation
  - Troubleshooting section (common errors)
- [ ] Add inline comments for:
  - `parseMetrics()` function (file format, aggregation logic)
  - `calculateCost()` function (pricing source, update frequency)
  - Output file copying logic (checkpoint structure dependency)
- [ ] Update CHANGELOG with sprint deliverables

---

**Definition of Done Ready For**: Implementation
**Review Required**: Yes (code review + manual verification steps)
**Estimated Verification Time**: 2 hours (includes manual smoke tests)
