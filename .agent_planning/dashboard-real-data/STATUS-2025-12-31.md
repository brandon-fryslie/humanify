# Status Report: Turbo-V2 Experiment Dashboard Real Data Integration
**Timestamp**: 2025-12-31
**Scope**: Experiment Dashboard → Real Processing Pipeline Integration
**Confidence**: FRESH

---

## Executive Summary

**Status**: PARTIALLY COMPLETE - UI functional, processing pipeline exists but NOT INTEGRATED

**Core Problem**: Dashboard runs experiments that show all zeros because `run-experiment.ts` executes CLI commands but:
1. Sample files are in wrong location
2. CLI integration with turbo-v2 is NOT wired up in main CLI
3. Token/cost tracking exists in turbo-v2 but NOT captured by dashboard runner
4. No data flow from turbo-v2 metrics back to dashboard storage

**Gap**: The turbo-v2 infrastructure (Sprints 1-9) is complete and tested, but the dashboard runner bypasses it entirely by calling a non-existent CLI integration.

---

## What Exists (Assets)

### ✅ Complete Components

**1. Dashboard UI** (src/turbo-v2/webapp/client/)
- ExperimentList.tsx: List/filter/delete experiments
- ExperimentForm.tsx: Create experiments with preset/sample selection
- CompareView.tsx: Chart-based comparison
- ExperimentDetail.tsx: View results
- PresetManager.tsx: CRUD for custom presets
- All UI is functional and well-structured

**2. Backend API** (src/turbo-v2/webapp/server/api/)
- experiments.ts: CRUD endpoints
- presets.ts: Preset management
- runner.ts: Execute and status endpoints
- Storage: JSON file persistence in `.humanify-experiments/`

**3. Test Samples** (test-samples/canonical/)
```
test-samples/canonical/
  tiny-qs/
    minified.js      ✅ EXISTS
    original.js      ✅ EXISTS
  small-axios/
    minified.js      ✅ EXISTS
    original.js      ✅ EXISTS
  medium-chart/
    minified.js      ✅ EXISTS
    original.js      ✅ EXISTS
```

**4. Scoring Script** (scripts/score-semantic.ts)
- LLM-as-judge semantic scoring
- Extracts identifiers from original and output
- GPT-4o comparison (0-100 score)
- Returns JSON with: score, explanation, tokensUsed, cost
- Format: `npx tsx scripts/score-semantic.ts <original.js> <output.js>`

**5. Turbo-V2 Infrastructure** (src/turbo-v2/)
```
✅ Vault: Request-level caching (.humanify-cache/vault/)
✅ Ledger: Event-sourced checkpointing (events.jsonl)
✅ Analyzer: Identifier extraction and importance scoring
✅ PassEngine: Parallel/streaming/sequential execution
✅ MultiPass: N-pass orchestration with glossary injection
✅ Transformer: AST-level safe renaming
✅ MetricsCollector: Token/timing/error tracking
✅ ProgressRenderer: Terminal UI with color coding
✅ CheckpointManager: Mid-pass checkpointing and resume
✅ Presets: fast, balanced, thorough, quality, anchor
```

All have passing tests (Sprints 1-9 complete per STATUS-2025-12-30-015508.md)

---

## What's Missing (Critical Gaps)

### ❌ Gap 1: CLI Integration Missing

**Problem**: `run-experiment.ts` line 53 calls:
```typescript
const cmd = `npx tsx src/cli.ts unminify "${inputPath}" --turbo-v2 --preset ${preset} --output "${outputPath}" --provider openai`;
```

**Reality**: Main CLI (`src/cli.ts`) does NOT have `--turbo-v2` flag integrated.

**Evidence**:
```bash
$ grep -n "turbo-v2" /Users/bmf/code/brandon-fryslie_humanify/src/cli.ts
# (no results)
```

**What Exists**:
- `src/turbo-v2/cli/turbo-v2-command.ts` exports `executeTurboV2(options)` function
- Full implementation with pass parsing, preset handling, processor creation
- NOT wired into main CLI

**Fix Required**: Add `--turbo-v2` flag to main CLI that routes to `executeTurboV2()`.

---

### ❌ Gap 2: Sample File Path Mismatch

**Problem**: Dashboard runner looks for samples at:
```
test-samples/canonical/{sample}/minified.js
```

But executes from webapp working directory (`src/turbo-v2/webapp/`), so path resolves to:
```
src/turbo-v2/webapp/test-samples/canonical/...  ❌ DOES NOT EXIST
```

**Actual location**:
```
/Users/bmf/code/brandon-fryslie_humanify/test-samples/canonical/  ✅ EXISTS
```

**Evidence**: experiments.json shows errors:
```json
{
  "sample": "tiny-qs",
  "score": 0,
  "duration": 0.00004225,
  "error": "Sample file not found: test-samples/canonical/tiny-qs/minified.js"
}
```

**Fix Required**:
- Option A: Use absolute paths from project root
- Option B: Change working directory before execution
- Option C: Move/symlink samples into webapp directory

---

### ❌ Gap 3: Token/Cost Tracking Not Captured

**Problem**: Dashboard expects `SampleResult` with:
```typescript
interface SampleResult {
  sample: SampleName;
  score: number;
  duration: number;
  tokens?: TokenUsage;           // ❌ NOT POPULATED
  cost?: CostBreakdown;          // ❌ NOT POPULATED
  identifiersProcessed?: number; // ❌ NOT POPULATED
  identifiersRenamed?: number;   // ❌ NOT POPULATED
}
```

**What Exists**:
- `MetricsCollector` tracks all this data and writes to `logs/metrics.jsonl`
- Format:
  ```json
  {
    "type": "pass_completed",
    "stats": {
      "identifiersProcessed": 150,
      "identifiersRenamed": 142,
      "tokensUsed": { "prompt": 42000, "completion": 6000, "total": 48000 },
      "duration": 82100
    }
  }
  ```

**Gap**: No bridge from turbo-v2 metrics output to dashboard storage.

**Fix Required**:
- Parse metrics.jsonl after experiment completes
- Extract token/cost/identifier counts
- Calculate costs from token usage (model-specific pricing)
- Populate SampleResult fields

---

### ❌ Gap 4: Output Path Not Standardized

**Problem**: Dashboard expects outputs at:
```
.humanify-experiments/outputs/{experimentId}/{sample}/output.js
```

Turbo-v2 creates outputs at:
```
.humanify-checkpoints/{jobId}/snapshots/after-pass-{N}.js
```

**Mismatch**: Different directory structure, different naming convention.

**Fix Required**:
- Configure turbo-v2 `--output` to match dashboard expectations
- Or: Read from checkpoint snapshots and copy to expected location
- Or: Update dashboard to read from checkpoint locations

---

### ❌ Gap 5: Console Logging Not Captured

**Problem**: Dashboard shows "Console Logs" panel but `consoleLogs` array is always empty.

**What Exists**:
- Turbo-v2 has extensive console output
- MetricsCollector supports verbose logging
- No capture mechanism

**Fix Required**:
- Capture stdout/stderr during execution
- Parse log lines and structure as `ConsoleLogEntry[]`
- Store in experiments.json

---

## Data Flow Analysis

### Current (Broken) Flow

```
User clicks "Run"
  ↓
Dashboard calls POST /api/experiments/:id/run
  ↓
runner.ts calls runExperiment(id)
  ↓
run-experiment.ts executes CLI command
  ↓
execSync("npx tsx src/cli.ts unminify ... --turbo-v2 ...")
  ↓
❌ CLI does not recognize --turbo-v2 flag
  ↓
❌ Falls back to old sequential processing (or errors)
  ↓
❌ No output file created
  ↓
❌ Scoring fails (output not found)
  ↓
Result: score=0, error="Output file not found"
```

### Desired Flow

```
User clicks "Run"
  ↓
Dashboard calls POST /api/experiments/:id/run
  ↓
runner.ts calls runExperiment(id)
  ↓
run-experiment.ts:
  1. Read sample from test-samples/canonical/{sample}/minified.js
  2. Configure output: .humanify-experiments/outputs/{expId}/{sample}/
  3. Build command with correct paths and --turbo-v2
  4. execSync CLI command
  ↓
Main CLI (src/cli.ts) routes --turbo-v2 to executeTurboV2()
  ↓
executeTurboV2() runs full pipeline:
  - Analyzer extracts identifiers
  - MultiPass runs N passes
  - Each pass uses ProcessorFunction (OpenAI API calls)
  - MetricsCollector tracks tokens/costs
  - Snapshots written to checkpoints/
  - Final output copied to expected location
  ↓
run-experiment.ts:
  1. Parse logs/metrics.jsonl for token/cost/identifier counts
  2. Run scorer: npx tsx scripts/score-semantic.ts original.js output.js
  3. Build SampleResult with all fields populated
  4. Store result in experiments.json
  ↓
Frontend polls status endpoint
  ↓
Displays: score, duration, tokens, cost, identifier counts
```

---

## Validation Gaps

### Missing Checks (Implementer Should Create)

**1. Smoke Test for Dashboard Runner**
```bash
# Test end-to-end experiment execution
cd src/turbo-v2/webapp/
npm run build  # Build frontend
npm start      # Start server in background
curl -X POST http://localhost:3001/api/experiments \
  -H "Content-Type: application/json" \
  -d '{"preset":"fast","samples":["tiny-qs"]}'
# Should create experiment

curl -X POST http://localhost:3001/api/experiments/{id}/run
# Should execute and populate results

curl http://localhost:3001/api/experiments/{id}
# Should return non-zero score and populated fields
```

**2. Token Tracking Validation**
```bash
# Run experiment, verify tokens are captured
grep -A5 "tokens" .humanify-experiments/experiments.json
# Should show non-zero promptTokens, completionTokens, totalTokens
```

**3. Cost Calculation Test**
```typescript
// Verify cost calculation matches OpenAI pricing
const tokens = { prompt: 1000, completion: 500 };
const cost = calculateCost(tokens, "gpt-4o-mini");
// Should return ~$0.000225 for gpt-4o-mini ($0.150/1M input, $0.600/1M output)
```

---

## Implementation Roadmap

### Phase 1: CLI Integration (BLOCKER)
**Priority**: CRITICAL - Nothing works without this

**Tasks**:
1. Add `--turbo-v2` flag to main CLI (src/cli.ts)
2. Route to `executeTurboV2()` from turbo-v2-command.ts
3. Test basic execution: `npx tsx src/cli.ts unminify test.js --turbo-v2`

**Estimated Effort**: 2-4 hours

---

### Phase 2: Path Configuration
**Priority**: HIGH - Fixes sample not found errors

**Tasks**:
1. Update SAMPLES_DIR in run-experiment.ts to use absolute path
2. Configure output directory to match dashboard expectations
3. Ensure executeTurboV2 writes final snapshot to correct location

**Estimated Effort**: 1-2 hours

---

### Phase 3: Metrics Extraction
**Priority**: HIGH - Enables token/cost tracking

**Tasks**:
1. Create `parseMetrics()` function to read logs/metrics.jsonl
2. Extract: identifiersProcessed, identifiersRenamed, tokensUsed
3. Implement `calculateCost()` for model-specific pricing:
   - gpt-4o-mini: $0.150/1M input, $0.600/1M output
   - gpt-4o: $2.50/1M input, $10/1M output
4. Populate SampleResult fields from metrics

**Estimated Effort**: 3-4 hours

---

### Phase 4: Console Logging Capture
**Priority**: MEDIUM - Nice to have for debugging

**Tasks**:
1. Capture stdout/stderr during execSync
2. Parse log lines into structured ConsoleLogEntry[]
3. Store in experiments.json per experiment

**Estimated Effort**: 2-3 hours

---

### Phase 5: Progress Tracking
**Priority**: LOW - Future enhancement

**Tasks**:
1. Implement live progress updates via SSE or WebSockets
2. Stream ProgressRenderer output to dashboard
3. Show real-time pass/batch progress

**Estimated Effort**: 4-6 hours

---

## Cost Calculation Reference

### Model Pricing (as of 2025-12-31)

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| gpt-4o-mini | $0.150 | $0.600 |
| gpt-4o | $2.50 | $10.00 |
| gpt-3.5-turbo | $0.50 | $1.50 |

### Calculation Formula
```typescript
function calculateCost(tokens: TokenUsage, model: string): CostBreakdown {
  const pricing = MODEL_PRICING[model];
  const inputCost = (tokens.promptTokens / 1_000_000) * pricing.input;
  const outputCost = (tokens.completionTokens / 1_000_000) * pricing.output;

  return {
    inputCost,
    outputCost,
    totalCost: inputCost + outputCost,
  };
}
```

---

## Test Sample Details

### Sample Characteristics

| Sample | Identifiers | Lines | Size | Purpose |
|--------|-------------|-------|------|---------|
| tiny-qs | ~150 | 356 | 5KB | Fast iteration, smoke tests |
| small-axios | ~800 | 3K | 50KB | Medium complexity |
| medium-chart | ~5000 | 11K | 200KB | Stress testing |

### Expected Performance (Fast Preset)

| Sample | Duration | Tokens | Cost (gpt-4o-mini) |
|--------|----------|--------|-------------------|
| tiny-qs | ~30-60s | ~15K | ~$0.01 |
| small-axios | ~3-5min | ~80K | ~$0.05 |
| medium-chart | ~15-25min | ~500K | ~$0.30 |

---

## Ambiguities Found

### Question 1: Where should final output be written?

**Context**: Dashboard expects `.humanify-experiments/outputs/`, turbo-v2 uses `.humanify-checkpoints/`.

**Options**:
- A: Configure turbo-v2 to write directly to dashboard location (breaks checkpoint isolation)
- B: Copy final snapshot after completion (2x disk space)
- C: Update dashboard to read from checkpoint snapshots (coupling to turbo-v2 structure)

**Impact**: Affects file organization and checkpoint cleanup logic.

**Recommendation**: Option B (copy after completion) - maintains separation of concerns.

---

### Question 2: How to handle experiments in progress?

**Context**: Dashboard has no live progress tracking. Status endpoint only shows completedSamples/totalSamples.

**Options**:
- A: Poll metrics.jsonl for real-time stats (requires file watching)
- B: Implement SSE/WebSocket streaming (more complex)
- C: Show basic progress only (current approach)

**Impact**: User experience during long-running experiments.

**Recommendation**: Option C for MVP, Option B for future enhancement.

---

### Question 3: Should dashboard validate samples before running?

**Context**: Experiments fail silently if samples don't exist. User only sees error after execution.

**Options**:
- A: Validate sample files exist before creating experiment
- B: Show warnings but allow creation
- C: No validation (current)

**Impact**: User experience and error clarity.

**Recommendation**: Option A - fail fast at creation time.

---

## Findings Summary

### Implementation Status

| Component | Status | Confidence | Evidence | Issues |
|-----------|--------|------------|----------|--------|
| Dashboard UI | COMPLETE | FRESH | All 5 components exist and functional | None - ready for data |
| Backend API | COMPLETE | FRESH | CRUD endpoints tested | None |
| Sample Files | COMPLETE | FRESH | All 3 samples in test-samples/canonical/ | Wrong path in runner |
| Scoring Script | COMPLETE | FRESH | score-semantic.ts exists and tested | None |
| Turbo-V2 Core | COMPLETE | RECENT | Sprints 1-9 complete per STATUS-2025-12-30 | Not integrated with CLI |
| CLI Integration | STUB | FRESH | executeTurboV2() exists but not wired | **BLOCKER** |
| Token Tracking | PARTIAL | FRESH | MetricsCollector exists but not read by dashboard | Missing bridge |
| Output Handling | STUB | FRESH | No copy from checkpoints to outputs | Missing |
| Console Logging | NOT_STARTED | FRESH | No capture mechanism | Missing |

---

## Recommendations

### Priority 1: Unblock Basic Functionality

1. **Wire CLI Integration** (2-4 hours)
   - Add `--turbo-v2` flag to src/cli.ts
   - Route to executeTurboV2()
   - Test basic execution

2. **Fix Sample Paths** (1-2 hours)
   - Use absolute paths from project root
   - Verify all samples are accessible

3. **Configure Output Location** (1 hour)
   - Ensure final snapshot written to `.humanify-experiments/outputs/{expId}/{sample}/output.js`

**Result**: Basic experiment execution will work, showing scores.

---

### Priority 2: Enable Full Metrics

4. **Parse Metrics Data** (3-4 hours)
   - Read logs/metrics.jsonl after execution
   - Extract tokens, identifier counts
   - Calculate costs
   - Populate all SampleResult fields

**Result**: Dashboard shows complete data (tokens, costs, identifier counts).

---

### Priority 3: Polish

5. **Console Log Capture** (2-3 hours)
   - Capture stdout/stderr
   - Structure as ConsoleLogEntry[]

6. **Sample Validation** (1 hour)
   - Check files exist at experiment creation

**Result**: Better UX and error handling.

---

## Verdict

**Workflow**: ⏸️ PAUSE - Clarification needed before implementation

### Clarification Needed

**Question**: Where should turbo-v2 write final output?

**Context**: Dashboard runner expects outputs at `.humanify-experiments/outputs/`, but turbo-v2 uses `.humanify-checkpoints/` for isolation. The checkpoint structure is designed for:
- Resume functionality
- Audit trails
- Multi-job isolation

Copying breaks DRY. Reading from checkpoints couples dashboard to turbo-v2 internals. Writing directly breaks checkpoint isolation.

**Options**:
- A: Configure `--output` to write directly to dashboard location (breaks isolation)
- B: Copy final snapshot after completion (2x disk, but clean separation)
- C: Dashboard reads from checkpoint snapshots (coupling, but efficient)

**Impact**: Affects checkpoint cleanup, disk usage, and coupling between systems.

**Recommendation**: Proceed with Option B (copy) for MVP, refactor to Option C if disk space becomes issue.

---

## Next Steps

### Immediate (Phase 1)
```bash
# 1. Add CLI integration
cd /Users/bmf/code/brandon-fryslie_humanify
# Edit src/cli.ts to add --turbo-v2 flag
# Test: npx tsx src/cli.ts unminify test-samples/canonical/tiny-qs/minified.js --turbo-v2

# 2. Fix paths in run-experiment.ts
# Use path.join(__dirname, '../../../test-samples/canonical/...')

# 3. Test end-to-end
cd src/turbo-v2/webapp
npm start
# Create and run experiment via UI
```

### Follow-up (Phase 2)
```bash
# 4. Implement metrics parsing
# Create parseMetrics() and calculateCost()
# Test with completed experiment

# 5. Verify all fields populated
curl http://localhost:3001/api/experiments/{id}
# Should show: tokens, cost, identifiersProcessed, identifiersRenamed
```

---

**Status report generated by project-evaluator**
**Ready for**: Implementation (after clarification)
**Blockers**: CLI integration (can proceed with recommended approach)
**Risk Level**: MEDIUM (clear path forward, but multiple integration points)
